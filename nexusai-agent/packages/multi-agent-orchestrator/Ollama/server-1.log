time=2025-08-24T13:10:38.519-04:00 level=INFO source=routes.go:1235 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:D:\\UNIFIED_OLLAMA_MODELS OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-08-24T13:10:38.524-04:00 level=INFO source=images.go:480 msg="total blobs: 69"
time=2025-08-24T13:10:38.525-04:00 level=INFO source=images.go:487 msg="total unused blobs removed: 0"
time=2025-08-24T13:10:38.526-04:00 level=INFO source=routes.go:1288 msg="Listening on [::]:11434 (version 0.9.2)"
time=2025-08-24T13:10:38.526-04:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-08-24T13:10:38.526-04:00 level=INFO source=gpu_windows.go:167 msg=packages count=1
time=2025-08-24T13:10:38.526-04:00 level=INFO source=gpu_windows.go:183 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-08-24T13:10:38.526-04:00 level=INFO source=gpu_windows.go:214 msg="" package=0 cores=8 efficiency=4 threads=12
time=2025-08-24T13:10:38.661-04:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda variant=v12 compute=8.6 driver=12.3 name="NVIDIA GeForce RTX 3050 6GB Laptop GPU" total="6.0 GiB" available="5.0 GiB"
[GIN] 2025/08/24 - 13:10:38 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/24 - 13:10:38 | 200 |        2.02ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/24 - 13:10:45 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/24 - 13:10:45 | 200 |     21.5669ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:14:51 | 200 |            0s |             ::1 | GET      "/api/version"
[GIN] 2025/08/24 - 13:14:51 | 200 |      4.2322ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/24 - 13:14:51 | 200 |     30.5791ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:14:51 | 200 |     31.2044ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:14:51 | 200 |     32.7821ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:14:51 | 200 |     28.5122ms |             ::1 | POST     "/api/show"
time=2025-08-24T13:14:51.699-04:00 level=ERROR source=images.go:91 msg="couldn't decode ggml" error="invalid file magic"
[GIN] 2025/08/24 - 13:14:51 | 500 |      2.0858ms |             ::1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:14:53 | 200 |     36.2104ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:14:53 | 200 |     36.7442ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:14:53 | 200 |     40.4625ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:16:03 | 200 |     34.9039ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:16:03 | 200 |     36.3794ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:16:03 | 200 |     38.6554ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:16:04 | 200 |            0s |             ::1 | GET      "/api/version"
[GIN] 2025/08/24 - 13:16:04 | 200 |      3.2188ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/24 - 13:16:04 | 200 |     30.2112ms |             ::1 | POST     "/api/show"
time=2025-08-24T13:16:04.216-04:00 level=ERROR source=images.go:91 msg="couldn't decode ggml" error="invalid file magic"
[GIN] 2025/08/24 - 13:16:04 | 500 |      2.6403ms |             ::1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:16:05 | 200 |      31.564ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:16:05 | 200 |     33.6173ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:16:05 | 200 |     34.1062ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:17:55 | 200 |      27.619ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:17:55 | 200 |     30.3195ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:17:55 | 200 |     30.8456ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:17:55 | 200 |            0s |             ::1 | GET      "/api/version"
[GIN] 2025/08/24 - 13:17:55 | 200 |      2.6316ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/24 - 13:17:55 | 200 |     29.2867ms |             ::1 | POST     "/api/show"
time=2025-08-24T13:17:55.834-04:00 level=ERROR source=images.go:91 msg="couldn't decode ggml" error="invalid file magic"
[GIN] 2025/08/24 - 13:17:55 | 500 |      2.0678ms |             ::1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:17:57 | 200 |     29.8649ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:17:57 | 200 |     30.4301ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:17:57 | 200 |     34.8581ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:21:25 | 200 |     34.4546ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:21:25 | 200 |     34.9824ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:21:25 | 200 |     36.6821ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:21:25 | 200 |            0s |             ::1 | GET      "/api/version"
[GIN] 2025/08/24 - 13:21:25 | 200 |      2.1929ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/24 - 13:21:25 | 200 |      26.768ms |             ::1 | POST     "/api/show"
time=2025-08-24T13:21:25.452-04:00 level=ERROR source=images.go:91 msg="couldn't decode ggml" error="invalid file magic"
[GIN] 2025/08/24 - 13:21:25 | 500 |      2.1491ms |             ::1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:21:27 | 200 |     31.4495ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:21:27 | 200 |     32.4587ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:21:27 | 200 |     33.5416ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:24:06 | 200 |            0s |             ::1 | GET      "/api/version"
[GIN] 2025/08/24 - 13:24:06 | 200 |      3.6554ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/24 - 13:24:06 | 200 |     29.6051ms |             ::1 | POST     "/api/show"
time=2025-08-24T13:24:06.485-04:00 level=ERROR source=images.go:91 msg="couldn't decode ggml" error="invalid file magic"
[GIN] 2025/08/24 - 13:24:06 | 500 |        2.88ms |             ::1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:24:08 | 200 |     30.3287ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:24:08 | 200 |     33.6404ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:24:08 | 200 |     34.7615ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:24:10 | 200 |     33.7989ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:24:10 | 200 |     35.9461ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 13:24:10 | 200 |      38.474ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/24 - 22:11:54 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/24 - 22:11:54 | 200 |      8.8069ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 09:50:35 | 200 |            0s |       127.0.0.1 | HEAD     "/"
time=2025-08-25T09:50:38.573-04:00 level=INFO source=download.go:177 msg="downloading dde5aa3fc5ff in 16 126 MB part(s)"
time=2025-08-25T09:51:14.308-04:00 level=INFO source=download.go:177 msg="downloading 966de95ca8a6 in 1 1.4 KB part(s)"
time=2025-08-25T09:51:15.972-04:00 level=INFO source=download.go:177 msg="downloading fcc5a6bec9da in 1 7.7 KB part(s)"
time=2025-08-25T09:51:17.646-04:00 level=INFO source=download.go:177 msg="downloading a70ff7e570d9 in 1 6.0 KB part(s)"
time=2025-08-25T09:51:19.206-04:00 level=INFO source=download.go:177 msg="downloading 34bb5ab01051 in 1 561 B part(s)"
[GIN] 2025/08/25 - 09:51:24 | 200 |   48.5807814s |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/08/25 - 09:51:35 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/25 - 09:51:35 | 200 |     99.9505ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-25T09:51:35.484-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=6079389696 required="3.7 GiB"
time=2025-08-25T09:51:35.504-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="66.6 GiB" free_swap="71.7 GiB"
time=2025-08-25T09:51:35.505-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[5.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-25T09:51:36.271-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 4 --no-mmap --parallel 2 --port 61800"
time=2025-08-25T09:51:36.285-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-25T09:51:36.285-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-25T09:51:36.286-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-25T09:51:36.323-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-25T09:51:36.435-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-25T09:51:36.437-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:61800"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-25T09:51:36.538-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB
llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-08-25T09:51:37.541-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.26 seconds"
[GIN] 2025/08/25 - 09:51:37 | 200 |    2.2258414s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/25 - 09:51:47 | 200 |    1.9053982s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 09:52:18 | 200 |    4.9566394s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 09:52:33 | 200 |    3.2389519s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 09:53:04 | 200 |     920.348ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 09:53:17 | 200 |     3.123114s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 09:54:06 | 200 |    3.3946378s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 09:55:07 | 200 |    723.6348ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 09:55:50 | 200 |    4.3441252s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 09:56:41 | 200 |    3.2716972s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 09:57:09 | 200 |    3.0261608s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 09:58:07 | 200 |    3.1618433s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 09:58:37 | 200 |    822.9789ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 09:59:13 | 200 |     2.582397s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 10:07:23 | 200 |      9.7497ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:23 | 200 |      9.8878ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:23 | 200 |     13.0196ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:23 | 200 |     12.5115ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:25 | 200 |      9.4622ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:25 | 200 |      9.9879ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:34 | 200 |      9.5741ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:34 | 200 |      9.5741ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:35 | 200 |      11.009ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:35 | 200 |     12.2811ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:37 | 200 |      8.5231ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:37 | 200 |      8.7774ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:39 | 200 |      11.023ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:39 | 200 |      12.238ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:41 | 200 |      8.5058ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:41 | 200 |        8.32ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:43 | 200 |     10.2652ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:43 | 200 |     11.1496ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:43 | 200 |      9.0895ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:43 | 200 |     10.3431ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:45 | 200 |     10.7694ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:45 | 200 |     11.3464ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:47 | 200 |       7.699ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:47 | 200 |      9.0896ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:49 | 200 |     12.1013ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:07:49 | 200 |     11.3587ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:08:08 | 200 |            0s |             ::1 | GET      "/"
[GIN] 2025/08/25 - 10:08:09 | 404 |            0s |             ::1 | GET      "/favicon.ico"
[GIN] 2025/08/25 - 10:10:13 | 200 |     11.5979ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:13 | 200 |     12.2471ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:13 | 200 |     12.2245ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:13 | 200 |     12.7398ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:15 | 200 |      9.5846ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:15 | 200 |     10.1478ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:17 | 200 |     12.8955ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:17 | 200 |     12.7439ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:19 | 200 |      8.9723ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:19 | 200 |      8.7012ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:21 | 200 |     10.9069ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:21 | 200 |      11.931ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:23 | 200 |     11.3357ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:23 | 200 |     11.3357ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:25 | 200 |     10.1159ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:25 | 200 |     11.2781ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:27 | 200 |      9.9541ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:27 | 200 |     10.5485ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:29 | 200 |     13.6037ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:29 | 200 |     14.3735ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:31 | 200 |     10.1031ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:31 | 200 |     10.7425ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:33 | 200 |     11.6774ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:33 | 200 |     11.0304ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:35 | 200 |      9.6371ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:35 | 200 |      9.6371ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:37 | 200 |     10.1384ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:37 | 200 |     10.9986ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:39 | 200 |      7.2994ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:39 | 200 |      9.2573ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:41 | 200 |      8.8625ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:41 | 200 |     11.8429ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:43 | 200 |      9.0044ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:43 | 200 |     10.2458ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:45 | 200 |       7.593ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:45 | 200 |     11.4705ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:47 | 200 |     11.0579ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:10:47 | 200 |     11.5661ms |       127.0.0.1 | GET      "/api/tags"
time=2025-08-25T10:11:07.556-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="66.3 GiB" free_swap="71.2 GiB"
time=2025-08-25T10:11:07.557-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=19 layers.split="" memory.available="[5.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="11.2 GiB" memory.required.partial="5.5 GiB" memory.required.kv="2.5 GiB" memory.required.allocations="[5.5 GiB]" memory.weights.total="6.8 GiB" memory.weights.repeating="6.0 GiB" memory.weights.nonrepeating="787.5 MiB" memory.graph.full="1.1 GiB" memory.graph.partial="1.4 GiB"
time=2025-08-25T10:11:07.675-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2 --ctx-size 32768 --batch-size 512 --n-gpu-layers 19 --threads 4 --no-mmap --parallel 1 --port 63068"
time=2025-08-25T10:11:07.689-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-25T10:11:07.690-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-25T10:11:07.690-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-25T10:11:07.738-04:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-25T10:11:07.739-04:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:63068"
time=2025-08-25T10:11:07.763-04:00 level=INFO source=ggml.go:92 msg="" architecture=gemma3 file_type=Q4_K_M name="Gemma 3 12b It" description="" num_tensors=626 num_key_values=41
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-25T10:11:07.861-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-25T10:11:07.936-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CPU size="5.2 GiB"
time=2025-08-25T10:11:07.936-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CUDA0 size="2.4 GiB"
time=2025-08-25T10:11:07.943-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-25T10:11:08.257-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="1.1 GiB"
time=2025-08-25T10:11:08.257-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="1.0 GiB"
time=2025-08-25T10:11:11.700-04:00 level=INFO source=server.go:630 msg="llama runner started in 4.01 seconds"
[GIN] 2025/08/25 - 10:12:57 | 200 |      9.0479ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:12:57 | 200 |     10.6636ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:12:59 | 200 |     12.2647ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:12:59 | 200 |     19.2078ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:13:01 | 200 |     17.8251ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:13:01 | 200 |     17.8251ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:13:03 | 200 |     11.3543ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:13:03 | 200 |      12.058ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:13:05 | 200 |     11.6544ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:13:05 | 200 |     14.9369ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:13:07 | 200 |     13.6405ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:13:07 | 200 |     16.4892ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:13:09 | 200 |     15.9862ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:13:09 | 200 |      18.495ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:13:11 | 200 |     14.9371ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:13:11 | 200 |     15.4665ms |       127.0.0.1 | GET      "/api/tags"
time=2025-08-25T10:22:21.197-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0196855 runner.name=registry.ollama.ai/library/nexus-ai:latest runner.inference=cuda runner.devices=1 runner.size="11.2 GiB" runner.vram="5.5 GiB" runner.parallel=1 runner.pid=31256 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2 runner.num_ctx=32768
time=2025-08-25T10:22:21.447-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2695667 runner.name=registry.ollama.ai/library/nexus-ai:latest runner.inference=cuda runner.devices=1 runner.size="11.2 GiB" runner.vram="5.5 GiB" runner.parallel=1 runner.pid=31256 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2 runner.num_ctx=32768
time=2025-08-25T10:32:17.187-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=601.0094989 runner.size="11.2 GiB" runner.vram="5.5 GiB" runner.parallel=1 runner.pid=31256 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
time=2025-08-25T10:37:44.151-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="61.1 GiB" free_swap="57.8 GiB"
time=2025-08-25T10:37:44.152-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=7 layers.split="" memory.available="[1.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.4 GiB" memory.required.partial="1.6 GiB" memory.required.kv="448.0 MiB" memory.required.allocations="[1.6 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="256.5 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-25T10:37:45.291-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 4096 --batch-size 512 --n-gpu-layers 7 --threads 4 --no-mmap --parallel 1 --port 64208"
time=2025-08-25T10:37:45.308-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-25T10:37:45.308-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-25T10:37:45.309-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-25T10:37:45.351-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-25T10:37:45.451-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-25T10:37:45.452-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:64208"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 2459 MiB free
time=2025-08-25T10:37:45.562-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 7 repeating layers to GPU
load_tensors: offloaded 7/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:    CUDA_Host model buffer size =  1505.39 MiB
load_tensors:        CUDA0 model buffer size =   412.97 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:        CPU KV buffer size =   336.00 MiB
llama_kv_cache_unified:      CUDA0 KV buffer size =   112.00 MiB
llama_kv_cache_unified: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context:      CUDA0 compute buffer size =   570.73 MiB
llama_context:  CUDA_Host compute buffer size =    14.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 236 (with bs=512), 3 (with bs=1)
time=2025-08-25T10:37:46.568-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.26 seconds"
[GIN] 2025/08/25 - 10:37:59 | 200 |    16.026516s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 10:39:30 | 200 |    1.9714081s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 10:39:47 | 200 |    2.0891094s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 10:40:16 | 200 |   13.4401376s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 10:42:35 | 200 |      6.5828ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:35 | 200 |      9.1047ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:37 | 200 |     11.1523ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:37 | 200 |      12.667ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:39 | 200 |     15.0479ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:39 | 200 |     15.0479ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:41 | 200 |     23.5656ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:41 | 200 |     24.1678ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:43 | 200 |     14.5667ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:43 | 200 |     18.2441ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:45 | 200 |     12.6939ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:45 | 200 |     13.6894ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:47 | 200 |     13.7503ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:47 | 200 |      20.082ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:49 | 200 |      16.942ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:49 | 200 |     16.5603ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:51 | 200 |     10.3027ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:51 | 200 |     12.7686ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:53 | 200 |     11.5495ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:53 | 200 |     13.1281ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:55 | 200 |     13.6382ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:55 | 200 |     13.6382ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:57 | 200 |     14.9721ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:57 | 200 |     14.8497ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:59 | 200 |     19.5495ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:42:59 | 200 |     19.5495ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:01 | 200 |     15.2612ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:01 | 200 |     15.8152ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:03 | 200 |     19.9953ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:03 | 200 |     20.9981ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:05 | 200 |     17.9722ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:05 | 200 |     20.5109ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:07 | 200 |     21.1543ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:07 | 200 |     21.2581ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:09 | 200 |     12.4425ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:09 | 200 |     15.5813ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:11 | 200 |     13.5349ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:11 | 200 |     12.0223ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:56 | 200 |     20.3084ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:56 | 200 |     21.1241ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:58 | 200 |     11.4379ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:43:58 | 200 |     10.4184ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:44:10 | 200 |     10.5963ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:44:10 | 200 |     11.6163ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:44:12 | 200 |      17.335ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 10:44:12 | 200 |      17.335ms |       127.0.0.1 | GET      "/api/tags"
time=2025-08-25T10:44:40.856-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="385.9 MiB"
time=2025-08-25T10:44:45.879-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0197106 runner.size="3.4 GiB" runner.vram="1.6 GiB" runner.parallel=1 runner.pid=35480 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
time=2025-08-25T10:44:46.048-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="60.8 GiB" free_swap="58.5 GiB"
time=2025-08-25T10:44:46.048-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[1.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.8 GiB" memory.required.partial="0 B" memory.required.kv="1.8 GiB" memory.required.allocations="[0 B]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="1.8 GiB" memory.graph.partial="2.3 GiB"
time=2025-08-25T10:44:46.128-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2680775 runner.size="3.4 GiB" runner.vram="1.6 GiB" runner.parallel=1 runner.pid=35480 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
time=2025-08-25T10:44:46.379-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5191808 runner.size="3.4 GiB" runner.vram="1.6 GiB" runner.parallel=1 runner.pid=35480 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-25T10:44:46.785-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 32768 --batch-size 512 --threads 4 --no-mmap --parallel 1 --port 64680"
time=2025-08-25T10:44:46.800-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-25T10:44:46.800-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-25T10:44:46.801-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-25T10:44:46.859-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
time=2025-08-25T10:44:46.872-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(clang)
time=2025-08-25T10:44:46.873-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:64680"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
time=2025-08-25T10:44:47.052-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors:          CPU model buffer size =  4460.45 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32768
llama_context: n_ctx_per_seq = 32768
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 32768, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:        CPU KV buffer size =  1792.00 MiB
llama_kv_cache_unified: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB
llama_context:        CPU compute buffer size =  1884.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 1
time=2025-08-25T10:44:48.812-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.01 seconds"
time=2025-08-25T10:49:46.237-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:64680/completion\": context canceled"
[GIN] 2025/08/25 - 10:49:46 | 200 |          5m5s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-25T10:50:17.270-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:64680/completion\": context canceled"
time=2025-08-25T10:50:17.270-04:00 level=INFO source=runner.go:590 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/25 - 10:50:17 | 200 |          5m5s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-25T10:50:49.164-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:64680/completion\": context canceled"
time=2025-08-25T10:50:49.164-04:00 level=INFO source=runner.go:590 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/25 - 10:50:49 | 200 |          5m5s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-25T10:54:54.853-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
time=2025-08-25T10:54:54.853-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
time=2025-08-25T10:54:54.854-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:64680/completion\": context canceled"
[GIN] 2025/08/25 - 10:54:54 | 200 |   51.5656962s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 10:54:54 | 200 |   19.5555776s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 10:54:54 | 200 |         1m22s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-25T10:54:54.854-04:00 level=INFO source=runner.go:590 msg="aborting completion request due to client closing the connection"
time=2025-08-25T10:59:59.877-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0230759 runner.size="5.8 GiB" runner.vram="0 B" runner.parallel=1 runner.pid=37436 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463
time=2025-08-25T11:00:00.127-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.273127 runner.size="5.8 GiB" runner.vram="0 B" runner.parallel=1 runner.pid=37436 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463
time=2025-08-25T11:00:00.376-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5224185 runner.size="5.8 GiB" runner.vram="0 B" runner.parallel=1 runner.pid=37436 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463
[GIN] 2025/08/25 - 11:10:05 | 200 |     17.6491ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:10:05 | 200 |     19.5261ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:10:07 | 200 |      14.239ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:10:07 | 200 |     15.2448ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:10:12 | 200 |      9.0536ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:10:12 | 200 |     14.1524ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:10:17 | 200 |     30.3643ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:10:17 | 200 |     25.5118ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:10:19 | 200 |     13.5608ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:10:19 | 200 |     12.7797ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:30 | 200 |     16.0717ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:30 | 200 |     16.8984ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:30 | 200 |     28.8326ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:30 | 200 |     28.8326ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:30 | 200 |     30.3542ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:30 | 200 |     29.3488ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:32 | 200 |      12.139ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:32 | 200 |     12.4132ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:34 | 200 |     14.3504ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:34 | 200 |     14.1253ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:36 | 200 |      28.171ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:36 | 200 |      28.171ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:38 | 200 |      21.196ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:38 | 200 |     20.9204ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:40 | 200 |     15.8054ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:40 | 200 |      18.074ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:42 | 200 |     20.1102ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:42 | 200 |     20.7284ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:44 | 200 |     16.1273ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:44 | 200 |     22.2934ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:46 | 200 |     13.9726ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:46 | 200 |      16.997ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:48 | 200 |     14.1448ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:48 | 200 |     15.1475ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:50 | 200 |     17.0922ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:50 | 200 |     19.0929ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:52 | 200 |      11.151ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:52 | 200 |     11.3879ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:54 | 200 |     17.0495ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:54 | 200 |     17.0495ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:56 | 200 |     14.0833ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:13:56 | 200 |     14.0718ms |       127.0.0.1 | GET      "/api/tags"
time=2025-08-25T11:19:28.709-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="75.1 GiB" free_swap="77.0 GiB"
time=2025-08-25T11:19:28.711-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=18 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="11.2 GiB" memory.required.partial="5.1 GiB" memory.required.kv="2.5 GiB" memory.required.allocations="[5.1 GiB]" memory.weights.total="6.8 GiB" memory.weights.repeating="6.0 GiB" memory.weights.nonrepeating="787.5 MiB" memory.graph.full="1.1 GiB" memory.graph.partial="1.4 GiB"
time=2025-08-25T11:19:28.871-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2 --ctx-size 32768 --batch-size 512 --n-gpu-layers 18 --threads 4 --no-mmap --parallel 1 --port 51678"
time=2025-08-25T11:19:28.884-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-25T11:19:28.884-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-25T11:19:28.885-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-25T11:19:28.957-04:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-25T11:19:28.958-04:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:51678"
time=2025-08-25T11:19:28.982-04:00 level=INFO source=ggml.go:92 msg="" architecture=gemma3 file_type=Q4_K_M name="Gemma 3 12b It" description="" num_tensors=626 num_key_values=41
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-25T11:19:29.085-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-25T11:19:29.137-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-25T11:19:29.156-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CPU size="5.3 GiB"
time=2025-08-25T11:19:29.156-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CUDA0 size="2.3 GiB"
time=2025-08-25T11:19:29.527-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="1.1 GiB"
time=2025-08-25T11:19:29.527-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="1.0 GiB"
time=2025-08-25T11:19:30.648-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/25 - 11:20:36 | 200 |     12.5058ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:20:36 | 200 |     17.0383ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:20:36 | 200 |     15.9531ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:20:36 | 200 |     23.0522ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:20:36 | 200 |     22.0475ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:20:36 | 200 |     22.9926ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:20:38 | 200 |     16.5737ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:20:38 | 200 |     18.1159ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:20:40 | 200 |      9.4725ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:20:40 | 200 |     10.9861ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:20:42 | 200 |     13.9202ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:20:42 | 200 |       19.85ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:24:01 | 200 |     13.2489ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:24:01 | 200 |     26.4101ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:24:02 | 200 |      13.041ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:24:02 | 200 |     23.2204ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:24:02 | 200 |      35.582ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:24:02 | 200 |     33.6606ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:24:03 | 200 |     14.6452ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 11:24:03 | 200 |     19.8203ms |       127.0.0.1 | GET      "/api/tags"
time=2025-08-25T11:24:34.404-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:51678/completion\": context canceled"
[GIN] 2025/08/25 - 11:24:34 | 200 |          5m5s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/25 - 11:25:44 | 200 |         5m45s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-25T11:25:44.679-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/25 - 11:28:43 | 200 |         8m12s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-25T11:28:43.498-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/25 - 12:04:33 | 200 |        42m12s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-25T12:04:33.362-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/25 - 12:14:36 | 200 |     12.0511ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 12:14:36 | 200 |     14.4681ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 12:14:42 | 200 |     12.5934ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 12:14:42 | 200 |     16.7886ms |             ::1 | GET      "/api/tags"
time=2025-08-25T12:14:52.375-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-25T12:14:52.541-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=6016843776 required="3.7 GiB"
time=2025-08-25T12:14:52.571-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="75.6 GiB" free_swap="78.4 GiB"
time=2025-08-25T12:14:52.572-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[5.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-25T12:14:53.624-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 4 --no-mmap --parallel 2 --port 54628"
time=2025-08-25T12:14:53.637-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-25T12:14:53.637-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-25T12:14:53.641-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-25T12:14:53.692-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-25T12:14:53.800-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-25T12:14:53.800-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:54628"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-25T12:14:53.900-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB
llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-08-25T12:14:54.911-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.27 seconds"
[GIN] 2025/08/25 - 12:14:58 | 200 |     5.966524s |             ::1 | POST     "/api/generate"
time=2025-08-25T12:15:17.501-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/25 - 12:15:21 | 200 |    4.0777969s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/25 - 12:20:27 | 200 |     12.6589ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 12:20:29 | 200 |     20.6041ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 12:20:29 | 200 |     12.2944ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 12:26:42 | 200 |       1h4m50s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-25T12:26:42.268-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/25 - 12:31:05 | 200 |       1h8m42s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-25T12:31:05.192-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/25 - 12:32:59 | 200 |      8.6335ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 12:32:59 | 200 |     15.9132ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 12:33:51 | 200 |     18.8331ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 12:33:51 | 200 |     11.8815ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 12:36:00 | 200 |       1h13m8s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-25T12:36:00.244-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/25 - 12:40:13 | 200 |      1h17m18s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-25T12:40:13.641-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/25 - 12:45:42 | 200 |      1h22m18s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-25T12:45:42.452-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/25 - 13:29:21 | 200 |     15.2533ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:29:21 | 200 |      6.1205ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:29:52 | 200 |     12.1397ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:29:52 | 200 |      9.8924ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:36:33 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/25 - 13:36:33 | 200 |     12.9593ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:37:34 | 200 |     15.1991ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:37:34 | 200 |      9.7879ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:43:30 | 200 |      9.0504ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:43:33 | 200 |      7.2987ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:43:33 | 200 |      6.5528ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:43:59 | 200 |     14.8017ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:43:59 | 200 |     14.7848ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:48:01 | 200 |     11.9491ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:48:04 | 200 |      8.9632ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:48:04 | 200 |     13.1888ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:48:23 | 200 |     14.5144ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:48:23 | 200 |     12.1427ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:50:22 | 200 |     17.2552ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:50:25 | 200 |      6.6912ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:50:25 | 200 |       11.75ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:50:41 | 200 |     10.7871ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:50:41 | 200 |     17.3609ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:50:51 | 200 |     13.0752ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/25 - 13:50:51 | 200 |     11.1008ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:07:04 | 200 |    331.4886ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:07:04 | 200 |     117.548ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:35:04 | 200 |     11.7545ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:35:04 | 200 |     15.4671ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:11 | 200 |     14.3872ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:13 | 200 |     13.7526ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:15 | 200 |     13.6871ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:17 | 200 |     12.5485ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:19 | 200 |     13.6581ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:21 | 200 |      9.6172ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:23 | 200 |     12.5246ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:25 | 200 |     11.7319ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:27 | 200 |     10.3363ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:29 | 200 |     14.8597ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:31 | 200 |      8.5396ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:33 | 200 |     10.3204ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:35 | 200 |     20.7621ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:37 | 200 |     11.7398ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:39 | 200 |     13.0488ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:41 | 200 |      9.5739ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:43 | 200 |     19.0064ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:42:45 | 200 |      9.1825ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:44:26 | 200 |     10.7546ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:44:27 | 200 |     11.0469ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:44:29 | 200 |     12.8064ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:44:31 | 200 |     10.4645ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:44:33 | 200 |     13.5825ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 14:44:35 | 200 |     17.0105ms |       127.0.0.1 | GET      "/api/tags"
time=2025-08-26T14:44:39.858-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=6232739840 required="3.7 GiB"
time=2025-08-26T14:44:39.877-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="75.6 GiB" free_swap="79.1 GiB"
time=2025-08-26T14:44:39.878-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[5.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-26T14:44:40.905-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 4 --no-mmap --parallel 2 --port 64694"
time=2025-08-26T14:44:40.921-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-26T14:44:40.921-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-26T14:44:40.922-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-26T14:44:41.120-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-26T14:44:41.443-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-26T14:44:41.447-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:64694"
time=2025-08-26T14:44:41.698-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB
llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-08-26T14:44:45.536-04:00 level=INFO source=server.go:630 msg="llama runner started in 4.61 seconds"
time=2025-08-26T14:44:45.775-04:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=10447 keep=5 new=4096
[GIN] 2025/08/26 - 14:45:28 | 200 |   48.4593657s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/08/26 - 21:19:36 | 200 |     15.5511ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:19:38 | 200 |     15.3629ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:19:40 | 200 |       16.54ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:19:42 | 200 |     17.4054ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:19:44 | 200 |     20.0499ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:19:46 | 200 |     15.6372ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:19:48 | 200 |     19.1595ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:19:50 | 200 |     19.4675ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:19:54 | 400 |     86.8311ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/08/26 - 21:20:01 | 200 |     14.7813ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:20:03 | 200 |      17.956ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:20:05 | 200 |     10.2101ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:20:07 | 200 |     14.0755ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:20:09 | 200 |     14.1295ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:20:11 | 200 |     14.9976ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:20:13 | 200 |     13.5296ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:20:16 | 400 |     78.9328ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/08/26 - 21:20:24 | 200 |     12.5408ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:20:26 | 200 |     11.8526ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:20:28 | 200 |     14.0635ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:20:30 | 400 |     79.6269ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/08/26 - 21:34:27 | 200 |     11.6433ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:34:29 | 200 |     10.0414ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:34:31 | 200 |     14.0553ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:34:33 | 200 |     14.9178ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:34:35 | 200 |     11.0987ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:02 | 200 |     12.0779ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:04 | 200 |     14.2349ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:06 | 200 |     12.4728ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:08 | 200 |     20.9253ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:10 | 200 |     13.7504ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:12 | 200 |      19.321ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:14 | 200 |     17.6936ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:16 | 200 |     10.7255ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:18 | 200 |      14.662ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:20 | 200 |     11.4031ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:22 | 200 |     15.6068ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:24 | 200 |     18.2151ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:26 | 200 |     20.1419ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:28 | 200 |     12.9885ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:30 | 200 |     11.9351ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:32 | 200 |     14.6593ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:34 | 200 |      15.046ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:36 | 200 |     14.1625ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:38 | 200 |     15.5875ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:40 | 200 |     17.3869ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:42 | 200 |      15.182ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:44 | 200 |     18.1456ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:46 | 200 |     17.8036ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:48 | 200 |     20.4328ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:50 | 200 |     14.1108ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:52 | 200 |     12.8168ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:54 | 200 |     13.1757ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:56 | 200 |     11.6419ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:35:58 | 200 |      17.187ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:00 | 200 |     14.1788ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:02 | 200 |     13.8525ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:04 | 200 |     12.7401ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:06 | 200 |     17.4769ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:08 | 200 |     36.2986ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:10 | 200 |     10.8518ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:12 | 200 |      15.139ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:14 | 200 |      12.074ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:16 | 200 |     16.5199ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:18 | 200 |     18.1529ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:20 | 200 |     13.6121ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:22 | 200 |     13.6844ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:24 | 200 |      12.746ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:26 | 200 |     12.0635ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:28 | 200 |     15.6432ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:30 | 200 |     20.4585ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:32 | 200 |     18.7924ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:34 | 200 |     19.5834ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:36 | 200 |     12.0998ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:38 | 200 |     13.2521ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:40 | 200 |     20.1134ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:42 | 200 |     12.9089ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:44 | 200 |      17.999ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:46 | 200 |     12.6595ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:48 | 200 |     20.6775ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:50 | 200 |     16.1003ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:52 | 200 |     17.5824ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:54 | 200 |     12.9515ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:56 | 200 |     15.4895ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:36:58 | 200 |     11.7096ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:00 | 200 |      18.253ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:02 | 200 |     21.7358ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:04 | 200 |     18.0864ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:06 | 200 |      17.499ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:08 | 200 |     14.3495ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:10 | 200 |     12.8617ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:12 | 200 |     13.5813ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:14 | 200 |     16.5679ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:16 | 200 |     14.3847ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:18 | 200 |     15.1719ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:20 | 200 |     11.0884ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:22 | 200 |     12.5974ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:24 | 200 |     14.6066ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:26 | 200 |     13.6275ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:28 | 200 |     11.2461ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:30 | 200 |     13.3082ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:32 | 200 |     16.0128ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:34 | 200 |      11.716ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:36 | 200 |     15.6118ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:38 | 200 |     16.0774ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:48 | 200 |     13.6974ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:50 | 200 |      9.8004ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:52 | 200 |     17.2336ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:54 | 200 |     11.8535ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:56 | 200 |       9.675ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:37:58 | 200 |     13.5107ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:49:17 | 200 |     12.4216ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:49:20 | 200 |      8.6029ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:49:20 | 200 |     14.4089ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:51:05 | 200 |       10.58ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 21:51:05 | 200 |      14.904ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:02:13 | 200 |     11.0964ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:02:13 | 200 |     14.9149ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:04:56 | 200 |     16.0356ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:04:56 | 200 |     12.9783ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:10:14 | 200 |      7.8965ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:10:14 | 200 |       11.98ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:19:23 | 200 |     15.4219ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:19:23 | 200 |     13.9992ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:23:20 | 200 |     14.0362ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:23:20 | 200 |     12.8565ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:23:25 | 200 |      18.098ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:23:25 | 200 |     16.0355ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:26:52 | 200 |     24.2464ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:26:52 | 200 |     12.1008ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:27:17 | 200 |     22.2961ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:27:17 | 200 |     16.2397ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:27:29 | 200 |     10.8209ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:27:29 | 200 |      5.9956ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:37:44 | 200 |     10.8312ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:37:44 | 200 |     11.6827ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:58:55 | 200 |     14.2439ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:58:56 | 200 |     10.3093ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:58:58 | 200 |     11.8189ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:59:00 | 200 |     14.5473ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/26 - 22:59:02 | 200 |     14.3728ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/27 - 11:12:13 | 200 |     98.6944ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 11:12:13 | 200 |      20.213ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 11:12:25 | 200 |      35.231ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 11:12:25 | 200 |     31.2334ms |             ::1 | GET      "/api/tags"
time=2025-08-27T11:12:44.549-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-27T11:12:44.716-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=6118404096 required="3.7 GiB"
time=2025-08-27T11:12:44.735-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="67.7 GiB" free_swap="70.2 GiB"
time=2025-08-27T11:12:44.736-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[5.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-27T11:12:45.740-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 4 --no-mmap --parallel 2 --port 59744"
time=2025-08-27T11:12:45.761-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-27T11:12:45.761-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-27T11:12:45.763-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-27T11:12:45.994-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-27T11:12:46.190-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-27T11:12:46.191-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:59744"
time=2025-08-27T11:12:46.266-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB
llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-08-27T11:12:48.026-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.26 seconds"
[GIN] 2025/08/27 - 11:12:51 | 200 |    7.0506186s |             ::1 | POST     "/api/generate"
time=2025-08-27T11:13:14.955-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 11:13:23 | 200 |    8.2727545s |             ::1 | POST     "/api/generate"
time=2025-08-27T11:13:42.496-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 11:13:49 | 200 |    7.1368526s |             ::1 | POST     "/api/generate"
time=2025-08-27T11:14:03.537-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 11:14:13 | 200 |    9.8131534s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/27 - 18:51:56 | 200 |      9.0414ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 18:51:56 | 200 |     11.4371ms |             ::1 | GET      "/api/tags"
time=2025-08-27T18:52:22.005-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-27T18:52:22.126-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5812834304 required="3.7 GiB"
time=2025-08-27T18:52:22.151-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="71.1 GiB" free_swap="73.6 GiB"
time=2025-08-27T18:52:22.151-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-27T18:52:22.839-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 4 --no-mmap --parallel 2 --port 62880"
time=2025-08-27T18:52:22.855-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-27T18:52:22.855-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-27T18:52:22.855-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-27T18:52:22.914-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-27T18:52:23.020-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-27T18:52:23.021-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:62880"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-27T18:52:23.107-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB
llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-08-27T18:52:24.115-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.26 seconds"
[GIN] 2025/08/27 - 18:52:27 | 200 |    5.5103919s |             ::1 | POST     "/api/generate"
time=2025-08-27T18:52:48.371-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 18:52:51 | 200 |    3.6620706s |             ::1 | POST     "/api/generate"
time=2025-08-27T18:53:13.806-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 18:53:17 | 200 |    3.6506709s |             ::1 | POST     "/api/generate"
time=2025-08-27T18:53:51.342-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 18:53:54 | 200 |    2.9593523s |             ::1 | POST     "/api/generate"
time=2025-08-27T18:54:24.116-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 18:54:32 | 200 |    8.8272804s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/27 - 19:00:29 | 200 |     16.0098ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:00:29 | 200 |     10.9222ms |             ::1 | GET      "/api/tags"
time=2025-08-27T19:01:10.679-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-27T19:01:10.833-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=6113390592 required="3.7 GiB"
time=2025-08-27T19:01:10.856-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="71.2 GiB" free_swap="74.1 GiB"
time=2025-08-27T19:01:10.857-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[5.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-27T19:01:11.842-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 4 --no-mmap --parallel 2 --port 63337"
time=2025-08-27T19:01:11.856-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-27T19:01:11.856-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-27T19:01:11.857-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-27T19:01:11.919-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-27T19:01:12.024-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-27T19:01:12.025-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:63337"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-27T19:01:12.109-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB
llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-08-27T19:01:13.117-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.26 seconds"
[GIN] 2025/08/27 - 19:01:16 | 200 |     6.058359s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/27 - 19:06:29 | 200 |     16.5163ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:06:29 | 200 |      15.777ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:12:49 | 200 |     15.5254ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:14:15 | 200 |     13.1534ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:15:09 | 200 |     12.5023ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:16:49 | 200 |       18.05ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:19:58 | 200 |     13.2169ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:19:58 | 200 |      13.652ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:20:13 | 200 |     14.5429ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:20:13 | 200 |     13.7378ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:22:21 | 200 |      7.4546ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:22:21 | 200 |      9.5194ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:24:41 | 200 |     16.5232ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:24:41 | 200 |     10.7197ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:25:16 | 200 |     10.9838ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:25:16 | 200 |     15.8412ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:27:41 | 200 |     13.4385ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:27:41 | 200 |     12.1089ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:30:46 | 200 |     12.1176ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:30:46 | 200 |     16.2918ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:32:58 | 200 |      8.3374ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:33:00 | 200 |     13.0658ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:33:02 | 200 |      15.797ms |       127.0.0.1 | GET      "/api/tags"
time=2025-08-27T19:33:05.996-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=6000996352 required="3.7 GiB"
time=2025-08-27T19:33:06.016-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="69.3 GiB" free_swap="71.6 GiB"
time=2025-08-27T19:33:06.017-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[5.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-27T19:33:06.929-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 4 --no-mmap --parallel 2 --port 65383"
time=2025-08-27T19:33:06.945-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-27T19:33:06.945-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-27T19:33:06.946-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-27T19:33:07.077-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-27T19:33:07.182-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-27T19:33:07.183-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:65383"
time=2025-08-27T19:33:07.198-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB
llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-08-27T19:33:08.205-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.26 seconds"
time=2025-08-27T19:33:08.240-04:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=10983 keep=5 new=4096
[GIN] 2025/08/27 - 19:33:18 | 200 |   12.9883011s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/08/27 - 19:50:47 | 200 |     18.9898ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:50:49 | 200 |     11.0801ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:50:51 | 200 |      12.878ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:50:53 | 200 |     14.4517ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:50:55 | 200 |     10.5352ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:50:57 | 200 |     12.0459ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:50:59 | 200 |     14.5133ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/27 - 19:51:01 | 200 |     16.2352ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:06:53 | 200 |     11.1017ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:06:53 | 200 |     11.7743ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:07:24 | 200 |     15.0939ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:07:24 | 200 |      9.5739ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:07:42 | 200 |     15.7178ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:07:42 | 200 |      9.0431ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:10:55 | 200 |     19.3832ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:10:55 | 200 |     16.7512ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:11:28 | 200 |      10.966ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:11:28 | 200 |     11.1818ms |             ::1 | GET      "/api/tags"
time=2025-08-27T20:12:55.494-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-27T20:12:55.638-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5966819328 required="3.7 GiB"
time=2025-08-27T20:12:55.661-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="69.9 GiB" free_swap="72.6 GiB"
time=2025-08-27T20:12:55.662-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[5.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-27T20:12:56.555-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 4 --no-mmap --parallel 2 --port 62867"
time=2025-08-27T20:12:56.572-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-27T20:12:56.573-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-27T20:12:56.574-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-27T20:12:56.626-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-27T20:12:56.724-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-27T20:12:56.725-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:62867"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
time=2025-08-27T20:12:56.825-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB
llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-08-27T20:12:57.829-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.26 seconds"
[GIN] 2025/08/27 - 20:12:58 | 200 |    3.2452528s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/27 - 20:13:44 | 200 |     10.0618ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:13:44 | 200 |      9.3235ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:22:25 | 200 |     16.1598ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:22:25 | 200 |     11.0802ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:22:40 | 200 |     12.1463ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:22:40 | 200 |     14.1768ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:22:57 | 200 |     11.5942ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:22:57 | 200 |      15.575ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:24:26 | 200 |     14.1118ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:24:26 | 200 |     21.4986ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:27:38 | 200 |     17.7708ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:27:38 | 200 |     13.6203ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:27:43 | 200 |     14.4244ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:27:43 | 200 |     15.2219ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:29:05 | 200 |     12.2926ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:29:05 | 200 |     13.2423ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:29:44 | 200 |     10.5372ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:29:44 | 200 |     13.8789ms |             ::1 | GET      "/api/tags"
time=2025-08-27T20:30:29.890-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-27T20:30:30.028-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5948911616 required="3.7 GiB"
time=2025-08-27T20:30:30.059-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="68.9 GiB" free_swap="70.6 GiB"
time=2025-08-27T20:30:30.062-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[5.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-27T20:30:30.947-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 4 --no-mmap --parallel 2 --port 56001"
time=2025-08-27T20:30:30.963-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-27T20:30:30.963-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-27T20:30:30.966-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-27T20:30:31.020-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-27T20:30:31.138-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-27T20:30:31.139-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:56001"
time=2025-08-27T20:30:31.221-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB
llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-08-27T20:30:32.229-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.27 seconds"
[GIN] 2025/08/27 - 20:30:32 | 200 |    3.1111408s |             ::1 | POST     "/api/generate"
time=2025-08-27T20:30:37.676-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 20:30:38 | 200 |    882.0665ms |             ::1 | POST     "/api/generate"
time=2025-08-27T20:30:54.169-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 20:30:55 | 200 |    1.4497285s |             ::1 | POST     "/api/generate"
time=2025-08-27T20:31:24.892-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 20:31:25 | 200 |    915.7766ms |             ::1 | POST     "/api/generate"
time=2025-08-27T20:31:52.742-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 20:31:53 | 200 |    1.1263456s |             ::1 | POST     "/api/generate"
time=2025-08-27T20:32:03.060-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 20:32:04 | 200 |    1.4308634s |             ::1 | POST     "/api/generate"
time=2025-08-27T20:32:52.234-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 20:32:57 | 200 |    5.2853394s |             ::1 | POST     "/api/generate"
time=2025-08-27T20:33:19.636-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 20:33:26 | 200 |    6.7685105s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/27 - 20:35:29 | 200 |     13.0331ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:35:29 | 200 |     12.5105ms |             ::1 | GET      "/api/tags"
time=2025-08-27T20:35:42.393-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 20:35:48 | 200 |    5.8234387s |             ::1 | POST     "/api/generate"
time=2025-08-27T20:40:53.193-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0357528 runner.size="3.7 GiB" runner.vram="3.7 GiB" runner.parallel=2 runner.pid=25516 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
time=2025-08-27T20:40:53.442-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2845807 runner.size="3.7 GiB" runner.vram="3.7 GiB" runner.parallel=2 runner.pid=25516 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
time=2025-08-27T20:40:53.692-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5349433 runner.size="3.7 GiB" runner.vram="3.7 GiB" runner.parallel=2 runner.pid=25516 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
[GIN] 2025/08/27 - 20:42:10 | 200 |     15.0677ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:42:10 | 200 |     15.1242ms |             ::1 | GET      "/api/tags"
time=2025-08-27T20:42:22.722-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-27T20:42:22.893-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="67.3 GiB" free_swap="65.5 GiB"
time=2025-08-27T20:42:22.894-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=26 layers.split="" memory.available="[2.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.4 GiB" memory.required.partial="2.9 GiB" memory.required.kv="448.0 MiB" memory.required.allocations="[2.9 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="256.5 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-27T20:42:23.898-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 4096 --batch-size 512 --n-gpu-layers 26 --threads 4 --no-mmap --parallel 1 --port 56762"
time=2025-08-27T20:42:23.916-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-27T20:42:23.916-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-27T20:42:23.918-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-27T20:42:23.973-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-27T20:42:24.080-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-27T20:42:24.081-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:56762"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-27T20:42:24.170-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:    CUDA_Host model buffer size =   430.21 MiB
load_tensors:        CUDA0 model buffer size =  1488.14 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:        CPU KV buffer size =    32.00 MiB
llama_kv_cache_unified:      CUDA0 KV buffer size =   416.00 MiB
llama_kv_cache_unified: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context:      CUDA0 compute buffer size =   564.73 MiB
llama_context:  CUDA_Host compute buffer size =    14.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 27 (with bs=512), 3 (with bs=1)
time=2025-08-27T20:42:25.176-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.26 seconds"
[GIN] 2025/08/27 - 20:42:32 | 200 |    9.6304856s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/27 - 20:44:39 | 200 |     14.8378ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 20:44:39 | 200 |     11.8285ms |             ::1 | GET      "/api/tags"
time=2025-08-27T20:44:43.296-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 20:44:58 | 200 |   15.6251695s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/27 - 21:47:50 | 200 |     21.3452ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 21:47:50 | 200 |       9.229ms |             ::1 | GET      "/api/tags"
time=2025-08-27T21:47:57.666-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-27T21:47:57.824-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="63.3 GiB" free_swap="59.2 GiB"
time=2025-08-27T21:47:57.827-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=25 layers.split="" memory.available="[2.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.4 GiB" memory.required.partial="2.9 GiB" memory.required.kv="448.0 MiB" memory.required.allocations="[2.9 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="256.5 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-27T21:47:58.824-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 4096 --batch-size 512 --n-gpu-layers 25 --threads 4 --no-mmap --parallel 1 --port 59890"
time=2025-08-27T21:47:58.841-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-27T21:47:58.841-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-27T21:47:58.844-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-27T21:47:58.951-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-27T21:47:59.077-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-27T21:47:59.077-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:59890"
time=2025-08-27T21:47:59.101-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 25 repeating layers to GPU
load_tensors: offloaded 25/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   484.24 MiB
load_tensors:        CUDA0 model buffer size =  1434.12 MiB
load_tensors:          CPU model buffer size =   308.23 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   400.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    48.00 MiB
llama_kv_cache_unified: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context:      CUDA0 compute buffer size =   570.73 MiB
llama_context:  CUDA_Host compute buffer size =    14.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 38 (with bs=512), 3 (with bs=1)
time=2025-08-27T21:48:00.356-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.52 seconds"
[GIN] 2025/08/27 - 21:48:01 | 200 |    3.9510204s |             ::1 | POST     "/api/generate"
time=2025-08-27T21:48:19.812-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 21:48:27 | 200 |    7.6937758s |             ::1 | POST     "/api/generate"
time=2025-08-27T21:50:07.565-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 21:50:20 | 200 |   13.0241138s |             ::1 | POST     "/api/generate"
time=2025-08-27T21:50:50.704-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/27 - 21:50:54 | 200 |     3.919559s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/27 - 22:01:51 | 200 |     18.6736ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 22:01:51 | 200 |     15.9993ms |             ::1 | GET      "/api/tags"
time=2025-08-27T22:02:24.716-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-27T22:02:24.884-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="61.3 GiB" free_swap="57.1 GiB"
time=2025-08-27T22:02:24.885-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=25 layers.split="" memory.available="[2.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.4 GiB" memory.required.partial="2.9 GiB" memory.required.kv="448.0 MiB" memory.required.allocations="[2.9 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="256.5 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-27T22:02:25.853-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 4096 --batch-size 512 --n-gpu-layers 25 --threads 4 --no-mmap --parallel 1 --port 61255"
time=2025-08-27T22:02:25.864-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-27T22:02:25.864-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-27T22:02:25.866-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-27T22:02:25.933-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-27T22:02:26.036-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-27T22:02:26.037-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:61255"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-27T22:02:26.123-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 25 repeating layers to GPU
load_tensors: offloaded 25/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   484.24 MiB
load_tensors:        CUDA0 model buffer size =  1434.12 MiB
load_tensors:          CPU model buffer size =   308.23 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   400.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    48.00 MiB
llama_kv_cache_unified: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context:      CUDA0 compute buffer size =   570.73 MiB
llama_context:  CUDA_Host compute buffer size =    14.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 38 (with bs=512), 3 (with bs=1)
time=2025-08-27T22:02:27.132-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.27 seconds"
[GIN] 2025/08/27 - 22:02:38 | 200 |   14.2143058s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/27 - 22:03:36 | 200 |     11.5897ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 22:03:36 | 200 |     15.5979ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 22:06:19 | 200 |     16.6256ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 22:06:19 | 200 |     10.1258ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 22:18:55 | 200 |     17.2585ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 22:18:55 | 200 |     12.7192ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/27 - 22:36:29 | 200 |     60h24m44s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-27T22:36:29.313-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/27 - 23:35:14 | 200 |      61h24m0s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-27T23:35:14.051-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/28 - 00:07:16 | 200 |     61h54m59s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-28T00:07:16.311-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/28 - 18:57:16 | 200 |      9.2226ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 18:57:16 | 200 |      8.3434ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 19:04:34 | 200 |      7.0885ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 19:04:34 | 200 |      7.4664ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 19:05:19 | 200 |      6.3475ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 19:05:19 | 200 |      8.1774ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 19:21:36 | 200 |      9.7598ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 19:21:36 | 200 |      8.9954ms |             ::1 | GET      "/api/tags"
time=2025-08-28T19:21:43.934-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-28T19:21:44.042-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5889318912 required="3.7 GiB"
time=2025-08-28T19:21:44.057-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="56.7 GiB" free_swap="56.4 GiB"
time=2025-08-28T19:21:44.058-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[5.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-28T19:21:44.800-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 4 --no-mmap --parallel 2 --port 60459"
time=2025-08-28T19:21:44.814-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-28T19:21:44.814-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-28T19:21:44.816-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-28T19:21:44.917-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-28T19:21:45.043-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-28T19:21:45.044-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:60459"
time=2025-08-28T19:21:45.067-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB
llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-08-28T19:21:46.069-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.25 seconds"
[GIN] 2025/08/28 - 19:21:46 | 200 |    2.9739712s |             ::1 | POST     "/api/generate"
time=2025-08-28T19:21:59.918-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/28 - 19:22:00 | 200 |    856.0992ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/28 - 20:02:13 | 200 |     10.8366ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:02:13 | 200 |      8.5513ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:02:27 | 200 |      4.2325ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:02:27 | 200 |     12.7963ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:03:25 | 200 |      9.8696ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:03:25 | 200 |      7.6697ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:04:13 | 200 |     20.5966ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:04:13 | 200 |      7.8978ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:04:48 | 200 |      7.4037ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:04:48 | 200 |      8.4354ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:05:29 | 200 |       9.504ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:05:29 | 200 |      8.8457ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:06:20 | 200 |      7.4449ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:06:20 | 200 |      7.8934ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:10:08 | 200 |      7.2094ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:10:08 | 200 |      8.1307ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:17:19 | 200 |      9.0263ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:17:19 | 200 |      8.3338ms |             ::1 | GET      "/api/tags"
time=2025-08-28T20:25:19.162-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-28T20:25:19.262-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5475479552 required="3.7 GiB"
time=2025-08-28T20:25:19.278-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="61.8 GiB" free_swap="61.6 GiB"
time=2025-08-28T20:25:19.280-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[5.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-28T20:25:20.021-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 4 --no-mmap --parallel 2 --port 57256"
time=2025-08-28T20:25:20.038-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-28T20:25:20.038-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-28T20:25:20.039-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-28T20:25:20.131-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-28T20:25:20.242-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-28T20:25:20.243-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:57256"
time=2025-08-28T20:25:20.290-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB
llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-08-28T20:25:21.295-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.26 seconds"
[GIN] 2025/08/28 - 20:25:22 | 200 |    3.8252658s |             ::1 | POST     "/api/generate"
time=2025-08-28T20:26:17.613-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/28 - 20:26:30 | 200 |   12.6276474s |             ::1 | POST     "/api/generate"
time=2025-08-28T20:27:39.961-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-28T20:27:40.011-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="1.8 GiB"
time=2025-08-28T20:27:40.422-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="62.2 GiB" free_swap="62.1 GiB"
time=2025-08-28T20:27:40.423-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=27 layers.split="" memory.available="[5.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.0 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-28T20:27:40.994-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 27 --threads 4 --no-mmap --parallel 1 --port 57376"
time=2025-08-28T20:27:41.001-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-28T20:27:41.001-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-28T20:27:41.001-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-28T20:27:41.039-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-28T20:27:41.133-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-28T20:27:41.135-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:57376"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-08-28T20:27:41.253-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 27 repeating layers to GPU
load_tensors: offloaded 27/29 layers to GPU
load_tensors:          CPU model buffer size =   292.36 MiB
load_tensors:    CUDA_Host model buffer size =   568.58 MiB
load_tensors:        CUDA0 model buffer size =  3599.51 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:        CPU KV buffer size =     8.00 MiB
llama_kv_cache_unified:      CUDA0 KV buffer size =   216.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 18 (with bs=512), 3 (with bs=1)
time=2025-08-28T20:27:46.013-04:00 level=INFO source=server.go:630 msg="llama runner started in 5.01 seconds"
[GIN] 2025/08/28 - 20:28:34 | 200 |   55.0523035s |             ::1 | POST     "/api/generate"
time=2025-08-28T20:34:09.273-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-28T20:34:09.375-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5505822720 required="3.7 GiB"
time=2025-08-28T20:34:09.402-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="62.5 GiB" free_swap="62.2 GiB"
time=2025-08-28T20:34:09.403-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[5.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-28T20:34:10.163-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 4 --no-mmap --parallel 2 --port 57653"
time=2025-08-28T20:34:10.175-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-28T20:34:10.175-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-28T20:34:10.175-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-28T20:34:10.215-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-28T20:34:10.311-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-28T20:34:10.312-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:57653"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-08-28T20:34:10.427-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB
llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-08-28T20:34:11.179-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.00 seconds"
[GIN] 2025/08/28 - 20:34:28 | 200 |   18.8228682s |             ::1 | POST     "/api/generate"
time=2025-08-28T20:35:16.996-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/28 - 20:35:51 | 200 |   34.0612077s |             ::1 | POST     "/api/generate"
time=2025-08-28T20:36:26.086-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/28 - 20:36:36 | 200 |   10.4233121s |             ::1 | POST     "/api/generate"
time=2025-08-28T20:37:21.565-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/28 - 20:37:32 | 200 |   10.5096209s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/28 - 20:40:33 | 200 |      8.9709ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:40:33 | 200 |     10.4448ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:41:08 | 200 |     42.6459ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:41:08 | 200 |     10.3038ms |             ::1 | GET      "/api/tags"
time=2025-08-28T20:41:19.596-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/28 - 20:41:24 | 200 |    4.8179855s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/28 - 20:45:00 | 200 |      5.3155ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 20:45:00 | 200 |      6.1231ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 21:31:08 | 200 |     13.2506ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/28 - 21:31:08 | 200 |      9.3532ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/29 - 10:59:46 | 200 |     10.5408ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/29 - 10:59:46 | 200 |      8.7536ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/29 - 11:23:13 | 200 |      7.2291ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/29 - 11:23:13 | 200 |      7.7515ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/29 - 12:14:11 | 200 |     11.2194ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/29 - 12:14:11 | 200 |      8.2625ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/29 - 22:42:37 | 200 |      6.6095ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/29 - 22:42:37 | 200 |       6.945ms |             ::1 | GET      "/api/tags"
time=2025-08-29T22:42:50.924-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-29T22:42:51.029-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5867294720 required="3.7 GiB"
time=2025-08-29T22:42:51.053-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="57.5 GiB" free_swap="59.4 GiB"
time=2025-08-29T22:42:51.054-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[5.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-29T22:42:51.818-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 4 --no-mmap --parallel 2 --port 64396"
time=2025-08-29T22:42:51.833-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-29T22:42:51.833-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-29T22:42:51.833-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-29T22:42:51.950-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-29T22:42:52.057-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-29T22:42:52.058-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:64396"
time=2025-08-29T22:42:52.085-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:          CPU model buffer size =   308.23 MiB
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB
llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-08-29T22:42:53.088-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.25 seconds"
[GIN] 2025/08/29 - 22:42:53 | 200 |     3.106362s |             ::1 | POST     "/api/generate"
