time=2025-07-30T11:53:33.051-04:00 level=INFO source=routes.go:1235 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:D:\\UNIFIED_OLLAMA_MODELS OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-07-30T11:53:33.055-04:00 level=INFO source=images.go:480 msg="total blobs: 63"
time=2025-07-30T11:53:33.056-04:00 level=INFO source=images.go:487 msg="total unused blobs removed: 0"
time=2025-07-30T11:53:33.056-04:00 level=INFO source=routes.go:1288 msg="Listening on [::]:11434 (version 0.9.2)"
time=2025-07-30T11:53:33.056-04:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-07-30T11:53:33.057-04:00 level=INFO source=gpu_windows.go:167 msg=packages count=1
time=2025-07-30T11:53:33.057-04:00 level=INFO source=gpu_windows.go:183 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-07-30T11:53:33.057-04:00 level=INFO source=gpu_windows.go:214 msg="" package=0 cores=8 efficiency=4 threads=12
time=2025-07-30T11:53:33.164-04:00 level=INFO source=gpu.go:319 msg="detected OS VRAM overhead" id=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda compute=8.6 driver=12.3 name="NVIDIA GeForce RTX 3050 6GB Laptop GPU" overhead="326.3 MiB"
time=2025-07-30T11:53:33.165-04:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda variant=v12 compute=8.6 driver=12.3 name="NVIDIA GeForce RTX 3050 6GB Laptop GPU" total="6.0 GiB" available="5.0 GiB"
[GIN] 2025/07/30 - 11:53:44 | 200 |      2.9985ms |             ::1 | GET      "/api/tags"
[GIN] 2025/07/30 - 11:53:44 | 200 |            0s |       127.0.0.1 | HEAD     "/"
time=2025-07-30T11:53:44.811-04:00 level=INFO source=download.go:177 msg="downloading 667b0c1932bc in 16 307 MB part(s)"
[GIN] 2025/07/30 - 11:55:27 | 200 |         1m43s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/07/30 - 12:07:36 | 200 |      2.9105ms |             ::1 | GET      "/api/tags"
[GIN] 2025/07/30 - 13:13:55 | 200 |      4.5486ms |             ::1 | GET      "/api/tags"
[GIN] 2025/07/30 - 13:32:19 | 200 |      4.8348ms |             ::1 | GET      "/api/tags"
[GIN] 2025/07/30 - 13:33:50 | 200 |      4.2831ms |             ::1 | GET      "/api/tags"
[GIN] 2025/07/30 - 21:43:19 | 200 |     10.4803ms |             ::1 | GET      "/api/tags"
[GIN] 2025/07/30 - 21:50:36 | 200 |      1.5972ms |             ::1 | GET      "/api/tags"
[GIN] 2025/07/31 - 13:09:56 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/31 - 13:09:56 | 200 |      5.6294ms |       127.0.0.1 | GET      "/api/tags"
time=2025-07-31T13:16:39.885-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-07-31T13:16:39.928-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="53.1 GiB" free_swap="45.2 GiB"
time=2025-07-31T13:16:39.928-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=8 layers.split="" memory.available="[5.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="13.6 GiB" memory.required.partial="5.0 GiB" memory.required.kv="8.0 GiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="3.5 GiB" memory.weights.repeating="3.4 GiB" memory.weights.nonrepeating="102.6 MiB" memory.graph.full="1.1 GiB" memory.graph.partial="1.3 GiB"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-4a501ed4ce55e5611922b3ee422501ff7cc773b472d196c3c416859b6d375273 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = ise-uiuc
llama_model_loader: - kv   2:                       llama.context_length u32              = 16384
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 3.56 GiB (4.54 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 3
load: token to piece cache size = 0.1684 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 6.74 B
print_info: general.name     = ise-uiuc
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-07-31T13:16:39.964-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-4a501ed4ce55e5611922b3ee422501ff7cc773b472d196c3c416859b6d375273 --ctx-size 16384 --batch-size 512 --n-gpu-layers 8 --threads 4 --no-mmap --parallel 1 --port 59521"
time=2025-07-31T13:16:39.970-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-07-31T13:16:39.970-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-07-31T13:16:39.970-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-07-31T13:16:40.016-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-07-31T13:16:40.558-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-07-31T13:16:40.560-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:59521"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-4a501ed4ce55e5611922b3ee422501ff7cc773b472d196c3c416859b6d375273 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = ise-uiuc
llama_model_loader: - kv   2:                       llama.context_length u32              = 16384
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 3.56 GiB (4.54 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 3
load: token to piece cache size = 0.1684 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 16384
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 4096
print_info: n_embd_v_gqa     = 4096
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 11008
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 16384
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 6.74 B
print_info: general.name     = ise-uiuc
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = false)
time=2025-07-31T13:16:40.725-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 8 repeating layers to GPU
load_tensors: offloaded 8/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =  2779.12 MiB
load_tensors:        CUDA0 model buffer size =   868.75 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 16384
llama_context: n_ctx_per_seq = 16384
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context:        CPU  output buffer size =     0.14 MiB
llama_kv_cache_unified: kv_size = 16384, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  2048.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =  6144.00 MiB
llama_kv_cache_unified: KV self size  = 8192.00 MiB, K (f16): 4096.00 MiB, V (f16): 4096.00 MiB
llama_context:      CUDA0 compute buffer size =  1337.00 MiB
llama_context:  CUDA_Host compute buffer size =    48.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 268 (with bs=512), 3 (with bs=1)
time=2025-07-31T13:16:46.986-04:00 level=INFO source=server.go:630 msg="llama runner started in 7.02 seconds"
[GIN] 2025/07/31 - 13:16:51 | 200 |   11.2159773s |             ::1 | POST     "/api/generate"
time=2025-07-31T13:17:49.845-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/07/31 - 13:18:15 | 200 |   26.1421083s |             ::1 | POST     "/api/generate"
[GIN] 2025/07/31 - 13:30:13 | 200 |       2.558ms |             ::1 | GET      "/api/tags"
[GIN] 2025/07/31 - 13:30:22 | 200 |      2.9429ms |             ::1 | GET      "/api/tags"
[GIN] 2025/07/31 - 14:12:21 | 200 |       585.8µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/31 - 14:12:21 | 200 |       6.646ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/07/31 - 17:40:32 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/07/31 - 17:40:32 | 200 |      4.1852ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/07/31 - 23:07:59 | 200 |      8.7041ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/07/31 - 23:07:59 | 200 |     81.8152ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:07:59 | 200 |     90.8123ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:07:59 | 200 |     99.6534ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:07:59 | 200 |    106.4733ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:07:59 | 200 |     137.196ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:07:59 | 200 |     173.622ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:07:59 | 200 |    226.9616ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:07:59 | 200 |    231.8314ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:07:59 | 200 |    260.7953ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:07:59 | 200 |    270.4286ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:07:59 | 200 |    273.6911ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:07:59 | 200 |    298.2642ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:07:59 | 200 |    299.4956ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:07:59 | 200 |    342.5467ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |      5.2764ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/07/31 - 23:13:48 | 200 |     96.9599ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |    159.3658ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |    159.7385ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |    153.8016ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |    255.2354ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |     259.179ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |    311.6114ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |    328.0292ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |    321.4297ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |     333.027ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |      365.17ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |    354.4194ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |    370.6563ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/07/31 - 23:13:48 | 200 |    435.2195ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:42 | 200 |      8.3239ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 04:40:42 | 200 |     122.891ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:42 | 200 |    152.4326ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:43 | 200 |    203.1304ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:43 | 200 |    228.8048ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:43 | 200 |    239.4809ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:43 | 200 |      274.99ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:43 | 200 |    289.0818ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:43 | 200 |    302.8031ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:43 | 200 |    296.9727ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:43 | 200 |    324.8051ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:43 | 200 |    332.1502ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:43 | 200 |     336.931ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:43 | 200 |    345.7025ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 04:40:43 | 200 |     437.413ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 11:15:37 | 200 |      8.9517ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:15:39 | 200 |      5.3601ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:16:07 | 200 |      6.6195ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:16:37 | 200 |      8.8423ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:17:07 | 200 |      7.1346ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:17:37 | 200 |      7.0632ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:18:07 | 200 |      7.9002ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:18:37 | 200 |       7.082ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:19:07 | 200 |      7.7498ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:19:37 | 200 |      7.5891ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:20:07 | 200 |     12.8571ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:20:37 | 200 |      7.1199ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:21:07 | 200 |      8.4674ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:21:37 | 200 |      4.7001ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:22:07 | 200 |      6.6401ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:22:37 | 200 |      7.0111ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:23:07 | 200 |      7.9125ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:23:37 | 200 |      6.5726ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:24:07 | 200 |      7.4483ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:24:37 | 200 |      8.3157ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:25:07 | 200 |      7.7295ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:25:37 | 200 |      7.3453ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:26:07 | 200 |      7.2528ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:26:37 | 200 |      7.6032ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:27:07 | 200 |      7.1417ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:27:37 | 200 |      7.5912ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:28:07 | 200 |      8.2911ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:28:37 | 200 |      4.0813ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:29:07 | 200 |      7.1419ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:29:37 | 200 |      7.5007ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:30:07 | 200 |      8.5072ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:30:37 | 200 |     10.3147ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:31:07 | 200 |      7.5579ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:31:37 | 200 |      6.7668ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:32:07 | 200 |      8.3166ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:32:37 | 200 |      9.8514ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:33:07 | 200 |      6.5288ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:33:37 | 200 |      7.8426ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:34:07 | 200 |      7.1166ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:34:37 | 200 |      6.8184ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:35:07 | 200 |      5.9154ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:35:37 | 200 |      7.2769ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:36:07 | 200 |     10.7122ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:36:37 | 200 |      7.1142ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:37:07 | 200 |      6.6044ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:37:37 | 200 |      6.9085ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:38:07 | 200 |      7.9018ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:38:37 | 200 |      6.8516ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:39:07 | 200 |      7.3971ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:39:37 | 200 |      7.9241ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:40:07 | 200 |      7.6258ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:40:37 | 200 |      7.3177ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:41:07 | 200 |      6.0184ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:41:37 | 200 |      6.0937ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:42:07 | 200 |      7.4563ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:42:37 | 200 |      6.6085ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:43:07 | 200 |      7.6651ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:43:37 | 200 |      6.5614ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:44:07 | 200 |      7.8066ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:44:37 | 200 |      6.3851ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:45:07 | 200 |       7.091ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 11:45:37 | 200 |      4.9102ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 12:03:13 | 200 |      6.9924ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 12:03:13 | 200 |     93.8837ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:03:13 | 200 |    150.4378ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:03:13 | 200 |    203.3014ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:03:13 | 200 |      200.47ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:03:13 | 200 |    200.7738ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:03:13 | 200 |    220.0497ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:03:13 | 200 |    292.0159ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:03:13 | 200 |    294.5216ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:03:13 | 200 |    322.7524ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:03:13 | 200 |    324.7807ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:03:13 | 200 |    325.7074ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:03:13 | 200 |    337.7908ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:03:13 | 200 |    347.7628ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:03:13 | 200 |    395.4925ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:19:23 | 200 |      7.0499ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 12:27:24 | 200 |      6.6426ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 12:27:24 | 200 |     47.7862ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:27:24 | 200 |     59.9161ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:27:24 | 200 |    127.0286ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:27:24 | 200 |    229.1474ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:27:24 | 200 |    227.0822ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:27:24 | 200 |    218.6555ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:27:24 | 200 |     245.748ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:27:24 | 200 |    276.7258ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:27:24 | 200 |    294.8134ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:27:24 | 200 |    311.0467ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:27:24 | 200 |    315.4522ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:27:24 | 200 |    304.1951ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:27:24 | 200 |    311.5785ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:27:24 | 200 |    409.3029ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-01T12:28:53.499-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="63.8 GiB" free_swap="57.3 GiB"
time=2025-08-01T12:28:53.500-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=11 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="20.7 GiB" memory.required.partial="4.9 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="18.1 GiB" memory.weights.repeating="17.5 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="348.0 MiB" memory.graph.partial="916.1 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 771 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-ac3d1ba8aa77755dab3806d9024e9c385ea0d5b412d6bdf9157f8a4a7e9fc0d9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 32B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 32B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 32B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 64
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 18.48 GiB (4.85 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 32.76 B
print_info: general.name     = Qwen2.5 Coder 32B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-01T12:28:54.097-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-ac3d1ba8aa77755dab3806d9024e9c385ea0d5b412d6bdf9157f8a4a7e9fc0d9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 11 --threads 4 --no-mmap --parallel 1 --port 50061"
time=2025-08-01T12:28:54.106-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-01T12:28:54.106-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-01T12:28:54.107-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-01T12:28:54.228-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-01T12:28:54.342-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-01T12:28:54.343-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:50061"
time=2025-08-01T12:28:54.359-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 771 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-ac3d1ba8aa77755dab3806d9024e9c385ea0d5b412d6bdf9157f8a4a7e9fc0d9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 32B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 32B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 32B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 64
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 18.48 GiB (4.85 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 5120
print_info: n_layer          = 64
print_info: n_head           = 40
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 5
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 27648
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 32B
print_info: model params     = 32.76 B
print_info: general.name     = Qwen2.5 Coder 32B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 11 repeating layers to GPU
load_tensors: offloaded 11/65 layers to GPU
load_tensors:    CUDA_Host model buffer size = 15305.59 MiB
load_tensors:        CUDA0 model buffer size =  3202.76 MiB
load_tensors:          CPU model buffer size =   417.66 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   176.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =   848.00 MiB
llama_kv_cache_unified: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      CUDA0 compute buffer size =   926.08 MiB
llama_context:  CUDA_Host compute buffer size =    18.01 MiB
llama_context: graph nodes  = 2374
llama_context: graph splits = 746 (with bs=512), 3 (with bs=1)
time=2025-08-01T12:29:07.647-04:00 level=INFO source=server.go:630 msg="llama runner started in 13.54 seconds"
[GIN] 2025/08/01 - 12:31:30 | 200 |         2m37s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/01 - 12:33:48 | 200 |         1m51s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/01 - 12:38:11 | 200 |      5.8701ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 12:38:11 | 200 |     116.691ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:38:11 | 200 |    141.0459ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:38:11 | 200 |     160.594ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:38:12 | 200 |      206.99ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:38:12 | 200 |     209.104ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:38:12 | 200 |    217.2022ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:38:12 | 200 |    236.0268ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:38:12 | 200 |    263.6894ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:38:12 | 200 |    285.9475ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:38:12 | 200 |    291.4116ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:38:12 | 200 |    285.3962ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:38:12 | 200 |    304.5361ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:38:12 | 200 |    307.8585ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 12:38:12 | 200 |    379.8795ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:37 | 200 |      9.2337ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 15:00:37 | 200 |     97.8912ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:37 | 200 |    195.9973ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:37 | 200 |    234.1447ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:37 | 200 |    261.1758ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:37 | 200 |    337.6688ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:37 | 200 |    459.4813ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:37 | 200 |    501.2925ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:37 | 200 |    503.8809ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:37 | 200 |    568.0984ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:37 | 200 |    596.1089ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:38 | 200 |    609.9656ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:38 | 200 |    625.3603ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:38 | 200 |    630.9621ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:38 | 200 |    722.2952ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 15:00:45 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/01 - 15:00:45 | 200 |      6.1735ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 19:35:26 | 200 |     18.2389ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 19:35:26 | 200 |     43.6007ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 19:35:26 | 200 |     84.6314ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 19:35:26 | 200 |     83.6065ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 19:35:26 | 200 |     94.7038ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 19:35:26 | 200 |    108.6115ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 19:35:26 | 200 |    107.0033ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 19:35:26 | 200 |    178.0565ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 19:35:26 | 200 |     199.727ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 19:35:26 | 200 |    202.1882ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 19:35:26 | 200 |    214.7479ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 19:35:26 | 200 |     225.098ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 19:35:26 | 200 |    224.7281ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 19:35:26 | 200 |    225.3332ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 19:35:26 | 200 |    303.9078ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |      6.6032ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 20:17:46 | 200 |     99.5453ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |      98.992ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |    119.6556ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |    133.2002ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |     140.536ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |    164.1213ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |    223.5787ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |    232.9822ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |    237.4325ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |     253.663ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |    245.6217ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |    255.8585ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |    270.6694ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 20:17:46 | 200 |    353.6092ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:26:03 | 200 |      8.2511ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 21:26:31 | 200 |      6.7461ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 21:27:01 | 200 |      8.4901ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 21:30:18 | 200 |      6.2698ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 21:30:18 | 200 |     94.8482ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:30:18 | 200 |    155.5097ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:30:18 | 200 |    153.6129ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:30:18 | 200 |    144.5795ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:30:18 | 200 |    190.5891ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:30:18 | 200 |    204.1343ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:30:18 | 200 |    222.1054ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:30:18 | 200 |    259.6536ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:30:18 | 200 |    252.5455ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:30:18 | 200 |    269.6294ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:30:18 | 200 |    270.9753ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:30:18 | 200 |    303.6811ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:30:18 | 200 |    277.7492ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:30:18 | 200 |    364.5113ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:21 | 200 |      4.7055ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 21:36:22 | 200 |     61.8403ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:22 | 200 |    103.2743ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:22 | 200 |    103.3003ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:22 | 200 |     97.4659ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:22 | 200 |    117.3623ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:22 | 200 |    134.5754ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:22 | 200 |    187.3704ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:22 | 200 |    214.0365ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:22 | 200 |    232.7274ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:22 | 200 |    247.4937ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:22 | 200 |    249.5131ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:22 | 200 |    244.3757ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:22 | 200 |    266.9474ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 21:36:22 | 200 |    349.0695ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:57 | 200 |      9.0097ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 22:35:58 | 200 |    107.5189ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:58 | 200 |    143.5547ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:58 | 200 |     154.178ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:58 | 200 |     165.316ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:58 | 200 |    180.0927ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:58 | 200 |    196.9415ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:58 | 200 |    236.5367ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:58 | 200 |    239.7302ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:58 | 200 |    249.5744ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:58 | 200 |    257.2172ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:58 | 200 |    265.1928ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:58 | 200 |    266.8326ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:58 | 200 |     285.287ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:35:58 | 200 |    358.1226ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:25 | 200 |      6.3911ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 22:42:25 | 200 |     86.9887ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:26 | 200 |    176.8948ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:26 | 200 |    181.8776ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:26 | 200 |    180.7923ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:26 | 200 |    215.5675ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:26 | 200 |    261.4944ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:26 | 200 |     229.348ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:26 | 200 |    257.1086ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:26 | 200 |    262.2507ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:26 | 200 |    258.1655ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:26 | 200 |     264.922ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:26 | 200 |    271.2111ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:26 | 200 |    276.8348ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 22:42:26 | 200 |    384.3526ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:01:33 | 200 |      7.7962ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:02:01 | 200 |      7.9211ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:02:12 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/01 - 23:02:12 | 200 |    322.2017ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/01 - 23:02:31 | 200 |      5.8997ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:03:01 | 200 |      6.8131ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:03:31 | 200 |      5.9985ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:04:01 | 200 |      8.2929ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:04:31 | 200 |      9.2072ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:05:01 | 200 |      4.9844ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:05:22 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/01 - 23:05:22 | 200 |      7.6771ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:05:31 | 200 |      7.4866ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:05:39 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/01 - 23:05:40 | 200 |    200.8569ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-01T23:05:40.364-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="58.9 GiB" free_swap="52.7 GiB"
time=2025-08-01T23:05:40.368-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=1 layers.split="" memory.available="[4.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="65.9 GiB" memory.required.partial="4.3 GiB" memory.required.kv="768.0 MiB" memory.required.allocations="[4.3 GiB]" memory.weights.total="60.6 GiB" memory.weights.repeating="59.8 GiB" memory.weights.nonrepeating="809.3 MiB" memory.graph.full="404.6 MiB" memory.graph.partial="1.1 GiB" projector.weights="1.6 GiB" projector.graph="0 B"
time=2025-08-01T23:05:40.535-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-9d507a36062c2845dd3bb3e93364e9abc1607118acd8650727a700f72fb126e5 --ctx-size 4096 --batch-size 512 --n-gpu-layers 1 --threads 4 --no-mmap --parallel 1 --port 52070"
time=2025-08-01T23:05:40.542-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-01T23:05:40.542-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-01T23:05:40.542-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-01T23:05:40.586-04:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-01T23:05:40.588-04:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:52070"
time=2025-08-01T23:05:40.628-04:00 level=INFO source=ggml.go:92 msg="" architecture=llama4 file_type=Q4_K_M name="" description="" num_tensors=1182 num_key_values=45
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
time=2025-08-01T23:05:40.794-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-01T23:05:41.243-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-01T23:05:41.690-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CPU size="61.5 GiB"
time=2025-08-01T23:05:41.690-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CUDA0 size="1.3 GiB"
time=2025-08-01T23:05:41.897-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="0 B"
time=2025-08-01T23:05:41.897-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="389.4 MiB"
time=2025-08-01T23:05:42.250-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="358.0 MiB"
time=2025-08-01T23:05:42.250-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="389.4 MiB"
time=2025-08-01T23:05:53.563-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-01T23:05:53.921-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
[GIN] 2025/08/01 - 23:06:02 | 200 |    436.3761ms |             ::1 | GET      "/api/tags"
time=2025-08-01T23:06:03.798-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-01T23:06:04.189-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-01T23:06:10.239-04:00 level=INFO source=server.go:630 msg="llama runner started in 29.70 seconds"
[GIN] 2025/08/01 - 23:06:10 | 200 |    30.222153s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/01 - 23:06:31 | 200 |      7.0304ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:07:01 | 200 |      6.5346ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:07:14 | 200 |          1m2s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/01 - 23:07:31 | 200 |      7.3002ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:08:01 | 200 |      7.4486ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:08:31 | 200 |      9.3081ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:09:01 | 200 |      6.2733ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:09:31 | 200 |      6.4964ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:10:01 | 200 |      6.1205ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:10:17 | 200 |         1m21s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/01 - 23:10:31 | 200 |      9.2833ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:11:01 | 200 |      6.0051ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:11:31 | 200 |      7.6526ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:11:35 | 200 |      7.0156ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:12:01 | 200 |       5.215ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:12:03 | 200 |      7.3642ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:12:31 | 200 |      8.9778ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:13:01 | 200 |      6.8377ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:13:31 | 200 |      8.5323ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:14:01 | 200 |      9.1262ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:14:31 | 200 |      9.1953ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:15:01 | 200 |      6.9584ms |             ::1 | GET      "/api/tags"
time=2025-08-01T23:15:22.665-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0271384 runner.size="65.9 GiB" runner.vram="4.3 GiB" runner.parallel=1 runner.pid=85440 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9d507a36062c2845dd3bb3e93364e9abc1607118acd8650727a700f72fb126e5
time=2025-08-01T23:15:22.914-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2768844999999995 runner.size="65.9 GiB" runner.vram="4.3 GiB" runner.parallel=1 runner.pid=85440 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9d507a36062c2845dd3bb3e93364e9abc1607118acd8650727a700f72fb126e5
time=2025-08-01T23:15:23.164-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5268022 runner.size="65.9 GiB" runner.vram="4.3 GiB" runner.parallel=1 runner.pid=85440 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9d507a36062c2845dd3bb3e93364e9abc1607118acd8650727a700f72fb126e5
[GIN] 2025/08/01 - 23:15:32 | 200 |       5.649ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:16:02 | 200 |      6.6948ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:16:32 | 200 |      7.2429ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:17:02 | 200 |      7.9726ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:17:32 | 200 |      4.4745ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:18:02 | 200 |      6.8622ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:18:32 | 200 |      7.3663ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:19:02 | 200 |      7.2217ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:19:32 | 200 |      5.0226ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:20:02 | 200 |       7.347ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:20:32 | 200 |      4.9704ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:21:02 | 200 |      7.5342ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:21:32 | 200 |      5.0333ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:22:02 | 200 |      6.7661ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:22:32 | 200 |      7.6718ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:23:02 | 200 |       7.569ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:23:32 | 200 |      7.2853ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:23:50 | 200 |      6.9485ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:23:50 | 200 |    107.8484ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:23:50 | 200 |    138.1023ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:23:50 | 200 |    142.7203ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:23:50 | 200 |    143.9233ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:23:50 | 200 |    153.5528ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:23:50 | 200 |    149.3495ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:23:50 | 200 |    196.1804ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:23:50 | 200 |    208.1478ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:23:50 | 200 |    236.6219ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:23:50 | 200 |    237.6514ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:23:50 | 200 |    256.6295ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:23:50 | 200 |    251.2704ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:23:50 | 200 |    275.4151ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:23:50 | 200 |    298.0646ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/01 - 23:24:02 | 200 |      5.8741ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:24:32 | 200 |      6.5566ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:25:02 | 200 |     10.3631ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:25:32 | 200 |       4.946ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:26:02 | 200 |       5.084ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:26:32 | 200 |      6.8203ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:27:02 | 200 |      8.8834ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:27:32 | 200 |      6.8681ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:28:02 | 200 |      7.7537ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:28:32 | 200 |      7.4134ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:29:02 | 200 |      8.0011ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:29:32 | 200 |      5.8991ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:30:02 | 200 |      5.8581ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:30:32 | 200 |      6.4245ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:31:02 | 200 |      7.2881ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:31:32 | 200 |      7.8105ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:32:02 | 200 |      7.3817ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:32:32 | 200 |      6.1377ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:33:02 | 200 |      7.8958ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:33:32 | 200 |      3.4355ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:34:02 | 200 |      5.6499ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:34:32 | 200 |      6.1066ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:35:02 | 200 |      7.9041ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:35:32 | 200 |      8.2572ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:36:02 | 200 |      5.8083ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:36:32 | 200 |      6.2828ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:37:02 | 200 |      8.2484ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:37:32 | 200 |      7.5914ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:38:02 | 200 |      7.1025ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:38:32 | 200 |      6.8529ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:39:02 | 200 |      7.5019ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:39:32 | 200 |      7.8396ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:40:02 | 200 |      5.1411ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:40:32 | 200 |       4.959ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:41:02 | 200 |      7.1353ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:41:32 | 200 |       7.886ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:42:02 | 200 |      5.0722ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:42:32 | 200 |      6.6351ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:43:02 | 200 |      8.9076ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:43:32 | 200 |      5.1864ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:44:02 | 200 |      6.0975ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:44:32 | 200 |      7.6365ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:45:02 | 200 |      7.2179ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:45:32 | 200 |      5.4259ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:46:02 | 200 |      5.2041ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:46:32 | 200 |      7.3718ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:47:02 | 200 |      8.4702ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:47:32 | 200 |      4.6265ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:48:02 | 200 |      4.0192ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:48:32 | 200 |      7.4369ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:49:02 | 200 |      8.5123ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:49:32 | 200 |      7.7098ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:50:02 | 200 |      5.3913ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:50:32 | 200 |      7.0239ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:51:02 | 200 |      8.4488ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:51:32 | 200 |      5.6509ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:52:02 | 200 |      5.6493ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:52:32 | 200 |      7.1445ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:53:02 | 200 |      8.3536ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:53:32 | 200 |      7.4315ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:54:02 | 200 |      6.5358ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:54:32 | 200 |        6.98ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:55:02 | 200 |      7.0277ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:55:32 | 200 |      6.6997ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/01 - 23:56:02 | 200 |      7.2602ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:14:57 | 200 |    105.8695ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:15:27 | 200 |      6.4989ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:15:57 | 200 |      7.2476ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:16:27 | 200 |      6.7836ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:16:57 | 200 |      2.3644ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:17:27 | 200 |      7.3086ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:17:57 | 200 |      7.5573ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:18:27 | 200 |      6.7222ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:18:57 | 200 |     10.6275ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:19:27 | 200 |      7.4205ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:19:57 | 200 |      6.4272ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:20:27 | 200 |       6.802ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:20:37 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/02 - 16:20:37 | 200 |      8.2167ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:20:57 | 200 |      7.6766ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:20:57 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/02 - 16:20:58 | 200 |    448.5152ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/02 - 16:21:27 | 200 |      6.5578ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:21:40 | 200 |            0s |       127.0.0.1 | HEAD     "/"
time=2025-08-02T16:21:41.448-04:00 level=INFO source=download.go:177 msg="downloading ecdedd393ed1 in 245 1 GB part(s)"
[GIN] 2025/08/02 - 16:21:57 | 200 |      7.1879ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:22:12 | 200 |      5.5924ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:22:13 | 200 |     33.0372ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:13 | 200 |    100.3424ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:13 | 200 |    103.9452ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:13 | 200 |    114.6197ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:13 | 200 |    108.3238ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:13 | 200 |    108.5814ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:13 | 200 |    199.9798ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:13 | 200 |    213.9997ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:13 | 200 |    231.1536ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:13 | 200 |    227.5466ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:13 | 200 |     240.194ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:13 | 200 |    253.3226ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:13 | 200 |    257.4447ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:13 | 200 |    333.8163ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:22:28 | 200 |    886.2438ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:22:57 | 200 |      7.1819ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:23:27 | 200 |      5.6952ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:23:37 | 200 |         1m56s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/02 - 16:23:57 | 200 |      6.9062ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:24:27 | 200 |      5.1293ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:24:57 | 200 |      6.3305ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:25:27 | 200 |      8.5077ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:25:57 | 200 |      8.8077ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:26:27 | 200 |      7.8344ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:26:57 | 200 |     10.6783ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:27:27 | 200 |      8.0354ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:27:57 | 200 |      7.0443ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:28:13 | 200 |      6.5698ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:28:14 | 200 |    118.6357ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:14 | 200 |    119.5668ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:14 | 200 |    119.5668ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:14 | 200 |    157.2356ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:14 | 200 |    171.0612ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:14 | 200 |    183.8788ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:14 | 200 |    211.2352ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:14 | 200 |    221.3781ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:14 | 200 |    234.1094ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:14 | 200 |     234.721ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:14 | 200 |    248.3269ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:14 | 200 |    256.9078ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:14 | 200 |    271.8746ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:14 | 200 |    345.2709ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:28:27 | 200 |      6.5052ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:28:57 | 200 |      7.1714ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:29:27 | 200 |      7.6088ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:29:57 | 200 |      7.1679ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:30:25 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/02 - 16:30:25 | 200 |      7.6357ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:30:27 | 200 |      7.3437ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:30:57 | 200 |      5.1184ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:31:12 | 200 |      6.7187ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:31:28 | 200 |    157.8234ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:31:40 | 200 |      5.7347ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:31:57 | 200 |      6.2566ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:32:10 | 200 |      4.4998ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:32:11 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/08/02 - 16:32:27 | 200 |     11.4626ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:32:40 | 200 |      6.4796ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:32:57 | 200 |      6.4983ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:33:15 | 200 |      5.3285ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:33:27 | 200 |      6.3468ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:33:57 | 200 |      6.5834ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:34:27 | 200 |      3.9072ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:34:57 | 200 |      5.4444ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:35:27 | 200 |      5.5806ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:35:57 | 200 |      7.1281ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:36:27 | 200 |      7.0722ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:36:57 | 200 |      5.1439ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:37:27 | 200 |      5.5125ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:37:57 | 200 |      5.1241ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:38:27 | 200 |      5.5194ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:38:57 | 200 |      7.1085ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:39:28 | 200 |      6.9645ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:39:58 | 200 |      5.6782ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:40:28 | 200 |      6.8409ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:40:58 | 200 |      6.0931ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:41:28 | 200 |      6.3417ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:41:58 | 200 |      5.9542ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:42:28 | 200 |      5.0261ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:42:58 | 200 |      6.2042ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:43:28 | 200 |      6.4559ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:43:58 | 200 |      6.3978ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:44:28 | 200 |      7.1977ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:44:58 | 200 |      4.9967ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:45:28 | 200 |      5.3986ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:45:58 | 200 |      6.3489ms |             ::1 | GET      "/api/tags"
time=2025-08-02T16:46:24.593-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="70.3 GiB" free_swap="59.2 GiB"
time=2025-08-02T16:46:24.595-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=1 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="65.9 GiB" memory.required.partial="4.3 GiB" memory.required.kv="768.0 MiB" memory.required.allocations="[4.3 GiB]" memory.weights.total="60.6 GiB" memory.weights.repeating="59.8 GiB" memory.weights.nonrepeating="809.3 MiB" memory.graph.full="404.6 MiB" memory.graph.partial="1.1 GiB" projector.weights="1.6 GiB" projector.graph="0 B"
time=2025-08-02T16:46:24.721-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-9d507a36062c2845dd3bb3e93364e9abc1607118acd8650727a700f72fb126e5 --ctx-size 4096 --batch-size 512 --n-gpu-layers 1 --threads 4 --no-mmap --parallel 1 --port 57844"
time=2025-08-02T16:46:24.729-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-02T16:46:24.729-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-02T16:46:24.729-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-02T16:46:24.777-04:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-02T16:46:24.779-04:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:57844"
time=2025-08-02T16:46:24.819-04:00 level=INFO source=ggml.go:92 msg="" architecture=llama4 file_type=Q4_K_M name="" description="" num_tensors=1182 num_key_values=45
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-02T16:46:24.939-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-02T16:46:24.982-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-02T16:46:25.392-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CPU size="61.5 GiB"
time=2025-08-02T16:46:25.392-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CUDA0 size="1.3 GiB"
time=2025-08-02T16:46:25.600-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="0 B"
time=2025-08-02T16:46:25.600-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="389.4 MiB"
time=2025-08-02T16:46:25.921-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="358.0 MiB"
time=2025-08-02T16:46:25.921-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="389.4 MiB"
[GIN] 2025/08/02 - 16:46:28 | 200 |      5.8218ms |             ::1 | GET      "/api/tags"
time=2025-08-02T16:46:39.237-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-02T16:46:39.502-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-02T16:46:44.264-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-02T16:46:44.521-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-02T16:46:51.620-04:00 level=INFO source=server.go:630 msg="llama runner started in 26.89 seconds"
[GIN] 2025/08/02 - 16:46:58 | 200 |     19.4263ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:47:28 | 200 |      6.0242ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:47:55 | 200 |         1m30s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 16:47:58 | 200 |      4.6923ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:48:25 | 200 |      6.9618ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:48:28 | 200 |      5.3274ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:48:53 | 200 |      6.5882ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:48:58 | 200 |      4.4382ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:49:23 | 200 |      6.3913ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:49:28 | 200 |      6.2452ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:49:53 | 200 |      6.3863ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:49:58 | 200 |       8.622ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:50:23 | 200 |      6.1749ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:50:28 | 200 |      6.5285ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:50:53 | 200 |      5.5521ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:50:58 | 200 |      5.4127ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:51:23 | 200 |      6.5107ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:51:28 | 200 |      5.8312ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:51:53 | 200 |      5.8297ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:51:58 | 200 |      7.4983ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:52:23 | 200 |      5.7138ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:52:28 | 200 |      4.6002ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:52:53 | 200 |      3.9786ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:52:54 | 200 |      4.1898ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:52:54 | 200 |     38.7695ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:54 | 200 |     48.4793ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:54 | 200 |     49.8368ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:54 | 200 |     87.4025ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:54 | 200 |    130.2765ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:54 | 200 |    134.9453ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:54 | 200 |    144.6826ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:54 | 200 |    147.2735ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:54 | 200 |    154.8421ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:54 | 200 |    154.7275ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:54 | 200 |    165.4357ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:54 | 200 |    206.4127ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:54 | 200 |    205.8856ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:54 | 200 |    260.2169ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 16:52:58 | 200 |      4.8983ms |             ::1 | GET      "/api/tags"
time=2025-08-02T16:53:00.303-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0140324 runner.name=registry.ollama.ai/library/llama4:latest runner.inference=cuda runner.devices=1 runner.size="65.9 GiB" runner.vram="4.3 GiB" runner.parallel=1 runner.pid=92872 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9d507a36062c2845dd3bb3e93364e9abc1607118acd8650727a700f72fb126e5 runner.num_ctx=4096
time=2025-08-02T16:53:00.552-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2636589 runner.size="65.9 GiB" runner.vram="4.3 GiB" runner.parallel=1 runner.pid=92872 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9d507a36062c2845dd3bb3e93364e9abc1607118acd8650727a700f72fb126e5
time=2025-08-02T16:53:00.802-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5136231 runner.size="65.9 GiB" runner.vram="4.3 GiB" runner.parallel=1 runner.pid=92872 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9d507a36062c2845dd3bb3e93364e9abc1607118acd8650727a700f72fb126e5
[GIN] 2025/08/02 - 16:53:23 | 200 |      7.4006ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:53:28 | 200 |      7.2629ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:53:53 | 200 |      7.2168ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:53:58 | 200 |      7.2108ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:54:23 | 200 |      5.4446ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:54:28 | 200 |      5.4633ms |             ::1 | GET      "/api/tags"
time=2025-08-02T16:54:36.294-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5275230208 required="2.5 GiB"
time=2025-08-02T16:54:36.313-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="73.1 GiB" free_swap="59.2 GiB"
time=2025-08-02T16:54:36.313-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=17 layers.offload=17 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.5 GiB" memory.required.partial="2.5 GiB" memory.required.kv="256.0 MiB" memory.required.allocations="[2.5 GiB]" memory.weights.total="1.2 GiB" memory.weights.repeating="986.2 MiB" memory.weights.nonrepeating="266.2 MiB" memory.graph.full="544.0 MiB" memory.graph.partial="554.3 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-02T16:54:36.974-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 --ctx-size 8192 --batch-size 512 --n-gpu-layers 17 --threads 4 --no-mmap --parallel 2 --port 58349"
time=2025-08-02T16:54:36.982-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-02T16:54:36.982-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-02T16:54:36.983-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-02T16:54:37.088-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-02T16:54:37.189-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-02T16:54:37.190-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:58349"
time=2025-08-02T16:54:37.234-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 16 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 17/17 layers to GPU
load_tensors:    CUDA_Host model buffer size =   266.16 MiB
load_tensors:        CUDA0 model buffer size =  1252.41 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.99 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   256.00 MiB
llama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_context:      CUDA0 compute buffer size =   544.00 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 2
time=2025-08-02T16:54:38.737-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/02 - 16:54:39 | 200 |    2.8319768s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 16:54:53 | 200 |      5.3151ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:54:58 | 200 |      5.6908ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:55:23 | 200 |      5.4769ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:55:28 | 200 |      5.9214ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:55:39 | 200 |      4.8131ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:55:53 | 200 |      4.9553ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:55:58 | 200 |      7.4876ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:56:23 | 200 |      6.9392ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:56:28 | 200 |      7.9092ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:56:53 | 200 |      5.2408ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:56:58 | 200 |      6.3774ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:57:23 | 200 |      7.9512ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:57:28 | 200 |      7.3903ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:57:51 | 200 |      5.4879ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:57:53 | 200 |       5.865ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:57:58 | 200 |      6.1148ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:58:23 | 200 |      6.9603ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:58:28 | 200 |       4.303ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:58:59 | 200 |      7.5228ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:59:27 | 200 |      7.5481ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 16:59:57 | 200 |      4.5046ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:00:27 | 200 |      6.9869ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:00:57 | 200 |      6.3689ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:01:27 | 200 |       6.762ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:01:57 | 200 |      5.5642ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:02:35 | 200 |      5.4367ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:03:03 | 200 |      3.8642ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:03:33 | 200 |      6.4109ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:04:03 | 200 |      7.3289ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:04:33 | 200 |      6.7345ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:05:03 | 200 |      6.3767ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:05:33 | 200 |      6.5757ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:06:03 | 200 |      6.1815ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:06:33 | 200 |      6.6549ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:07:03 | 200 |      4.2485ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:07:33 | 200 |       7.495ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:08:03 | 200 |      4.8043ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:08:33 | 200 |      6.5656ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:09:03 | 200 |      5.8796ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:09:44 | 200 |      12.901ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:10:12 | 200 |      4.6615ms |             ::1 | GET      "/api/tags"
time=2025-08-02T17:10:25.089-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T17:10:25.089-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-02T17:10:25.179-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5296234496 required="2.5 GiB"
time=2025-08-02T17:10:25.200-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="72.4 GiB" free_swap="59.2 GiB"
time=2025-08-02T17:10:25.201-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=17 layers.offload=17 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.5 GiB" memory.required.partial="2.5 GiB" memory.required.kv="256.0 MiB" memory.required.allocations="[2.5 GiB]" memory.weights.total="1.2 GiB" memory.weights.repeating="986.2 MiB" memory.weights.nonrepeating="266.2 MiB" memory.graph.full="544.0 MiB" memory.graph.partial="554.3 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-02T17:10:25.729-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 --ctx-size 8192 --batch-size 512 --n-gpu-layers 17 --threads 4 --no-mmap --parallel 2 --port 59061"
time=2025-08-02T17:10:25.735-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-02T17:10:25.735-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-02T17:10:25.736-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-02T17:10:25.784-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-02T17:10:25.887-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-02T17:10:25.887-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:59061"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-08-02T17:10:25.988-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 16 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 17/17 layers to GPU
load_tensors:    CUDA_Host model buffer size =   266.16 MiB
load_tensors:        CUDA0 model buffer size =  1252.41 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.99 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   256.00 MiB
llama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_context:      CUDA0 compute buffer size =   544.00 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 2
time=2025-08-02T17:10:26.740-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.00 seconds"
[GIN] 2025/08/02 - 17:10:27 | 200 |    2.8940118s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 17:10:42 | 200 |      6.5372ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:11:12 | 200 |      5.2423ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:11:42 | 200 |      6.4783ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:12:34 | 200 |      4.9191ms |             ::1 | GET      "/api/tags"
time=2025-08-02T17:12:55.355-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-02T17:12:55.355-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/02 - 17:12:56 | 200 |    1.2377219s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 17:13:02 | 200 |      5.7118ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:13:32 | 200 |      8.6898ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:14:02 | 200 |      5.3675ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:14:32 | 200 |      5.0467ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:15:03 | 200 |      6.3806ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:15:33 | 200 |      6.2768ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:16:03 | 200 |      5.2747ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:16:33 | 200 |      6.1508ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:17:03 | 200 |      7.7774ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:17:33 | 200 |      7.9838ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:18:03 | 200 |      5.6195ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:18:33 | 200 |      6.0035ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:19:03 | 200 |      7.1105ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:19:33 | 200 |      6.0958ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:20:03 | 200 |     75.8424ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:20:33 | 200 |      5.6738ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:21:03 | 200 |      5.5099ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:21:33 | 200 |      5.4861ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:22:03 | 200 |      7.5335ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:22:33 | 200 |      5.1178ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:23:03 | 200 |      6.7783ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:23:33 | 200 |      6.6312ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:24:03 | 200 |      5.6056ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:24:33 | 200 |      6.2489ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:25:03 | 200 |      7.3028ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:25:33 | 200 |      5.7697ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:26:03 | 200 |      6.8428ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:26:33 | 200 |      5.5795ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:26:52 | 200 |            0s |             ::1 | GET      "/"
[GIN] 2025/08/02 - 17:26:53 | 404 |            0s |             ::1 | GET      "/favicon.ico"
[GIN] 2025/08/02 - 17:27:03 | 200 |      6.6085ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:27:33 | 200 |      5.7249ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:28:03 | 200 |      5.7889ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:28:33 | 200 |      6.7055ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:29:03 | 200 |      5.3146ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:29:33 | 200 |      5.3577ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:30:03 | 200 |       5.016ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:30:33 | 200 |      4.8692ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:31:03 | 200 |      38.547ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:31:33 | 200 |      6.0469ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:32:03 | 200 |      5.7473ms |             ::1 | GET      "/api/tags"
time=2025-08-02T17:32:18.047-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T17:32:18.048-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-02T17:32:18.048-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=includeContext
time=2025-08-02T17:32:18.132-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5261795328 required="2.5 GiB"
time=2025-08-02T17:32:18.148-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="71.7 GiB" free_swap="58.8 GiB"
time=2025-08-02T17:32:18.148-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=17 layers.offload=17 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.5 GiB" memory.required.partial="2.5 GiB" memory.required.kv="256.0 MiB" memory.required.allocations="[2.5 GiB]" memory.weights.total="1.2 GiB" memory.weights.repeating="986.2 MiB" memory.weights.nonrepeating="266.2 MiB" memory.graph.full="544.0 MiB" memory.graph.partial="554.3 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-02T17:32:18.794-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 --ctx-size 8192 --batch-size 512 --n-gpu-layers 17 --threads 4 --no-mmap --parallel 2 --port 50850"
time=2025-08-02T17:32:18.801-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-02T17:32:18.801-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-02T17:32:18.802-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-02T17:32:18.865-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-02T17:32:18.967-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-02T17:32:18.967-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:50850"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-02T17:32:19.054-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 16 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 17/17 layers to GPU
load_tensors:    CUDA_Host model buffer size =   266.16 MiB
load_tensors:        CUDA0 model buffer size =  1252.41 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.99 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   256.00 MiB
llama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_context:      CUDA0 compute buffer size =   544.00 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 2
time=2025-08-02T17:32:19.806-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.00 seconds"
[GIN] 2025/08/02 - 17:32:20 | 200 |    2.3058724s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 17:32:33 | 200 |      5.8189ms |             ::1 | GET      "/api/tags"
time=2025-08-02T17:32:44.459-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T17:32:44.459-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-02T17:32:44.459-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=includeContext
[GIN] 2025/08/02 - 17:32:45 | 200 |      1.25959s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 17:33:03 | 200 |      6.7151ms |             ::1 | GET      "/api/tags"
time=2025-08-02T17:33:23.029-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T17:33:23.029-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-02T17:33:23.029-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=includeContext
[GIN] 2025/08/02 - 17:33:23 | 200 |    613.3764ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 17:33:33 | 200 |      5.6698ms |             ::1 | GET      "/api/tags"
time=2025-08-02T17:33:48.656-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-02T17:33:48.656-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=includeContext
time=2025-08-02T17:33:48.656-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/02 - 17:33:49 | 200 |    969.0045ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 17:34:03 | 200 |      5.1226ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:34:33 | 200 |      7.1991ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:35:03 | 200 |      9.0156ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:35:33 | 200 |      5.2283ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:36:03 | 200 |      5.2918ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:36:33 | 200 |      6.1941ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:37:03 | 200 |      6.4701ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:37:33 | 200 |      7.2422ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:38:03 | 200 |      6.1023ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:38:33 | 200 |      5.5952ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:39:03 | 200 |      2.6586ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:39:33 | 200 |      6.4616ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:40:03 | 200 |      5.6125ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:40:33 | 200 |       5.287ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:41:03 | 200 |      7.0675ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:41:33 | 200 |      5.6189ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:42:03 | 200 |      4.1221ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:42:33 | 200 |       4.995ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:43:03 | 200 |      4.6193ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:43:33 | 200 |      5.1808ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:44:03 | 200 |      6.0111ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:44:33 | 200 |      4.8735ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:45:03 | 200 |      5.7808ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:45:33 | 200 |      5.3973ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:46:03 | 200 |      5.5585ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:46:33 | 200 |      6.3789ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:47:03 | 200 |        6.99ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:47:33 | 200 |      5.3912ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:48:03 | 200 |      6.2319ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:48:33 | 200 |      5.1655ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:49:03 | 200 |      6.4166ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:49:33 | 200 |      5.6607ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:50:03 | 200 |      5.0926ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:50:33 | 200 |      4.5798ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:51:03 | 200 |      5.1952ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:51:33 | 200 |      6.2988ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:52:03 | 200 |      4.8408ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:52:33 | 200 |        5.53ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:53:03 | 200 |      5.6526ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:53:33 | 200 |      4.9255ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:54:03 | 200 |      4.5916ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:54:33 | 200 |      5.7097ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:55:03 | 200 |      5.0215ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:55:33 | 200 |      4.6358ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:56:03 | 200 |      5.4549ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:56:33 | 200 |      5.4497ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:57:03 | 200 |      3.9187ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:57:33 | 200 |      6.7182ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:58:03 | 200 |      6.4116ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:58:33 | 200 |      5.1324ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:59:03 | 200 |       7.048ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 17:59:33 | 200 |      5.0191ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:00:03 | 200 |      5.6748ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:00:33 | 200 |      4.4231ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:01:03 | 200 |      6.8492ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:01:33 | 200 |      5.5408ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:02:03 | 200 |      5.6311ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:02:33 | 200 |      4.6575ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:03:03 | 200 |      5.0241ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:03:33 | 200 |      5.1484ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:04:03 | 200 |      5.3945ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:04:33 | 200 |      4.4158ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:05:03 | 200 |      5.5428ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:05:33 | 200 |      5.4215ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:06:03 | 200 |      5.5848ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:06:33 | 200 |      6.1838ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:07:03 | 200 |      7.7782ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:07:33 | 200 |      4.8842ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:08:03 | 200 |      3.7957ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:08:33 | 200 |      7.9014ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:09:03 | 200 |     10.6158ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:09:33 | 200 |      6.0513ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:10:03 | 200 |      8.4393ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:10:33 | 200 |       4.264ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:11:03 | 200 |      7.0619ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:11:33 | 200 |      6.4958ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:12:03 | 200 |      5.4451ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:12:33 | 200 |      6.1899ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:13:03 | 200 |      8.5007ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:13:33 | 200 |      5.0848ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:14:03 | 200 |      5.5135ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:14:33 | 200 |      5.1712ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:15:03 | 200 |      7.2862ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:15:33 | 200 |       5.192ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:16:03 | 200 |      5.5101ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:16:33 | 200 |      5.9035ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:17:03 | 200 |      8.6734ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:17:33 | 200 |      5.6797ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:18:03 | 200 |      6.7374ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:18:33 | 200 |      5.1415ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:19:03 | 200 |      7.6036ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:19:33 | 200 |      5.1981ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:19:59 | 200 |      5.4123ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:20:03 | 200 |      5.9797ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:20:27 | 200 |      5.5413ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:20:33 | 200 |      6.4086ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:20:57 | 200 |       8.247ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:21:03 | 200 |      5.1589ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:21:27 | 200 |      5.5467ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:21:34 | 200 |      6.7666ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:21:57 | 200 |      5.8367ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:22:04 | 200 |      5.3542ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:22:27 | 200 |      5.6559ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:22:34 | 200 |      5.8574ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:22:57 | 200 |      5.7596ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:23:04 | 200 |      6.9099ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:23:27 | 200 |      5.4006ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:23:34 | 200 |      7.7281ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:23:57 | 200 |      4.9256ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:24:02 | 404 |      3.5839ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 18:24:04 | 200 |      4.7174ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:24:27 | 200 |       5.663ms |             ::1 | GET      "/api/tags"
time=2025-08-02T18:24:29.126-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5297274880 required="2.5 GiB"
time=2025-08-02T18:24:29.138-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="69.8 GiB" free_swap="57.6 GiB"
time=2025-08-02T18:24:29.139-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=17 layers.offload=17 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.5 GiB" memory.required.partial="2.5 GiB" memory.required.kv="256.0 MiB" memory.required.allocations="[2.5 GiB]" memory.weights.total="1.2 GiB" memory.weights.repeating="986.2 MiB" memory.weights.nonrepeating="266.2 MiB" memory.graph.full="544.0 MiB" memory.graph.partial="554.3 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-02T18:24:29.661-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 --ctx-size 8192 --batch-size 512 --n-gpu-layers 17 --threads 4 --no-mmap --parallel 2 --port 56763"
time=2025-08-02T18:24:29.667-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-02T18:24:29.667-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-02T18:24:29.668-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-02T18:24:29.737-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-02T18:24:29.846-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-02T18:24:29.846-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:56763"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-02T18:24:29.919-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 16 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 17/17 layers to GPU
load_tensors:    CUDA_Host model buffer size =   266.16 MiB
load_tensors:        CUDA0 model buffer size =  1252.41 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.99 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   256.00 MiB
llama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_context:      CUDA0 compute buffer size =   544.00 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 2
time=2025-08-02T18:24:30.670-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.00 seconds"
[GIN] 2025/08/02 - 18:24:30 | 200 |    1.9503052s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 18:24:34 | 200 |      4.9741ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:24:57 | 200 |      3.9439ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:25:04 | 200 |      6.5901ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:25:27 | 200 |      5.5881ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:25:34 | 200 |       5.356ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:25:42 | 200 |      6.8991ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:25:57 | 200 |      4.6512ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:26:04 | 200 |      5.4801ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:26:10 | 200 |      6.1066ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:26:27 | 200 |      5.7764ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:26:34 | 200 |      5.5311ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:26:40 | 200 |      6.6072ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:26:57 | 200 |      8.6171ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:27:04 | 200 |      6.2369ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:27:10 | 200 |      7.6538ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:27:27 | 200 |      5.5626ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:27:34 | 200 |      5.5894ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:27:40 | 200 |      7.0465ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:27:57 | 200 |      5.2391ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:28:04 | 200 |      4.9244ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:28:10 | 200 |      6.6764ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:28:27 | 200 |      4.7271ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:28:34 | 200 |      5.5541ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:28:40 | 200 |      7.4258ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:28:57 | 200 |      3.7515ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:29:04 | 200 |      5.9808ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:29:10 | 200 |      5.9473ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:29:27 | 200 |      4.4272ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:29:34 | 200 |      4.4146ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:29:40 | 200 |      6.4447ms |             ::1 | GET      "/api/tags"
time=2025-08-02T18:29:53.393-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=rope_frequency_base
time=2025-08-02T18:29:53.438-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="69.6 GiB" free_swap="57.4 GiB"
time=2025-08-02T18:29:53.439-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=23 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.5 GiB" memory.required.partial="4.8 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[4.8 GiB]" memory.weights.total="3.5 GiB" memory.weights.repeating="3.4 GiB" memory.weights.nonrepeating="102.6 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="353.0 MiB"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-3a43f93b78ec50f7c4e4dc8bd1cb3fff5a900e7d574c51a6f7495e48486e0dac (version GGUF V2)
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = codellama
llama_model_loader: - kv   2:                       llama.context_length u32              = 16384
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V2
print_info: file type   = Q4_0
print_info: file size   = 3.56 GiB (4.54 BPW) 
load: control-looking token:  32009 '▁<MID>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control-looking token:  32008 '▁<SUF>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control-looking token:  32007 '▁<PRE>' was not control-type; this is probably a bug in the model. its type will be overridden
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 6
load: token to piece cache size = 0.1686 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 6.74 B
print_info: general.name     = codellama
print_info: vocab type       = SPM
print_info: n_vocab          = 32016
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: FIM PRE token    = 32007 '▁<PRE>'
print_info: FIM SUF token    = 32008 '▁<SUF>'
print_info: FIM MID token    = 32009 '▁<MID>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-08-02T18:29:53.506-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-3a43f93b78ec50f7c4e4dc8bd1cb3fff5a900e7d574c51a6f7495e48486e0dac --ctx-size 4096 --batch-size 512 --n-gpu-layers 23 --threads 4 --no-mmap --parallel 1 --port 57116"
time=2025-08-02T18:29:53.512-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-02T18:29:53.512-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-02T18:29:53.513-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-02T18:29:53.560-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-02T18:29:53.664-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-02T18:29:53.665-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:57116"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-3a43f93b78ec50f7c4e4dc8bd1cb3fff5a900e7d574c51a6f7495e48486e0dac (version GGUF V2)
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = codellama
llama_model_loader: - kv   2:                       llama.context_length u32              = 16384
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V2
print_info: file type   = Q4_0
print_info: file size   = 3.56 GiB (4.54 BPW) 
load: control-looking token:  32009 '▁<MID>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control-looking token:  32008 '▁<SUF>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control-looking token:  32007 '▁<PRE>' was not control-type; this is probably a bug in the model. its type will be overridden
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 6
load: token to piece cache size = 0.1686 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 16384
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 4096
print_info: n_embd_v_gqa     = 4096
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 11008
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 16384
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 6.74 B
print_info: general.name     = codellama
print_info: vocab type       = SPM
print_info: n_vocab          = 32016
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: FIM PRE token    = 32007 '▁<PRE>'
print_info: FIM SUF token    = 32008 '▁<SUF>'
print_info: FIM MID token    = 32009 '▁<MID>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = false)
time=2025-08-02T18:29:53.764-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 23/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =  1150.30 MiB
load_tensors:        CUDA0 model buffer size =  2497.66 MiB
[GIN] 2025/08/02 - 18:29:57 | 200 |      4.4799ms |             ::1 | GET      "/api/tags"
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (16384) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.14 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1472.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_context:      CUDA0 compute buffer size =   353.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 103 (with bs=512), 3 (with bs=1)
time=2025-08-02T18:29:59.275-04:00 level=INFO source=server.go:630 msg="llama runner started in 5.76 seconds"
[GIN] 2025/08/02 - 18:30:04 | 200 |       4.988ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:30:10 | 200 |      6.0421ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:30:10 | 200 |   17.2491534s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 18:30:27 | 200 |      5.7133ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:30:34 | 200 |      6.0601ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:30:40 | 200 |      4.8569ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:30:57 | 200 |       5.455ms |             ::1 | GET      "/api/tags"
time=2025-08-02T18:31:02.652-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="555.8 MiB"
time=2025-08-02T18:31:02.972-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="69.5 GiB" free_swap="57.3 GiB"
time=2025-08-02T18:31:02.974-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=7 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="13.6 GiB" memory.required.partial="4.6 GiB" memory.required.kv="8.0 GiB" memory.required.allocations="[4.6 GiB]" memory.weights.total="3.5 GiB" memory.weights.repeating="3.4 GiB" memory.weights.nonrepeating="102.6 MiB" memory.graph.full="1.1 GiB" memory.graph.partial="1.3 GiB"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-4a501ed4ce55e5611922b3ee422501ff7cc773b472d196c3c416859b6d375273 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = ise-uiuc
llama_model_loader: - kv   2:                       llama.context_length u32              = 16384
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 3.56 GiB (4.54 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 3
load: token to piece cache size = 0.1684 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 6.74 B
print_info: general.name     = ise-uiuc
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-08-02T18:31:03.006-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-4a501ed4ce55e5611922b3ee422501ff7cc773b472d196c3c416859b6d375273 --ctx-size 16384 --batch-size 512 --n-gpu-layers 7 --threads 4 --no-mmap --parallel 1 --port 57176"
time=2025-08-02T18:31:03.011-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-02T18:31:03.011-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-02T18:31:03.012-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-02T18:31:03.056-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-02T18:31:03.154-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-02T18:31:03.154-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:57176"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-4a501ed4ce55e5611922b3ee422501ff7cc773b472d196c3c416859b6d375273 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = ise-uiuc
llama_model_loader: - kv   2:                       llama.context_length u32              = 16384
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 3.56 GiB (4.54 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 3
load: token to piece cache size = 0.1684 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 16384
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 4096
print_info: n_embd_v_gqa     = 4096
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 11008
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 16384
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 6.74 B
print_info: general.name     = ise-uiuc
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = false)
time=2025-08-02T18:31:03.263-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 7 repeating layers to GPU
load_tensors: offloaded 7/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =  2887.71 MiB
load_tensors:        CUDA0 model buffer size =   760.16 MiB
[GIN] 2025/08/02 - 18:31:04 | 200 |       6.006ms |             ::1 | GET      "/api/tags"
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 16384
llama_context: n_ctx_per_seq = 16384
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context:        CPU  output buffer size =     0.14 MiB
llama_kv_cache_unified: kv_size = 16384, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1792.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =  6400.00 MiB
llama_kv_cache_unified: KV self size  = 8192.00 MiB, K (f16): 4096.00 MiB, V (f16): 4096.00 MiB
llama_context:      CUDA0 compute buffer size =  1337.00 MiB
llama_context:  CUDA_Host compute buffer size =    48.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 279 (with bs=512), 3 (with bs=1)
time=2025-08-02T18:31:10.278-04:00 level=INFO source=server.go:630 msg="llama runner started in 7.27 seconds"
[GIN] 2025/08/02 - 18:31:10 | 200 |      6.6592ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:31:27 | 200 |      5.4107ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:31:34 | 200 |      5.8409ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:31:40 | 200 |      5.6232ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:31:57 | 200 |      6.7948ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:32:04 | 200 |       4.809ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:32:10 | 200 |       6.537ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:32:12 | 200 |         1m10s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 18:32:27 | 200 |      9.3295ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:32:34 | 200 |      5.1397ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:32:40 | 200 |      4.9445ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:32:57 | 200 |      4.5721ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:33:04 | 200 |      4.8168ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:33:10 | 200 |      5.1827ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:33:27 | 200 |      7.2127ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:33:34 | 200 |      6.6522ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:33:40 | 200 |      4.8294ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:33:57 | 200 |      4.3722ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:34:04 | 200 |      5.2202ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:34:10 | 200 |      5.4893ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:34:27 | 200 |      9.0387ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:34:34 | 200 |      6.1095ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:34:40 | 200 |      5.1868ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:34:57 | 200 |      7.3599ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:35:04 | 200 |      4.8972ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:35:10 | 200 |      5.5212ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:35:27 | 200 |      9.4462ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:35:34 | 200 |      6.9276ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:35:40 | 200 |      6.7547ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:35:57 | 200 |      5.6978ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:36:04 | 200 |      4.5777ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:36:10 | 200 |      5.0401ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:36:27 | 200 |      5.0518ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:36:34 | 200 |      7.2305ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:36:40 | 200 |       5.539ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:36:57 | 200 |       6.161ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:37:04 | 200 |      5.8867ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:37:10 | 200 |       6.038ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:37:27 | 200 |      5.7266ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:37:34 | 200 |      8.1266ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:37:40 | 200 |      5.2174ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:37:57 | 200 |      6.7205ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:38:04 | 200 |       3.722ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:38:10 | 200 |      4.7196ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:38:27 | 200 |      7.3631ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:38:34 | 200 |      5.9571ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:38:40 | 200 |      4.9133ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:38:57 | 200 |      6.4494ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:39:04 | 200 |      4.8305ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:39:10 | 200 |      4.0811ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:39:27 | 200 |      4.4715ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:39:34 | 200 |      5.7306ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:39:40 | 200 |      4.7443ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:39:57 | 200 |       5.066ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:40:04 | 200 |      4.9187ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:40:10 | 200 |      5.0388ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:40:27 | 200 |      8.1301ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:40:34 | 200 |      3.8169ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:40:40 | 200 |      5.8636ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:40:57 | 200 |      4.4082ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:41:04 | 200 |      6.1081ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:41:10 | 200 |      5.8436ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:41:27 | 200 |      7.2922ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:41:34 | 200 |      6.7108ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:41:40 | 200 |      5.0199ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:41:57 | 200 |      6.5438ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:42:04 | 200 |      5.4503ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:42:10 | 200 |      6.2057ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:42:27 | 200 |      7.7309ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:42:34 | 200 |      6.5004ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:42:40 | 200 |      3.7809ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:42:57 | 200 |      4.8603ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:43:04 | 200 |      6.4109ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:43:10 | 200 |      4.3153ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:43:27 | 200 |      6.2982ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:43:34 | 200 |      7.1162ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:43:40 | 200 |      5.6633ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:43:57 | 200 |      5.3792ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:44:04 | 200 |      5.9852ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:44:10 | 200 |      4.8376ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:44:27 | 200 |      7.4518ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:44:34 | 200 |      5.7913ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:44:40 | 200 |      5.0301ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:44:57 | 200 |       4.881ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:45:04 | 200 |       4.702ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:45:10 | 200 |      6.9117ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:45:27 | 200 |      5.5153ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:45:34 | 200 |      6.9012ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:45:40 | 200 |      5.2156ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:45:57 | 200 |      4.3677ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:46:04 | 200 |      5.4843ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:46:10 | 200 |      6.7226ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:46:27 | 200 |      8.2744ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:46:34 | 200 |      6.2837ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:46:40 | 200 |       5.733ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:46:57 | 200 |      4.8422ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:47:04 | 200 |      5.2049ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:47:10 | 200 |      5.0194ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:47:27 | 200 |      6.6714ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:47:34 | 200 |      5.7352ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:47:40 | 200 |      4.6548ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:47:44 | 200 |      4.5103ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:47:44 | 200 |      4.9081ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:47:57 | 200 |      7.1812ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:48:04 | 200 |      5.7636ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:48:10 | 200 |      5.6429ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:48:27 | 200 |      5.0212ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:48:34 | 200 |      6.2957ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:48:40 | 200 |      5.6263ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:48:57 | 200 |      4.0071ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:49:04 | 200 |      4.5765ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:49:10 | 200 |      8.9119ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:49:27 | 200 |      7.3788ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:49:34 | 200 |      5.6197ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:49:40 | 200 |      4.1739ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:49:57 | 200 |      4.9828ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:50:04 | 200 |      6.2866ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:50:10 | 200 |      7.2333ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:50:27 | 200 |      6.0026ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:50:34 | 200 |      5.5755ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:50:40 | 200 |       5.594ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:50:57 | 200 |      5.6543ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:51:04 | 200 |       4.334ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:51:10 | 200 |      6.8081ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:51:27 | 200 |      5.0955ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:51:34 | 200 |      4.6319ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:51:40 | 200 |      6.4597ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:51:57 | 200 |      5.6494ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:52:04 | 200 |      4.7019ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:52:10 | 200 |      4.9503ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:52:27 | 200 |       5.705ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:52:34 | 200 |      5.6997ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:52:40 | 200 |       6.011ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:52:57 | 200 |      6.4894ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:53:04 | 200 |      7.8417ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:53:11 | 200 |      6.5587ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:53:27 | 200 |      5.4962ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:53:34 | 200 |      6.5602ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:53:41 | 200 |       5.775ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:53:57 | 200 |      3.8625ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:54:04 | 200 |      8.6494ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:54:11 | 200 |      5.1374ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:54:27 | 200 |      6.0345ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:54:34 | 200 |      6.2478ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:54:41 | 200 |      6.2108ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:54:57 | 200 |      4.3053ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:55:04 | 200 |      5.3636ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:55:11 | 200 |      6.6456ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:55:27 | 200 |       5.723ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:55:34 | 200 |       4.042ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:55:41 | 200 |      6.7795ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:55:57 | 200 |      5.7626ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:56:04 | 200 |       6.097ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:56:11 | 200 |      6.4923ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:56:27 | 200 |      6.1359ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:56:34 | 200 |      4.3545ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:56:41 | 200 |      4.8735ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:56:57 | 200 |      5.9184ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:57:04 | 200 |      7.5873ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:57:11 | 200 |      4.5789ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:57:27 | 200 |      6.7437ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:57:34 | 200 |       6.346ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:57:41 | 200 |      4.5377ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:57:57 | 200 |      6.6519ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:58:04 | 200 |      6.4619ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:58:11 | 200 |       4.619ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:58:27 | 200 |      5.8563ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:58:34 | 200 |      6.1188ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:58:41 | 200 |      6.0971ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:58:57 | 200 |      5.8415ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:59:04 | 200 |      6.6505ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:59:11 | 200 |      7.1117ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:59:27 | 200 |      5.4324ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:59:34 | 200 |      4.8024ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:59:41 | 200 |      6.6811ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 18:59:58 | 200 |      5.8376ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:00:04 | 200 |      5.4253ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:00:11 | 200 |      7.4145ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:00:28 | 200 |      5.5164ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:00:34 | 200 |      3.4841ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:00:41 | 200 |      5.2003ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:00:58 | 200 |      6.1552ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:01:04 | 200 |      6.8396ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:01:11 | 200 |      5.4448ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:01:28 | 200 |      4.5462ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:01:34 | 200 |      5.9693ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:01:41 | 200 |      5.8187ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:01:58 | 200 |      6.0331ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:02:04 | 200 |      5.8678ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:02:11 | 200 |       6.756ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:02:28 | 200 |      4.6934ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:02:34 | 200 |      5.1702ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:02:41 | 200 |      6.2286ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:02:58 | 200 |      4.3691ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:03:04 | 200 |      4.6359ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:03:11 | 200 |      9.2138ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:03:28 | 200 |      5.1068ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:03:34 | 200 |      5.2561ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:03:41 | 200 |      5.5328ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:03:58 | 200 |      4.8704ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:04:04 | 200 |      5.5595ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:04:11 | 200 |      5.5868ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:04:28 | 200 |      4.9525ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:04:34 | 200 |      5.5104ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:04:41 | 200 |        5.04ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:04:58 | 200 |      5.4093ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:05:04 | 200 |      5.8796ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:05:11 | 200 |      6.9566ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:05:28 | 200 |      5.8288ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:05:34 | 200 |      6.5951ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:05:41 | 200 |      5.2617ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:05:58 | 200 |      5.0981ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:06:04 | 200 |      5.5107ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:06:11 | 200 |       7.681ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:06:28 | 200 |      6.3346ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:06:34 | 200 |      4.7696ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:06:41 | 200 |      6.7404ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:06:58 | 200 |      6.4261ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:07:04 | 200 |      3.5068ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:07:11 | 200 |      6.7102ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:07:28 | 200 |      5.7283ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:07:34 | 200 |      6.1556ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:07:41 | 200 |      4.8161ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:07:58 | 200 |      5.6594ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:08:04 | 200 |      7.4543ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:08:11 | 200 |      4.2655ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:08:28 | 200 |      7.8319ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:08:34 | 200 |      4.8758ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:08:41 | 200 |      5.1323ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:08:58 | 200 |      5.0299ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:09:04 | 200 |      6.8471ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:09:11 | 200 |      5.9517ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:09:28 | 200 |      6.9064ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:09:34 | 200 |      6.2379ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:09:41 | 200 |      7.1385ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:09:58 | 200 |      5.1234ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:10:04 | 200 |      6.3384ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:10:11 | 200 |      7.0567ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:10:28 | 200 |      4.8391ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:10:34 | 200 |       6.143ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:10:41 | 200 |       4.993ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:10:58 | 200 |      4.6162ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:11:04 | 200 |      4.7893ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:11:11 | 200 |      6.2755ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:11:28 | 200 |      6.4105ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:11:34 | 200 |      4.3394ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:11:41 | 200 |      5.9756ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:11:58 | 200 |      5.4352ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:12:04 | 200 |      5.8588ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:12:11 | 200 |      6.5776ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:12:28 | 200 |      5.6926ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:12:34 | 200 |      5.5234ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:12:41 | 200 |      4.4341ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:12:58 | 200 |      4.9982ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:13:04 | 200 |      5.2572ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:13:11 | 200 |       6.282ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:13:28 | 200 |      5.9985ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:13:34 | 200 |      4.4455ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:13:41 | 200 |      5.0997ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:13:58 | 200 |      5.1049ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:14:04 | 200 |      6.5488ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:14:11 | 200 |      8.4839ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:14:28 | 200 |      4.2451ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:14:34 | 200 |      4.2958ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:14:41 | 200 |       5.042ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:14:58 | 200 |      4.1196ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:15:04 | 200 |       6.609ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:15:11 | 200 |      6.8451ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:15:28 | 200 |       5.862ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:15:34 | 200 |      6.1839ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:15:41 | 200 |      4.9431ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:15:58 | 200 |      6.0148ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:16:04 | 200 |      6.8621ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:16:11 | 200 |      5.1736ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:16:28 | 200 |      5.1672ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:16:34 | 200 |      7.3538ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:16:41 | 200 |      6.2168ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:16:58 | 200 |      6.3819ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:17:04 | 200 |      5.1557ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:17:11 | 200 |      5.9957ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:17:28 | 200 |       5.207ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:17:34 | 200 |      6.2522ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:17:41 | 200 |      5.1918ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:17:58 | 200 |      4.9546ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:18:04 | 200 |      5.9948ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:18:11 | 200 |      5.6945ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:18:28 | 200 |      5.7405ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:18:34 | 200 |      4.8466ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:18:41 | 200 |      5.3587ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:18:58 | 200 |      7.0868ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:19:04 | 200 |      5.4143ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:19:11 | 200 |      6.0868ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:19:28 | 200 |      5.0448ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:19:34 | 200 |       4.689ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:19:41 | 200 |      3.3856ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:19:58 | 200 |       6.307ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:20:04 | 200 |      5.9472ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:20:11 | 200 |     72.3543ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:20:28 | 200 |       4.856ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:20:34 | 200 |       5.248ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:20:41 | 200 |      4.8981ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:20:58 | 200 |      6.4036ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:21:04 | 200 |      6.3176ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:21:11 | 200 |      5.4941ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:21:28 | 200 |      7.0871ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:21:34 | 200 |      6.4468ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:21:41 | 200 |      4.8257ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:21:58 | 200 |      7.8342ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:22:04 | 200 |       5.237ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:22:11 | 200 |      4.2113ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:22:28 | 200 |      5.8124ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:22:34 | 200 |      4.7176ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:22:41 | 200 |      5.8157ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:22:58 | 200 |      7.9644ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:23:04 | 200 |       4.935ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:23:10 | 200 |      9.3781ms |             ::1 | GET      "/api/tags"
time=2025-08-02T19:23:10.582-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T19:23:10.667-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5330305024 required="2.5 GiB"
time=2025-08-02T19:23:10.691-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="66.6 GiB" free_swap="54.8 GiB"
time=2025-08-02T19:23:10.691-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=17 layers.offload=17 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.5 GiB" memory.required.partial="2.5 GiB" memory.required.kv="256.0 MiB" memory.required.allocations="[2.5 GiB]" memory.weights.total="1.2 GiB" memory.weights.repeating="986.2 MiB" memory.weights.nonrepeating="266.2 MiB" memory.graph.full="544.0 MiB" memory.graph.partial="554.3 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-02T19:23:11.238-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 --ctx-size 8192 --batch-size 512 --n-gpu-layers 17 --threads 4 --no-mmap --parallel 2 --port 59787"
time=2025-08-02T19:23:11.243-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-02T19:23:11.243-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-02T19:23:11.244-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-02T19:23:11.310-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-02T19:23:11.411-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-02T19:23:11.412-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:59787"
[GIN] 2025/08/02 - 19:23:11 | 200 |      5.5683ms |             ::1 | GET      "/api/tags"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-02T19:23:11.495-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 16 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 17/17 layers to GPU
load_tensors:    CUDA_Host model buffer size =   266.16 MiB
load_tensors:        CUDA0 model buffer size =  1252.41 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.99 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   256.00 MiB
llama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_context:      CUDA0 compute buffer size =   544.00 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 2
time=2025-08-02T19:23:12.247-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.00 seconds"
[GIN] 2025/08/02 - 19:23:14 | 200 |    4.3860314s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 19:23:28 | 200 |      5.0964ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:23:34 | 200 |      5.7883ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:23:41 | 200 |      5.5069ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:23:58 | 200 |      4.3087ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:24:04 | 200 |      5.8509ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:24:07 | 200 |       5.316ms |             ::1 | GET      "/api/tags"
time=2025-08-02T19:24:07.088-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/02 - 19:24:08 | 200 |    1.1592582s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 19:24:11 | 200 |      6.0022ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:24:28 | 200 |      5.8743ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:24:34 | 200 |      4.8871ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:24:41 | 200 |      7.0667ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:24:58 | 200 |      7.2565ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:25:04 | 200 |      5.9935ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:25:11 | 200 |      5.7742ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:25:28 | 200 |      6.0239ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:25:34 | 200 |      6.1529ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:25:41 | 200 |      6.9882ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:25:58 | 200 |      6.0269ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:26:04 | 200 |      6.7735ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:26:11 | 200 |      5.0677ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:26:28 | 200 |      3.8049ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:26:34 | 200 |       5.911ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:26:41 | 200 |      5.0707ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:26:58 | 200 |      4.0055ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:27:04 | 200 |      4.9315ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:27:11 | 200 |      4.6643ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:27:28 | 200 |      6.1617ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:27:34 | 200 |       5.993ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:27:41 | 200 |      4.3363ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:27:58 | 200 |      6.1179ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:28:04 | 200 |      5.8516ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:28:11 | 200 |      6.7204ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:28:28 | 200 |      5.5486ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:28:35 | 200 |      7.3229ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:28:41 | 200 |      6.0381ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:28:58 | 200 |      7.6664ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:29:05 | 200 |       5.097ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:29:11 | 200 |      6.0967ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:29:28 | 200 |      5.9058ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:29:35 | 200 |      4.0533ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:29:41 | 200 |      5.6963ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:29:58 | 200 |       8.831ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:30:05 | 200 |      6.0141ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:30:11 | 200 |       7.819ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:30:28 | 200 |      5.2916ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:30:35 | 200 |      4.0397ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:30:41 | 200 |      4.8863ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:30:58 | 200 |     10.9648ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:31:05 | 200 |      3.9977ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:31:11 | 200 |      5.5748ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:31:28 | 200 |      5.8535ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:31:35 | 200 |       4.769ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:31:41 | 200 |      5.8696ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:31:58 | 200 |      5.1013ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:32:05 | 200 |      7.9214ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:32:11 | 200 |      6.7634ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:32:28 | 200 |      5.6695ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:32:35 | 200 |      3.8077ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:32:41 | 200 |      5.1642ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:32:58 | 200 |      7.7396ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:33:05 | 200 |     14.4153ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:33:11 | 200 |      6.3956ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:33:28 | 200 |      3.8922ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:33:35 | 200 |      4.8495ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:33:41 | 200 |      4.3298ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:33:58 | 200 |      5.2897ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:34:05 | 200 |      3.6927ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:34:11 | 200 |       4.445ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:34:20 | 200 |     13.3715ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:34:28 | 200 |      5.5521ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:34:35 | 200 |      4.0913ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:34:41 | 200 |      4.2793ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:34:58 | 200 |      6.4363ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:35:05 | 200 |      5.1113ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:35:11 | 200 |      6.9579ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:35:28 | 200 |      5.7097ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:35:35 | 200 |      6.0349ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:35:41 | 200 |     11.1105ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:35:58 | 200 |      5.1286ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:36:05 | 200 |      5.9384ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:36:11 | 200 |      6.2627ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:36:28 | 200 |      6.4246ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:36:41 | 200 |      7.2385ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:36:46 | 200 |     16.3193ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/02 - 19:36:46 | 200 |    109.9618ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:36:46 | 200 |    115.2711ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:36:46 | 200 |     240.512ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:36:46 | 200 |    284.4705ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:36:46 | 200 |    291.1073ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:36:46 | 200 |    311.9436ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:36:46 | 200 |    329.4582ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:36:46 | 200 |    351.6957ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:36:46 | 200 |    356.5753ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:36:46 | 200 |    370.4134ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:36:46 | 200 |    398.0393ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:36:46 | 200 |    445.6632ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:36:46 | 200 |    457.5991ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:36:46 | 200 |    556.7204ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 19:57:53 | 200 |      5.2399ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 20:00:34 | 200 |      5.0785ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 20:01:16 | 200 |      5.2846ms |             ::1 | GET      "/api/tags"
time=2025-08-02T20:01:34.785-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:01:34.785-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
time=2025-08-02T20:01:34.873-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5297143808 required="2.5 GiB"
time=2025-08-02T20:01:34.887-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="72.0 GiB" free_swap="65.3 GiB"
time=2025-08-02T20:01:34.887-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=17 layers.offload=17 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.5 GiB" memory.required.partial="2.5 GiB" memory.required.kv="256.0 MiB" memory.required.allocations="[2.5 GiB]" memory.weights.total="1.2 GiB" memory.weights.repeating="986.2 MiB" memory.weights.nonrepeating="266.2 MiB" memory.graph.full="544.0 MiB" memory.graph.partial="554.3 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-02T20:01:35.465-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 --ctx-size 8192 --batch-size 512 --n-gpu-layers 17 --threads 4 --no-mmap --parallel 2 --port 61447"
time=2025-08-02T20:01:35.475-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-02T20:01:35.475-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-02T20:01:35.475-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-02T20:01:35.521-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-02T20:01:35.618-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-02T20:01:35.618-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:61447"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-08-02T20:01:35.727-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 16 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 17/17 layers to GPU
load_tensors:    CUDA_Host model buffer size =   266.16 MiB
load_tensors:        CUDA0 model buffer size =  1252.41 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.99 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   256.00 MiB
llama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_context:      CUDA0 compute buffer size =   544.00 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 2
time=2025-08-02T20:01:36.479-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.00 seconds"
[GIN] 2025/08/02 - 20:01:42 | 200 |     8.116792s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 20:05:24 | 200 |      5.1161ms |             ::1 | GET      "/api/tags"
time=2025-08-02T20:06:56.498-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:06:56.498-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
time=2025-08-02T20:06:56.568-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5285478400 required="2.5 GiB"
time=2025-08-02T20:06:56.590-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="71.7 GiB" free_swap="64.9 GiB"
time=2025-08-02T20:06:56.590-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=17 layers.offload=17 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.5 GiB" memory.required.partial="2.5 GiB" memory.required.kv="256.0 MiB" memory.required.allocations="[2.5 GiB]" memory.weights.total="1.2 GiB" memory.weights.repeating="986.2 MiB" memory.weights.nonrepeating="266.2 MiB" memory.graph.full="544.0 MiB" memory.graph.partial="554.3 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-02T20:06:57.154-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 --ctx-size 8192 --batch-size 512 --n-gpu-layers 17 --threads 4 --no-mmap --parallel 2 --port 61665"
time=2025-08-02T20:06:57.161-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-02T20:06:57.161-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-02T20:06:57.162-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-02T20:06:57.207-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-02T20:06:57.305-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-02T20:06:57.306-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:61665"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-08-02T20:06:57.413-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 16 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 17/17 layers to GPU
load_tensors:    CUDA_Host model buffer size =   266.16 MiB
load_tensors:        CUDA0 model buffer size =  1252.41 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.99 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   256.00 MiB
llama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_context:      CUDA0 compute buffer size =   544.00 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 2
time=2025-08-02T20:06:58.167-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.01 seconds"
[GIN] 2025/08/02 - 20:07:03 | 200 |    6.9932586s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 20:07:36 | 200 |       5.284ms |             ::1 | GET      "/api/tags"
time=2025-08-02T20:07:49.365-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:07:49.365-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:07:50 | 200 |    838.3116ms |             ::1 | POST     "/api/generate"
time=2025-08-02T20:08:16.260-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:08:16.260-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:08:20 | 200 |    3.9599683s |             ::1 | POST     "/api/generate"
time=2025-08-02T20:10:05.099-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:10:05.099-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:10:07 | 200 |    2.2983671s |             ::1 | POST     "/api/generate"
time=2025-08-02T20:10:15.981-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:10:15.981-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:10:20 | 200 |    4.3965482s |             ::1 | POST     "/api/generate"
time=2025-08-02T20:11:57.790-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:11:57.790-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:11:59 | 200 |    2.0261616s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 20:15:16 | 200 |       6.233ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 20:16:11 | 200 |      5.4191ms |             ::1 | GET      "/api/tags"
time=2025-08-02T20:16:17.603-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:16:17.603-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:16:18 | 200 |    862.9395ms |             ::1 | POST     "/api/generate"
time=2025-08-02T20:17:07.267-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:17:07.267-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:17:08 | 200 |    955.5464ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 20:19:31 | 200 |      5.8077ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 20:30:11 | 200 |       5.078ms |             ::1 | GET      "/api/tags"
time=2025-08-02T20:31:05.428-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:31:05.428-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
time=2025-08-02T20:31:05.505-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5281710080 required="2.5 GiB"
time=2025-08-02T20:31:05.525-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="71.6 GiB" free_swap="64.8 GiB"
time=2025-08-02T20:31:05.526-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=17 layers.offload=17 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.5 GiB" memory.required.partial="2.5 GiB" memory.required.kv="256.0 MiB" memory.required.allocations="[2.5 GiB]" memory.weights.total="1.2 GiB" memory.weights.repeating="986.2 MiB" memory.weights.nonrepeating="266.2 MiB" memory.graph.full="544.0 MiB" memory.graph.partial="554.3 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-02T20:31:06.093-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 --ctx-size 8192 --batch-size 512 --n-gpu-layers 17 --threads 4 --no-mmap --parallel 2 --port 62614"
time=2025-08-02T20:31:06.102-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-02T20:31:06.102-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-02T20:31:06.103-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-02T20:31:06.147-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-02T20:31:06.244-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-02T20:31:06.245-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:62614"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-08-02T20:31:06.354-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 16 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 17/17 layers to GPU
load_tensors:    CUDA_Host model buffer size =   266.16 MiB
load_tensors:        CUDA0 model buffer size =  1252.41 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.99 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   256.00 MiB
llama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_context:      CUDA0 compute buffer size =   544.00 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 2
time=2025-08-02T20:31:07.106-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.00 seconds"
[GIN] 2025/08/02 - 20:31:08 | 200 |    3.5429439s |             ::1 | POST     "/api/generate"
time=2025-08-02T20:32:09.569-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:32:09.569-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:32:11 | 200 |    2.4197578s |             ::1 | POST     "/api/generate"
time=2025-08-02T20:33:26.008-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:33:26.008-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:33:28 | 200 |    2.0870948s |             ::1 | POST     "/api/generate"
time=2025-08-02T20:50:39.493-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:50:39.493-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
time=2025-08-02T20:50:39.566-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5267488768 required="2.5 GiB"
time=2025-08-02T20:50:39.587-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="71.4 GiB" free_swap="64.6 GiB"
time=2025-08-02T20:50:39.587-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=17 layers.offload=17 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.5 GiB" memory.required.partial="2.5 GiB" memory.required.kv="256.0 MiB" memory.required.allocations="[2.5 GiB]" memory.weights.total="1.2 GiB" memory.weights.repeating="986.2 MiB" memory.weights.nonrepeating="266.2 MiB" memory.graph.full="544.0 MiB" memory.graph.partial="554.3 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-02T20:50:40.118-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 --ctx-size 8192 --batch-size 512 --n-gpu-layers 17 --threads 4 --no-mmap --parallel 2 --port 63349"
time=2025-08-02T20:50:40.125-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-02T20:50:40.125-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-02T20:50:40.126-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-02T20:50:40.170-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-02T20:50:40.278-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-02T20:50:40.278-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:63349"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-08-02T20:50:40.377-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 16 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 17/17 layers to GPU
load_tensors:    CUDA_Host model buffer size =   266.16 MiB
load_tensors:        CUDA0 model buffer size =  1252.41 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.99 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   256.00 MiB
llama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_context:      CUDA0 compute buffer size =   544.00 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 2
time=2025-08-02T20:50:41.128-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.00 seconds"
[GIN] 2025/08/02 - 20:50:43 | 200 |    4.5403002s |             ::1 | POST     "/api/generate"
time=2025-08-02T20:51:32.891-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:51:32.891-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:51:35 | 200 |    2.2848047s |             ::1 | POST     "/api/generate"
time=2025-08-02T20:52:26.107-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:52:26.107-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:52:29 | 200 |     2.985421s |             ::1 | POST     "/api/generate"
time=2025-08-02T20:55:02.282-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:55:02.282-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:55:03 | 200 |    1.0351296s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 20:57:42 | 200 |      5.2219ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 20:58:54 | 200 |      5.1134ms |             ::1 | GET      "/api/tags"
time=2025-08-02T20:59:05.004-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:59:05.004-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:59:05 | 200 |    404.3708ms |             ::1 | POST     "/api/generate"
time=2025-08-02T20:59:46.744-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T20:59:46.744-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=model
[GIN] 2025/08/02 - 20:59:47 | 200 |    900.9497ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 21:02:43 | 200 |      5.7905ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:03:23 | 200 |      5.1634ms |             ::1 | GET      "/api/tags"
time=2025-08-02T21:03:25.473-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T21:03:25.473-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=taskType
[GIN] 2025/08/02 - 21:03:25 | 200 |    461.6743ms |             ::1 | POST     "/api/generate"
time=2025-08-02T21:03:43.525-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T21:03:43.525-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=taskType
[GIN] 2025/08/02 - 21:03:43 | 200 |    487.0883ms |             ::1 | POST     "/api/generate"
time=2025-08-02T21:05:08.385-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T21:05:08.385-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=taskType
[GIN] 2025/08/02 - 21:05:08 | 200 |    484.8227ms |             ::1 | POST     "/api/generate"
time=2025-08-02T21:05:26.616-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T21:05:26.616-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=taskType
[GIN] 2025/08/02 - 21:05:27 | 200 |     539.225ms |             ::1 | POST     "/api/generate"
time=2025-08-02T21:06:17.516-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T21:06:17.516-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=taskType
[GIN] 2025/08/02 - 21:06:17 | 200 |    464.4754ms |             ::1 | POST     "/api/generate"
time=2025-08-02T21:06:49.262-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-02T21:06:49.262-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=taskType
[GIN] 2025/08/02 - 21:06:49 | 200 |    695.7609ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/02 - 21:20:42 | 200 |      6.3841ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:21:10 | 200 |      5.7712ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:21:40 | 200 |      5.7864ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:22:10 | 200 |      3.8368ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:22:40 | 200 |      4.5016ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:23:10 | 200 |      7.3007ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:23:40 | 200 |       6.551ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:24:10 | 200 |      5.0204ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:24:40 | 200 |      4.8875ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:25:10 | 200 |      5.7655ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:25:40 | 200 |      6.9903ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:26:10 | 200 |      5.2926ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:26:40 | 200 |       4.853ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:27:10 | 200 |      7.1037ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:27:40 | 200 |      3.9489ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:28:10 | 200 |      6.8668ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:28:40 | 200 |      5.2418ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:29:10 | 200 |      5.5123ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:29:40 | 200 |      6.1386ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:30:10 | 200 |      6.7019ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:30:40 | 200 |      6.5099ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:31:10 | 200 |       5.708ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:31:40 | 200 |      6.1645ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:32:10 | 200 |      4.9194ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:32:40 | 200 |      5.5324ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:33:10 | 200 |      7.1014ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:33:40 | 200 |      5.6772ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:34:10 | 200 |      5.6864ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:34:40 | 200 |      7.3339ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:35:10 | 200 |     129.074ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:35:40 | 200 |       6.187ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:36:10 | 200 |      5.0894ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:36:32 | 200 |      5.2211ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:37:00 | 200 |      6.4913ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:37:30 | 200 |      5.2478ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:38:00 | 200 |      5.5782ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:38:30 | 200 |       5.063ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:39:00 | 200 |      5.6499ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:39:30 | 200 |      5.9878ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:40:00 | 200 |      5.8096ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:40:30 | 200 |      6.6345ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:41:00 | 200 |      4.9366ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:41:30 | 200 |      5.3808ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:42:00 | 200 |      5.6638ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:42:30 | 200 |      4.9215ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:43:00 | 200 |      6.1823ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:43:30 | 200 |      5.4074ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:44:00 | 200 |      6.0426ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:44:30 | 200 |      6.1098ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:45:00 | 200 |      5.0402ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:45:30 | 200 |      6.3827ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:46:26 | 200 |      5.4671ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:46:54 | 200 |      6.1434ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:47:24 | 200 |      5.1268ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:47:54 | 200 |      5.6474ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:48:24 | 200 |      6.8062ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:48:54 | 200 |      5.4078ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:49:24 | 200 |      6.5198ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:49:54 | 200 |       4.796ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:50:24 | 200 |      6.2019ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:50:54 | 200 |      5.1375ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:51:24 | 200 |      5.4543ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:51:54 | 200 |      6.1766ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:52:24 | 200 |      5.3069ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:52:54 | 200 |      5.8274ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:53:24 | 200 |      4.5701ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:53:42 | 200 |      6.0897ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:54:10 | 200 |      4.5352ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 21:54:40 | 200 |      4.8753ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/02 - 22:04:29 | 200 |      5.8156ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/02 - 22:04:30 | 200 |     97.1802ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 22:04:30 | 200 |    127.3244ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 22:04:30 | 200 |    127.1785ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 22:04:30 | 200 |    174.8217ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 22:04:30 | 200 |     197.393ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 22:04:30 | 200 |    202.1178ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 22:04:30 | 200 |    227.7508ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 22:04:30 | 200 |    223.8805ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 22:04:30 | 200 |    230.0239ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 22:04:30 | 200 |    254.5379ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 22:04:30 | 200 |    255.5576ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 22:04:30 | 200 |    267.5673ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 22:04:30 | 200 |    277.9056ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/02 - 22:04:30 | 200 |    348.0993ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:29 | 200 |     18.0651ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 10:23:29 | 200 |    627.1567ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:29 | 200 |     779.081ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:29 | 200 |    918.9033ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:30 | 200 |    769.5551ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:30 | 200 |    775.2385ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:30 | 200 |    937.8661ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:30 | 200 |    1.1301392s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:30 | 200 |    1.1527314s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:30 | 200 |    1.2833169s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:30 | 200 |    1.2930219s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:30 | 200 |    1.2967442s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:30 | 200 |    1.1528828s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:30 | 200 |    1.3137306s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:23:30 | 200 |    1.2565249s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 10:53:35 | 200 |       555.5µs |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/08/03 - 10:53:41 | 200 |      7.8634ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 10:58:08 | 200 |      8.1953ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 10:58:36 | 200 |     14.5915ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 10:59:07 | 200 |    149.3631ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 10:59:36 | 200 |      13.114ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:00:07 | 200 |    104.8173ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:00:36 | 200 |     12.7284ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:01:06 | 200 |      9.3039ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:01:36 | 200 |      9.3944ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:02:07 | 200 |     97.7972ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:02:37 | 200 |       9.353ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:03:07 | 200 |     95.0934ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:03:37 | 200 |     12.1219ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:04:07 | 200 |     13.0196ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:04:37 | 200 |     14.4334ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:05:07 | 200 |     11.5606ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:05:38 | 200 |      13.097ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:06:06 | 200 |     13.6845ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:06:06.133-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:06:06.133-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:06:06.136-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:06:06.136-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:06:06.302-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="67.7 GiB" free_swap="61.8 GiB"
time=2025-08-03T11:06:06.304-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=11 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="20.7 GiB" memory.required.partial="4.9 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="18.1 GiB" memory.weights.repeating="17.5 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="348.0 MiB" memory.graph.partial="916.1 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 771 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-ac3d1ba8aa77755dab3806d9024e9c385ea0d5b412d6bdf9157f8a4a7e9fc0d9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 32B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 32B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 32B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 64
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 18.48 GiB (4.85 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 32.76 B
print_info: general.name     = Qwen2.5 Coder 32B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-03T11:06:07.184-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-ac3d1ba8aa77755dab3806d9024e9c385ea0d5b412d6bdf9157f8a4a7e9fc0d9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 11 --threads 4 --no-mmap --parallel 1 --port 52111"
time=2025-08-03T11:06:07.197-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T11:06:07.197-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T11:06:07.198-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T11:06:07.330-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T11:06:07.563-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T11:06:07.565-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:52111"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-03T11:06:07.703-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 771 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-ac3d1ba8aa77755dab3806d9024e9c385ea0d5b412d6bdf9157f8a4a7e9fc0d9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 32B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 32B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 32B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 64
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 18.48 GiB (4.85 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 5120
print_info: n_layer          = 64
print_info: n_head           = 40
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 5
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 27648
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 32B
print_info: model params     = 32.76 B
print_info: general.name     = Qwen2.5 Coder 32B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 11 repeating layers to GPU
load_tensors: offloaded 11/65 layers to GPU
load_tensors:    CUDA_Host model buffer size = 15305.59 MiB
load_tensors:        CUDA0 model buffer size =  3202.76 MiB
load_tensors:          CPU model buffer size =   417.66 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   176.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =   848.00 MiB
llama_kv_cache_unified: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      CUDA0 compute buffer size =   926.08 MiB
llama_context:  CUDA_Host compute buffer size =    18.01 MiB
llama_context: graph nodes  = 2374
llama_context: graph splits = 746 (with bs=512), 3 (with bs=1)
time=2025-08-03T11:06:26.497-04:00 level=INFO source=server.go:630 msg="llama runner started in 19.30 seconds"
[GIN] 2025/08/03 - 11:06:36 | 500 |   30.0852732s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:06:36.165-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/03 - 11:06:36 | 500 |   30.0920457s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:06:36 | 200 |     14.3932ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:06:36.799-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:06:36.799-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:06:36.866-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:06:36.867-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:06:54 | 200 |    18.740955s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:07:06 | 500 |   29.9970565s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:07:06 | 200 |    165.9637ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:07:06.571-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:07:06.571-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:07:06.582-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:07:06.582-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:07:34 | 200 |   28.3491356s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:07:36.019-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:52111/completion\": context canceled"
[GIN] 2025/08/03 - 11:07:36 | 500 |   30.0156448s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:07:36 | 200 |     99.8132ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:07:36.257-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:07:36.257-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:07:36.347-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:07:36.347-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:08:00 | 200 |   24.8074034s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:08:06.092-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:08:06.092-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:08:06.113-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:52111/completion\": context canceled"
[GIN] 2025/08/03 - 11:08:06 | 500 |   30.0107911s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:08:06 | 200 |     10.7378ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:08:06.169-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:08:06.169-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:08:30 | 200 |   24.1930879s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:08:36.204-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:52111/completion\": context canceled"
[GIN] 2025/08/03 - 11:08:36 | 500 |   30.0905369s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:08:36 | 200 |    167.0416ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:08:36.615-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:08:36.615-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:08:36.636-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:08:36.636-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:09:06 | 500 |   30.1105596s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:09:06 | 500 |   30.0244479s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:09:06 | 200 |     51.6592ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:09:06.432-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:09:06.432-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:09:06.464-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:09:06.465-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:09:36.299-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/03 - 11:09:36 | 500 |   30.1230764s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:09:36 | 500 |   30.1326599s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:09:36 | 200 |     42.4624ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:09:36.464-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:09:36.464-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:09:36.474-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:09:36.474-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:10:06 | 500 |   30.0362103s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:10:06.213-04:00 level=INFO source=runner.go:590 msg="aborting completion request due to client closing the connection"
time=2025-08-03T11:10:06.224-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:52111/completion\": context canceled"
[GIN] 2025/08/03 - 11:10:06 | 500 |   29.9548618s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:10:06 | 200 |     53.7717ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:10:07.172-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:10:07.173-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:10:07.180-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:10:07.181-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:10:36 | 500 |    29.987859s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:10:36.131-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:10:36.131-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:10:36.223-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:10:36.223-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:10:36 | 200 |       9.544ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:10:36.230-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:52111/completion\": context canceled"
time=2025-08-03T11:10:36.230-04:00 level=INFO source=runner.go:590 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/03 - 11:10:36 | 500 |   29.9940201s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:10:55 | 200 |   19.2277473s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:11:06.132-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:11:06.132-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:11:06 | 500 |   30.0034215s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:11:06.222-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:11:06.222-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:11:06 | 200 |      8.1355ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:11:36 | 500 |   30.2157062s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:11:36.353-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:52111/completion\": context canceled"
time=2025-08-03T11:11:36.350-04:00 level=INFO source=runner.go:590 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/03 - 11:11:36 | 500 |   30.1798171s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:11:36 | 200 |    176.6707ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:11:36.871-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:11:36.871-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:11:36.882-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:11:36.883-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:12:06 | 500 |   30.0264132s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:12:06 | 500 |   30.1405692s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:12:06 | 200 |    402.8917ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:12:07.142-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:12:07.142-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:12:07.171-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:12:07.171-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:12:32 | 200 |   25.9373948s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:12:36.146-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:12:36.146-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:12:36.247-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:52111/completion\": context canceled"
[GIN] 2025/08/03 - 11:12:36 | 500 |   29.9524504s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:12:36.337-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:12:36.337-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:12:36 | 200 |      5.4344ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:12:51 | 200 |   15.5690043s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:13:06.157-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:13:06.157-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:13:06 | 500 |   30.0054553s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:13:06.349-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:13:06.349-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:13:06 | 200 |       8.115ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:13:23 | 200 |   17.0470303s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:13:36.184-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:13:36.184-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:13:36 | 500 |   30.0007172s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:13:36.348-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:13:36.348-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:13:36 | 200 |      9.5881ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:13:54 | 200 |   18.1209848s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:14:06.174-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:14:06.174-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:14:06 | 500 |   30.0129333s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:14:06.365-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:14:06.365-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:14:06 | 200 |      7.2193ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:14:23 | 200 |   17.3292513s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:14:36.199-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:14:36.199-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:14:36 | 500 |   30.0044145s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:14:36.372-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:14:36.372-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:14:36 | 200 |     10.3233ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:14:51 | 200 |   15.5816684s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:15:06.196-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:15:06.196-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:15:06 | 500 |   30.0103027s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:15:06.362-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:15:06.362-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:15:06 | 200 |      6.5756ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:15:21 | 200 |   15.4009196s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:15:36.207-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:15:36.207-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:15:36 | 500 |   30.0163993s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:15:36 | 200 |     12.4944ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:15:36.390-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:15:36.390-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:15:57 | 200 |   21.2521666s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:16:06.217-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:16:06.217-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:16:06 | 500 |   30.0055519s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:16:06 | 200 |      7.2752ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:16:06.391-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:16:06.391-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:16:22 | 200 |   16.7532406s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:16:36.224-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:16:36.224-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:16:36 | 500 |   30.0064828s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:16:36 | 200 |        7.29ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:16:36.402-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:16:36.402-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:16:55 | 200 |   19.3770846s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:17:06.237-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:17:06.237-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:17:06 | 500 |   30.0113192s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:17:06 | 200 |      7.6961ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:17:06.411-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:17:06.411-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:17:24 | 200 |   18.1169977s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:17:36.255-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:17:36.255-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:17:36 | 500 |   30.0130444s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:17:36 | 200 |      7.5502ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:17:36.429-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:17:36.429-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:17:55 | 200 |   19.6998009s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:18:06.258-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:18:06.258-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:18:06 | 500 |   30.0038736s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:18:06 | 200 |      8.0899ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:18:06.432-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:18:06.432-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:18:23 | 200 |   17.0308072s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:18:36.254-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:18:36.254-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:18:36 | 500 |   30.0045273s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:18:36 | 200 |      6.2233ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:18:36.428-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:18:36.428-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:18:51 | 200 |   15.0902186s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:19:05 | 200 |   29.1694538s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:19:06.263-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:19:06.263-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:19:06 | 200 |      5.4682ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:19:06.442-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:19:06.442-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:19:20 | 200 |   13.7897811s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:19:36.274-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:19:36.275-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:19:36 | 500 |   30.0055037s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:19:36 | 200 |      8.1762ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:19:36.450-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:19:36.450-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:19:51 | 200 |    15.061408s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:20:06.282-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:20:06.282-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:20:06 | 500 |   30.0051388s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:20:06 | 200 |      4.0487ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:20:06.450-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:20:06.450-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:20:22 | 200 |   15.9252126s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:20:36.287-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:20:36.287-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:20:36 | 500 |   30.0073122s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:20:36 | 200 |      5.2793ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:20:36.457-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:20:36.457-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:20:50 | 200 |   14.2868201s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:21:03 | 200 |     26.66772s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:21:06.286-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:21:06.286-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:21:06 | 200 |      8.3139ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:21:06.471-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:21:06.471-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:21:17 | 200 |   11.5356447s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:21:31 | 200 |   25.5189844s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:21:36.301-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:21:36.302-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:21:36 | 200 |      7.1463ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:21:36.486-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:21:36.486-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:21:50 | 200 |     14.51225s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:21:57 | 500 |   20.5789986s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:22:17 | 200 |      5.3705ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:22:45 | 200 |     11.1699ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:22:45.787-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:22:45.787-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:22:45.787-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:22:45.788-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:22:45.916-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="700.5 MiB"
time=2025-08-03T11:22:47.868-04:00 level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 parallel=2 available=5369782272 required="2.5 GiB"
time=2025-08-03T11:22:47.907-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="67.8 GiB" free_swap="61.6 GiB"
time=2025-08-03T11:22:47.910-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=17 layers.offload=17 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.5 GiB" memory.required.partial="2.5 GiB" memory.required.kv="256.0 MiB" memory.required.allocations="[2.5 GiB]" memory.weights.total="1.2 GiB" memory.weights.repeating="986.2 MiB" memory.weights.nonrepeating="266.2 MiB" memory.graph.full="544.0 MiB" memory.graph.partial="554.3 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-03T11:22:48.864-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 --ctx-size 8192 --batch-size 512 --n-gpu-layers 17 --threads 4 --no-mmap --parallel 2 --port 49471"
time=2025-08-03T11:22:48.876-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T11:22:48.876-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T11:22:48.877-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T11:22:49.002-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T11:22:49.217-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T11:22:49.218-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:49471"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-03T11:22:49.380-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 1B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 16
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  18:                          general.file_type u32              = 7
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   34 tensors
llama_model_loader: - type q8_0:  113 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.22 GiB (8.50 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2048
print_info: n_layer          = 16
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1B
print_info: model params     = 1.24 B
print_info: general.name     = Llama 3.2 1B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 16 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 17/17 layers to GPU
load_tensors:    CUDA_Host model buffer size =   266.16 MiB
load_tensors:        CUDA0 model buffer size =  1252.41 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.99 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   256.00 MiB
llama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_context:      CUDA0 compute buffer size =   544.00 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 550
llama_context: graph splits = 2
time=2025-08-03T11:22:50.910-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.03 seconds"
[GIN] 2025/08/03 - 11:22:51 | 200 |    5.9346019s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:22:51 | 200 |    5.9796081s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:23:15 | 200 |     10.2397ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:23:15.742-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:23:15.742-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:23:15.789-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:23:15.789-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:23:16 | 200 |    496.0694ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:23:16 | 200 |    554.1171ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:23:45.753-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:23:45.753-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:23:45 | 200 |       8.026ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:23:45.802-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:23:45.802-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:23:46 | 200 |    541.1532ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:23:46 | 200 |    663.2337ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:24:15 | 200 |     15.1133ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:24:15.783-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:24:15.783-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:24:15.822-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:24:15.822-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:24:16 | 200 |    625.7827ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:24:16 | 200 |    767.5911ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:24:45 | 200 |      8.6711ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:24:45.784-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:24:45.784-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:24:45.821-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:24:45.821-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:24:46 | 200 |    616.5453ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:24:46 | 200 |     1.006886s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:25:15.788-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:25:15.788-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:25:15 | 200 |     10.9932ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:25:15.852-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:25:15.852-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:25:16 | 200 |    823.9901ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:25:16 | 200 |    951.8941ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:25:45 | 200 |       9.272ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:25:45.798-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:25:45.798-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:25:45.841-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:25:45.841-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:25:46 | 200 |     672.566ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:25:46 | 200 |    743.0952ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:26:15.773-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:26:15.774-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:26:15 | 200 |      5.4377ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:26:15.821-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:26:15.821-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:26:16 | 200 |    499.4738ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:26:16 | 200 |    580.0998ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:26:45.790-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:26:45.791-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:26:45 | 200 |      7.5567ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:26:45.838-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:26:45.838-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:26:46 | 200 |    318.9851ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:26:46 | 200 |    520.4341ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:27:15.800-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:27:15.800-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:27:15 | 200 |      6.4829ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:27:15.843-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:27:15.843-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:27:16 | 200 |    490.0508ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:27:16 | 200 |    490.7158ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:27:45.802-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:27:45.802-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:27:45 | 200 |      6.5671ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:27:45.847-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:27:45.847-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:27:46 | 200 |    309.6947ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:27:46 | 200 |    459.7062ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:28:15.825-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:28:15.825-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:28:15 | 200 |     10.1088ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:28:15.865-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:28:15.865-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:28:16 | 200 |     458.204ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:28:16 | 200 |    565.1437ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:28:45.821-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:28:45.821-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:28:45 | 200 |      7.1864ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:28:45.868-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:28:45.868-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:28:46 | 200 |    523.5045ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:28:46 | 200 |    768.0335ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:29:15.826-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:29:15.826-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:29:15 | 200 |      7.2276ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:29:15.871-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:29:15.871-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:29:16 | 200 |     503.661ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:29:16 | 200 |     670.206ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:29:45.835-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:29:45.835-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:29:45 | 200 |      7.4545ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:29:45.876-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:29:45.876-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:29:46 | 200 |    440.5821ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:29:46 | 200 |    443.5946ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:30:15.840-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:30:15.840-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:30:15 | 200 |       8.921ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:30:15.882-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:30:15.883-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:30:16 | 200 |    635.9038ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:30:16 | 200 |    590.6783ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:30:45.835-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:30:45.835-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:30:45 | 200 |      9.0807ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:30:45.898-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:30:45.898-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:30:46 | 200 |    619.1072ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:30:46 | 200 |    573.9042ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:31:15.841-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:31:15.841-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:31:15 | 200 |      5.8646ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:31:15.901-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:31:15.901-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:31:16 | 200 |    397.4142ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:31:16 | 200 |    474.1686ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:31:45.847-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:31:45.847-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:31:45 | 200 |      5.8923ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:31:45.905-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:31:45.905-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:31:46 | 200 |    402.6168ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:31:46 | 200 |    402.5359ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:32:32 | 200 |      5.5269ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:33:00 | 200 |      8.5661ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:33:00.681-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:33:00.681-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:33:00.681-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:33:00.681-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:33:01 | 200 |    416.7367ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:33:01 | 200 |    474.9243ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:33:30 | 200 |      6.1262ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:33:30.677-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:33:30.677-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:33:30.693-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:33:30.693-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:33:31 | 200 |    466.6591ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:33:31 | 200 |    552.2534ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:34:00 | 200 |      6.4834ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:34:00.679-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:34:00.679-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:34:00.693-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:34:00.693-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:34:01 | 200 |    467.6656ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:34:01 | 200 |      519.42ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:34:30 | 200 |      7.6362ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:34:30.682-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:34:30.682-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:34:30.694-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:34:30.694-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:34:31 | 200 |    380.2379ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:34:31 | 200 |    518.9036ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:35:00 | 200 |       7.729ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:35:00.681-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:35:00.682-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:35:00.700-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:35:00.700-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:35:01 | 200 |    380.6677ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:35:01 | 200 |    505.0375ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:35:30 | 200 |        9.23ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:35:30.700-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:35:30.700-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:35:30.716-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:35:30.716-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:35:31 | 200 |    498.0648ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:35:31 | 200 |    530.2883ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:36:00 | 200 |     25.9723ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:36:00.707-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:36:00.707-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:36:00.722-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:36:00.722-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:36:01 | 200 |    432.6328ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:36:01 | 200 |    540.3644ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:36:30 | 200 |      9.6705ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:36:30.710-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:36:30.710-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:36:30.712-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:36:30.712-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:36:31 | 200 |    387.4438ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:36:31 | 200 |    405.0451ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:37:00 | 200 |      8.9163ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:37:00.725-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:37:00.725-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:37:00.725-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:37:00.725-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:37:01 | 200 |     468.466ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:37:01 | 200 |    565.3314ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:37:30 | 200 |     10.8428ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:37:30.738-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:37:30.738-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:37:30.738-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:37:30.738-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:37:31 | 200 |    446.0991ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:37:31 | 200 |    599.1559ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:38:00 | 200 |       9.438ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:38:00.742-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:38:00.742-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:38:00.742-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:38:00.742-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:38:01 | 200 |    584.8576ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:38:01 | 200 |    616.8213ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:38:30 | 200 |      9.7199ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:38:30.754-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:38:30.754-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:38:30.754-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:38:30.754-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:38:31 | 200 |    321.0482ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:38:31 | 200 |     473.346ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:39:00 | 200 |      9.4734ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:39:00.775-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:39:00.775-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:39:00.776-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:39:00.776-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:39:01 | 200 |    515.1017ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:39:01 | 200 |    531.7898ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:39:30 | 200 |     23.7786ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:39:30.794-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:39:30.794-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:39:30.794-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:39:30.794-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:39:31 | 200 |    557.4685ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:39:31 | 200 |    588.6988ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:40:00 | 200 |     15.6522ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:40:00.791-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:40:00.791-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:40:00.791-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:40:00.791-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:40:01 | 200 |    574.9977ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:40:01 | 200 |    672.3032ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:40:30 | 200 |      4.8671ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:40:30.794-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:40:30.795-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:40:30.797-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:40:30.797-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:40:31 | 200 |    493.8605ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:40:31 | 200 |    601.4774ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:41:00 | 200 |     13.0258ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:41:00.809-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:41:00.809-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:41:00.809-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:41:00.810-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:41:01 | 200 |    606.6981ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:41:01 | 200 |    607.2444ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:42:33 | 200 |      6.6903ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:43:01 | 200 |     10.3111ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:43:01.942-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:43:01.942-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:43:01.944-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:43:01.944-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:43:02 | 200 |    375.9286ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:43:02 | 200 |    462.4839ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:43:31 | 200 |      8.8224ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:43:31.954-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:43:31.954-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:43:31.975-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:43:31.976-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:43:32 | 200 |    536.4527ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:43:32 | 200 |     680.803ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:44:01 | 200 |      8.1871ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:44:01.947-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:44:01.947-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:44:01.947-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:44:01.947-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:44:02 | 200 |    517.6547ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:44:02 | 200 |    535.2513ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:44:31 | 200 |     12.3758ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:44:31.976-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:44:31.978-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:44:31.989-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:44:31.989-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:44:32 | 200 |    418.6282ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:44:32 | 200 |     599.586ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:45:01 | 200 |     12.3838ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:45:01.960-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:45:01.960-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:45:01.960-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:45:01.960-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:45:02 | 200 |    451.9056ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:45:02 | 200 |    468.5744ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:45:25.681-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:45:25.681-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T11:45:25.681-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
[GIN] 2025/08/03 - 11:45:31 | 200 |     11.2039ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:45:31.963-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:45:31.963-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:45:31.963-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:45:31.963-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:45:32 | 200 |    642.5113ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:45:32 | 200 |    7.0315971s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:45:32 | 200 |    1.0644833s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:46:01 | 200 |      8.7385ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:46:01.969-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:46:01.969-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:46:01.969-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:46:01.969-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:46:02 | 200 |    696.8487ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:46:02 | 200 |     730.712ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:46:31 | 200 |     10.4646ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:46:31.981-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:46:31.982-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:46:31.981-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:46:31.982-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:46:32 | 200 |    535.7523ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:46:32 | 200 |    629.9329ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:46:56.522-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:46:56.522-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T11:46:56.522-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
[GIN] 2025/08/03 - 11:46:59 | 200 |    3.1042112s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:47:01 | 200 |      8.3572ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:47:01.982-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:47:01.982-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:47:01.985-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:47:01.985-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:47:02 | 200 |    412.7154ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:47:02 | 200 |    427.1409ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:47:31 | 200 |     10.9898ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:47:32.010-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:47:32.010-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:47:32.011-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:47:32.011-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:47:32 | 200 |    336.6819ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:47:32 | 200 |    631.4184ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:48:01 | 200 |      7.3575ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:48:02.005-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:48:02.006-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:48:02.006-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:48:02.006-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:48:02 | 200 |    475.2558ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:48:02 | 200 |     582.993ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:48:28.240-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:48:28.240-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T11:48:28.240-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
[GIN] 2025/08/03 - 11:48:28 | 200 |    397.0161ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:48:31 | 200 |      9.9316ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:48:32.015-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:48:32.015-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:48:32.022-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:48:32.022-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:48:32 | 200 |    612.5301ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:48:32 | 200 |    627.6404ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:48:45.467-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:48:45.467-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T11:48:45.467-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
[GIN] 2025/08/03 - 11:48:50 | 200 |    5.0601998s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:49:01 | 200 |      7.6644ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:49:02.037-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:49:02.037-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:49:02.041-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:49:02.041-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:49:02 | 200 |    403.7601ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:49:02 | 200 |    482.4011ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:49:30.958-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:49:30.958-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T11:49:30.958-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
[GIN] 2025/08/03 - 11:49:31 | 200 |     10.7939ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:49:32.023-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:49:32.025-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:49:32.025-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:49:32.025-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:49:32 | 200 |    426.6634ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:49:32 | 200 |    751.4923ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:49:35 | 200 |    4.1828036s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:50:02 | 200 |      41.225ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:50:02.089-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:50:02.089-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:50:02.099-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:50:02.099-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:50:02 | 200 |    498.8892ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:50:02 | 200 |    595.7811ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:50:32 | 200 |      7.3507ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:50:32.043-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:50:32.043-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:50:32.043-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:50:32.043-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:50:32 | 200 |    546.7245ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:50:32 | 200 |    563.0956ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:51:02 | 200 |      8.5617ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:51:02.059-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:51:02.059-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:51:02.059-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:51:02.059-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:51:02 | 200 |     574.034ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:51:02 | 200 |    624.7337ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:51:32 | 200 |      8.9915ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:51:32.072-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:51:32.072-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:51:32.072-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:51:32.072-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:51:32 | 200 |    556.5351ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:51:32 | 200 |      573.11ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:52:02 | 200 |      6.5422ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:52:02.079-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:52:02.079-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:52:02.079-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:52:02.079-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:52:02 | 200 |    523.6078ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:52:02 | 200 |    590.4164ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:52:32 | 200 |     10.7766ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:52:32.093-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:52:32.093-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:52:32.093-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:52:32.093-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:52:32 | 200 |    429.8759ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:52:32 | 200 |    699.6096ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:53:02 | 200 |      7.3154ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:53:02.098-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:53:02.098-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:53:02.098-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:53:02.098-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:53:02 | 200 |    431.3985ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:53:02 | 200 |    537.9331ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:53:32 | 200 |      9.4707ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:53:32.107-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:53:32.107-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:53:32.107-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:53:32.107-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:53:32 | 200 |    430.0559ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:53:32 | 200 |    492.0578ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:54:03 | 200 |      5.6099ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 11:54:31 | 200 |      6.7907ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:54:31.980-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:54:31.980-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:54:31.983-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:54:31.983-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:54:32 | 200 |     428.866ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:54:32 | 200 |    581.2384ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:55:01 | 200 |      8.5109ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:55:01.972-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:55:01.972-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:55:01.986-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:55:01.986-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:55:02 | 200 |    327.7913ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:55:02 | 200 |    390.9768ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:55:31 | 200 |      9.7953ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:55:31.991-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:55:31.991-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:55:32.001-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:55:32.001-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:55:32 | 200 |     403.347ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:55:32 | 200 |    452.5155ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:56:01 | 200 |      6.2516ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:56:01.988-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:56:01.988-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:56:02.007-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:56:02.007-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:56:02 | 200 |    474.5997ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:56:02 | 200 |    475.5594ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:56:31 | 200 |      7.1235ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:56:31.991-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:56:31.991-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:56:32.009-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:56:32.009-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:56:32.394-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:56:32.394-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T11:56:32.394-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
[GIN] 2025/08/03 - 11:56:32 | 200 |    450.9583ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:56:32 | 200 |    586.6468ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:56:32 | 200 |    580.3704ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:57:01 | 200 |      7.4399ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:57:01.996-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:57:01.996-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:57:02.010-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:57:02.010-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:57:02 | 200 |     380.758ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:57:02 | 200 |    435.6277ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:57:31 | 200 |      8.6943ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:57:32.006-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:57:32.006-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:57:32.023-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:57:32.023-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:57:32 | 200 |    388.1152ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:57:32 | 200 |    373.1828ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:57:56.942-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T11:57:56.942-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
time=2025-08-03T11:57:56.942-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:58:02 | 200 |      5.4695ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:58:02.014-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:58:02.014-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:58:02.031-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:58:02.031-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:58:02 | 200 |    431.3125ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:58:02 | 200 |    846.5231ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:58:04 | 200 |    7.8329028s |             ::1 | POST     "/api/generate"
time=2025-08-03T11:58:17.921-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:58:17.921-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T11:58:17.921-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
[GIN] 2025/08/03 - 11:58:22 | 200 |    5.0882248s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:58:32 | 200 |      8.9986ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:58:32.029-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:58:32.029-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:58:32.029-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:58:32.029-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:58:32 | 200 |     452.543ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:58:32 | 200 |    486.4265ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:59:02 | 200 |      9.1314ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:59:02.050-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:59:02.050-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:59:02.050-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:59:02.050-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:59:02 | 200 |     493.502ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:59:02 | 200 |    600.5878ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:59:32 | 200 |     11.7492ms |             ::1 | GET      "/api/tags"
time=2025-08-03T11:59:32.054-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:59:32.054-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T11:59:32.054-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T11:59:32.054-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 11:59:32 | 200 |    386.9904ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 11:59:32 | 200 |    478.7719ms |             ::1 | POST     "/api/generate"
time=2025-08-03T11:59:39.840-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T11:59:39.840-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
time=2025-08-03T11:59:39.840-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 11:59:45 | 200 |    5.6695857s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:00:02 | 200 |      9.6305ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:00:02.052-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:00:02.052-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T12:00:02.052-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:00:02.052-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 12:00:02 | 200 |    511.8737ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:00:02 | 200 |    674.8652ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:00:32 | 200 |     10.3778ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:00:32.064-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:00:32.064-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T12:00:32.064-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:00:32.064-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 12:00:32 | 200 |    488.9156ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:00:32 | 200 |    622.0237ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:01:02 | 200 |     12.3026ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:01:02.064-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:01:02.064-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T12:01:02.064-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:01:02.064-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 12:01:02 | 200 |    441.0622ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:01:02 | 200 |    533.2176ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:01:32 | 200 |      9.3383ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:01:32.084-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:01:32.084-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T12:01:32.084-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:01:32.084-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 12:01:32 | 200 |     510.054ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:01:32 | 200 |    527.5275ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:02:02 | 200 |     15.0277ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:02:02.088-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:02:02.088-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T12:02:02.094-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:02:02.094-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 12:02:02 | 200 |    432.9433ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:02:02 | 200 |    510.1536ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:02:32 | 200 |     12.5544ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:02:32.094-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:02:32.094-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T12:02:32.094-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:02:32.094-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 12:02:32 | 200 |    506.4039ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:02:32 | 200 |    584.7157ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:03:02 | 200 |       9.894ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:03:02.105-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:03:02.105-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T12:03:02.105-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:03:02.105-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 12:03:02 | 200 |    481.2163ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:03:02 | 200 |    691.4265ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:03:32 | 200 |      8.8582ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:03:32.105-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:03:32.105-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T12:03:32.105-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:03:32.106-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 12:03:32 | 200 |    425.8655ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:03:32 | 200 |    535.6119ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:03:55 | 200 |      5.7616ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:04:14.918-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T12:04:14.918-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
time=2025-08-03T12:04:14.918-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 12:04:18 | 200 |    3.4073324s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:04:23 | 200 |      7.0271ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:04:23.682-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:04:23.682-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T12:04:23.682-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:04:23.682-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 12:04:24 | 200 |    385.7211ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:04:24 | 200 |    554.4102ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:04:53 | 200 |      7.1365ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:04:53.689-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:04:53.689-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T12:04:53.689-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:04:53.689-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 12:04:54 | 200 |     418.831ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:04:54 | 200 |    494.2087ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:05:23 | 200 |       7.606ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:05:23.702-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:05:23.702-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T12:05:23.702-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:05:23.702-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 12:05:24 | 200 |    578.6954ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:05:24 | 200 |    579.2551ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:05:41 | 200 |      4.8752ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:06:09 | 200 |      7.4843ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:06:13.833-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T12:06:13.833-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
time=2025-08-03T12:06:13.833-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 12:06:15 | 200 |    1.3741995s |             ::1 | POST     "/api/generate"
time=2025-08-03T12:06:29.519-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:06:29.519-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T12:06:29.519-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
[GIN] 2025/08/03 - 12:06:33 | 200 |    4.1746697s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:06:39 | 200 |      6.0516ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:07:09 | 200 |      7.9902ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:07:39 | 200 |      4.9582ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:08:09 | 200 |      5.8414ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:08:12.769-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:08:12.769-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T12:08:12.769-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
[GIN] 2025/08/03 - 12:08:13 | 200 |    1.0651931s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:08:39 | 200 |      5.9202ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:09:09 | 200 |      6.1036ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:09:12.137-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
time=2025-08-03T12:09:12.137-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:09:12.137-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
[GIN] 2025/08/03 - 12:09:14 | 200 |    2.7396211s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:09:39 | 200 |      6.7508ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:10:09 | 200 |      6.0719ms |             ::1 | GET      "/api/tags"
time=2025-08-03T12:10:39.106-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:10:39.106-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T12:10:39.106-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
[GIN] 2025/08/03 - 12:10:39 | 200 |      7.0335ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:10:40 | 200 |    1.2439124s |             ::1 | POST     "/api/generate"
time=2025-08-03T12:11:07.120-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T12:11:07.120-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T12:11:07.120-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
[GIN] 2025/08/03 - 12:11:08 | 200 |    1.2431053s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:11:09 | 200 |      6.4426ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:11:39 | 200 |      6.3431ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:12:09 | 200 |      6.5842ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:12:39 | 200 |      8.5036ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:13:09 | 200 |      6.4423ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:13:39 | 200 |      6.3199ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:14:09 | 200 |      7.0158ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:14:27 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:27 | 200 |      5.2431ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:14:27 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:27 | 200 |      5.5778ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:27 | 200 |     32.4544ms |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:27 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:27 | 200 |      3.8515ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:27 | 200 |     327.413ms |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:27 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:27 | 200 |      4.2886ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:28 | 200 |    387.3936ms |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:28 | 200 |       580.5µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:28 | 200 |       5.077ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:28 | 200 |     25.1541ms |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:28 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:28 | 200 |      4.5926ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:28 | 200 |     29.0021ms |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:28 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:28 | 200 |      3.8171ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:30 | 200 |     1.886035s |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:30 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:30 | 200 |      2.7217ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:30 | 200 |     35.7978ms |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:30 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:30 | 200 |      3.3604ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:32 | 200 |    1.8821313s |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:32 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:32 | 200 |      2.8452ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:32 | 200 |      15.728ms |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:32 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:32 | 200 |       3.322ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:32 | 200 |     60.1572ms |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:32 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:32 | 200 |      1.5999ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:33 | 200 |    767.0208ms |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:33 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:33 | 200 |      2.2091ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:33 | 200 |      5.9725ms |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:33 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:33 | 200 |      2.2459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:33 | 200 |    153.3667ms |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:33 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:33 | 200 |      2.1627ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:14:33 | 200 |       5.905ms |       127.0.0.1 | DELETE   "/api/delete"
[GIN] 2025/08/03 - 12:14:33 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:14:33 | 200 |       593.3µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:14:39 | 200 |       514.4µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:14:44 | 200 |            0s |       127.0.0.1 | HEAD     "/"
time=2025-08-03T12:14:44.796-04:00 level=INFO source=download.go:177 msg="downloading ac3d1ba8aa77 in 20 1 GB part(s)"
[GIN] 2025/08/03 - 12:15:09 | 200 |       564.6µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:15:10 | 200 |   26.4398397s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/03 - 12:15:39 | 200 |       517.4µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:16:09 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:16:39 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:17:09 | 200 |       463.8µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:17:39 | 200 |         553µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:18:09 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:18:39 | 200 |       506.1µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:19:09 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:19:39 | 200 |       519.6µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:20:09 | 200 |       507.5µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:20:39 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:21:09 | 200 |       509.4µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:21:39 | 200 |       514.4µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:22:09 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:22:39 | 200 |       505.2µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:23:10 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:23:40 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:24:10 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:24:40 | 200 |       514.6µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:25:10 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:25:40 | 200 |         531µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:26:10 | 200 |       555.7µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:26:40 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:27:10 | 200 |       540.6µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:27:40 | 200 |       530.8µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:28:10 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:28:40 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:29:10 | 200 |       528.4µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:29:40 | 200 |       811.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:30:10 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:30:40 | 200 |       536.3µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:31:10 | 200 |       419.3µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:31:40 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:32:10 | 200 |       507.1µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:32:40 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:33:10 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:33:40 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:34:10 | 200 |       503.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:34:40 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:35:10 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:35:40 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:36:10 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:36:40 | 200 |       512.7µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:37:10 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:37:39 | 200 |       516.8µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:38:07 | 200 |       784.4µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:38:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:39:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:39:37 | 200 |       505.2µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:40:07 | 200 |       523.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:40:37 | 200 |       593.8µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:41:07 | 200 |       505.4µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:41:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:42:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:42:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:42:54 | 404 |       526.2µs |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:43:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:43:08 | 404 |       508.8µs |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 12:43:30 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:43:30 | 200 |            0s |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:43:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:44:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:44:36 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:44:37 | 200 |            0s |             ::1 | GET      "/api/tags"
time=2025-08-03T12:44:37.761-04:00 level=INFO source=download.go:177 msg="downloading 60e05f210007 in 16 292 MB part(s)"
[GIN] 2025/08/03 - 12:45:03 | 200 |       529.4µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:45:03 | 200 |       555.4µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:45:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:45:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:46:07 | 200 |       538.2µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:46:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:47:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:47:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:48:07 | 200 |            0s |             ::1 | GET      "/api/tags"
time=2025-08-03T12:48:14.573-04:00 level=INFO source=download.go:295 msg="60e05f210007 part 7 attempt 0 failed: unexpected EOF, retrying in 1s"
[GIN] 2025/08/03 - 12:48:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:48:38 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 12:48:38 | 200 |       561.4µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:49:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:49:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:50:07 | 200 |       525.1µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:50:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:51:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:51:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:52:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:52:37 | 200 |       789.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:53:07 | 200 |       515.3µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:53:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:54:07 | 200 |       515.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:54:37 | 200 |       529.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:55:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:55:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:56:07 | 200 |       509.8µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:56:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:57:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:57:37 | 200 |       666.3µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:58:07 | 200 |       513.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:58:37 | 200 |       522.1µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:59:07 | 200 |       508.7µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 12:59:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:00:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:00:37 | 200 |       560.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:01:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:01:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:02:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:02:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:03:07 | 200 |       515.6µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:03:37 | 200 |       525.8µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:04:07 | 200 |       504.7µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:04:37 | 200 |       606.3µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:05:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:05:37 | 200 |       511.1µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:06:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:06:37 | 200 |       521.7µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:07:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:07:37 | 200 |      1.0489ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:08:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:08:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:09:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:09:37 | 200 |       666.8µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:10:07 | 200 |         504µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:10:37 | 200 |       598.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:11:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:11:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:12:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:12:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:13:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:13:37 | 200 |       804.7µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:14:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:14:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:15:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:15:37 | 200 |       759.4µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:16:07 | 200 |         520µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:16:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:17:07 | 200 |       526.1µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:17:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:18:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:18:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:19:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:19:37 | 200 |       523.3µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:20:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:20:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:21:07 | 200 |       619.4µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:21:37 | 200 |       532.1µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:22:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:22:37 | 200 |       571.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:23:07 | 200 |       545.7µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:23:37 | 200 |       638.6µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:24:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:24:37 | 200 |       516.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:25:07 | 200 |       511.5µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:25:37 | 200 |       510.7µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:26:07 | 200 |       505.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:26:37 | 200 |       505.6µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:27:07 | 200 |       512.7µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:27:37 | 200 |       512.4µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:28:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:28:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:29:07 | 200 |       521.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:29:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:30:07 | 200 |         509µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:30:37 | 200 |       514.3µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:31:07 | 200 |       785.3µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:31:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:32:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:32:37 | 200 |       1.094ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:33:07 | 200 |       519.1µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:33:37 | 200 |       517.4µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:34:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:34:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:35:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:35:37 | 200 |       792.5µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:36:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:36:37 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:37:07 | 200 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:37:37 | 200 |       505.2µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:38:07 | 200 |       521.6µs |             ::1 | GET      "/api/tags"
time=2025-08-03T13:38:13.101-04:00 level=INFO source=download.go:177 msg="downloading 66b9ea09bd5b in 1 68 B part(s)"
time=2025-08-03T13:38:14.325-04:00 level=INFO source=download.go:177 msg="downloading 1e65450c3067 in 1 1.6 KB part(s)"
time=2025-08-03T13:38:15.555-04:00 level=INFO source=download.go:177 msg="downloading 832dd9e00a68 in 1 11 KB part(s)"
time=2025-08-03T13:38:16.754-04:00 level=INFO source=download.go:177 msg="downloading d9bb33f27869 in 1 487 B part(s)"
[GIN] 2025/08/03 - 13:38:23 | 200 |        53m46s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/03 - 13:38:37 | 200 |      1.0488ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:39:08 | 200 |      1.1895ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:39:38 | 200 |       523.6µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:40:08 | 200 |      1.0693ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:40:38 | 200 |      1.0833ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:41:08 | 200 |      2.3243ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:41:38 | 200 |       549.1µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:42:08 | 200 |       503.1µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:42:38 | 200 |       508.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:43:08 | 200 |      1.8957ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:43:38 | 200 |       504.9µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:44:08 | 200 |       945.3µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:44:38 | 200 |       503.2µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:45:08 | 200 |      2.4734ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:45:38 | 200 |       522.4µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:46:08 | 200 |       530.4µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:46:38 | 200 |      1.3197ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:47:08 | 200 |      1.9308ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:47:38 | 200 |      1.0524ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:48:08 | 200 |      1.0377ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:48:38 | 200 |      1.3519ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:49:08 | 200 |       1.135ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:49:10 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 13:49:38 | 200 |       650.8µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:50:08 | 200 |       657.1µs |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:50:34 | 201 |         1m19s |       127.0.0.1 | POST     "/api/blobs/sha256:9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2"
time=2025-08-03T13:50:34.608-04:00 level=WARN source=create.go:535 msg="couldn't remove blob" digest=sha256:d3a76cb8c4a07d0a6c82ac6e839f98816b5077699d393b2cc77008c16d8078ac error="remove D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-d3a76cb8c4a07d0a6c82ac6e839f98816b5077699d393b2cc77008c16d8078ac: The process cannot access the file because it is being used by another process."
[GIN] 2025/08/03 - 13:50:34 | 200 |    174.7301ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/08/03 - 13:50:38 | 200 |      1.7111ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:51:08 | 200 |      1.6833ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:51:38 | 200 |      1.4443ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:52:08 | 200 |      1.1207ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:52:38 | 200 |      2.1253ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:53:08 | 200 |      1.7414ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:53:38 | 200 |      1.0947ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:53:52 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 13:53:57 | 200 |            0s |       127.0.0.1 | POST     "/api/blobs/sha256:9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2"
time=2025-08-03T13:53:57.898-04:00 level=WARN source=create.go:535 msg="couldn't remove blob" digest=sha256:d3a76cb8c4a07d0a6c82ac6e839f98816b5077699d393b2cc77008c16d8078ac error="remove D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-d3a76cb8c4a07d0a6c82ac6e839f98816b5077699d393b2cc77008c16d8078ac: The process cannot access the file because it is being used by another process."
[GIN] 2025/08/03 - 13:53:57 | 200 |    193.7265ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/08/03 - 13:54:08 | 200 |      2.1589ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:54:38 | 200 |       1.704ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:55:08 | 200 |      2.0424ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:55:38 | 200 |      1.1064ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:56:08 | 200 |      1.3949ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:56:38 | 200 |      1.0563ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:57:08 | 200 |      1.0507ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:57:38 | 200 |       1.052ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:58:08 | 200 |       1.837ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:58:38 | 200 |      1.2633ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:59:08 | 200 |      1.9334ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 13:59:38 | 200 |      1.6373ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:00:08 | 200 |      2.8204ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:00:38 | 200 |      1.8249ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:01:08 | 200 |      2.2023ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:01:38 | 200 |      2.2961ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:02:08 | 200 |       1.784ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:02:38 | 200 |      1.7471ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:03:08 | 200 |       1.919ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:03:38 | 200 |      2.0918ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:04:08 | 200 |      1.8219ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:04:38 | 200 |      1.7255ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:05:08 | 200 |      1.7035ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:05:38 | 200 |       1.791ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:06:08 | 200 |        11.4ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:06:38 | 200 |      2.4584ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:07:08 | 200 |      1.5768ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:07:38 | 200 |      2.1697ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:08:08 | 200 |      2.6275ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:08:38 | 200 |      2.1467ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:09:08 | 200 |      1.7427ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:09:38 | 200 |       1.693ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:10:08 | 200 |      2.2245ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:10:38 | 200 |      2.0108ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:11:08 | 200 |      2.1754ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:11:35 | 200 |      3.1768ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:11:35 | 200 |     74.3403ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 14:11:35 | 200 |    115.0578ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 14:11:35 | 200 |    127.3424ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 14:11:38 | 200 |      2.4285ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:12:08 | 200 |      2.3673ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:12:38 | 200 |      2.2785ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:13:08 | 200 |      1.1946ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:13:38 | 200 |      1.9218ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:14:08 | 200 |      1.7602ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:14:38 | 200 |      2.0721ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:15:08 | 200 |      1.7049ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:15:38 | 200 |      1.9828ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:16:08 | 200 |      1.1595ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:16:38 | 200 |      3.3686ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:17:08 | 200 |      1.6982ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:17:38 | 200 |      1.0658ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:18:08 | 200 |      1.6332ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:18:38 | 200 |      3.1798ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:19:08 | 200 |      1.8892ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:19:38 | 200 |      2.7371ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:20:08 | 200 |      1.0813ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:20:38 | 200 |      3.2623ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:21:08 | 200 |      1.1924ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:21:38 | 200 |      1.8287ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:22:08 | 200 |      2.2919ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:22:38 | 200 |      2.5664ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:23:08 | 200 |      1.9249ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:23:38 | 200 |       1.156ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:24:08 | 200 |      2.0095ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:24:38 | 200 |      2.9838ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:25:08 | 200 |      1.5648ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:25:38 | 200 |      2.0168ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:26:08 | 200 |      1.6813ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:26:38 | 200 |      1.7462ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:27:08 | 200 |      1.2843ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:27:38 | 200 |      2.6989ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:28:08 | 200 |      1.6157ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:28:38 | 200 |      3.2715ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:29:08 | 200 |      1.3112ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:29:38 | 200 |      1.0677ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:30:08 | 200 |      1.7878ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:30:38 | 200 |       2.497ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:31:08 | 200 |      1.5661ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:31:38 | 200 |      1.0506ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:32:08 | 200 |      1.8119ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:32:38 | 200 |       2.727ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:33:08 | 200 |      2.2609ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:33:38 | 200 |      2.3022ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:34:08 | 200 |      2.2358ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:34:38 | 200 |      3.0878ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:35:08 | 200 |      1.9574ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:35:38 | 200 |      1.0825ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:36:08 | 200 |      1.0401ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:36:38 | 200 |      1.8554ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:36:49 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 14:36:49 | 200 |      1.1014ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:37:08 | 200 |      2.3204ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:37:18 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 14:37:18 | 200 |     103.926ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-03T14:37:18.448-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="63.8 GiB" free_swap="58.4 GiB"
time=2025-08-03T14:37:18.448-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=19 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="10.1 GiB" memory.required.partial="5.0 GiB" memory.required.kv="1.5 GiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="6.8 GiB" memory.weights.repeating="6.0 GiB" memory.weights.nonrepeating="787.5 MiB" memory.graph.full="575.0 MiB" memory.graph.partial="1.3 GiB"
time=2025-08-03T14:37:18.531-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2 --ctx-size 16384 --batch-size 512 --n-gpu-layers 19 --threads 4 --no-mmap --parallel 1 --port 59924"
time=2025-08-03T14:37:18.538-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T14:37:18.538-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T14:37:18.539-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T14:37:18.589-04:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-03T14:37:18.590-04:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:59924"
time=2025-08-03T14:37:18.614-04:00 level=INFO source=ggml.go:92 msg="" architecture=gemma3 file_type=Q4_K_M name="Gemma 3 12b It" description="" num_tensors=626 num_key_values=41
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T14:37:18.714-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T14:37:18.776-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CPU size="5.2 GiB"
time=2025-08-03T14:37:18.776-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CUDA0 size="2.4 GiB"
time=2025-08-03T14:37:18.791-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-03T14:37:19.004-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="578.0 MiB"
time=2025-08-03T14:37:19.004-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="543.0 MiB"
time=2025-08-03T14:37:20.047-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.51 seconds"
[GIN] 2025/08/03 - 14:37:20 | 200 |    1.8050634s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 14:37:38 | 200 |      1.0992ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:38:08 | 200 |      1.6527ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:38:38 | 200 |      1.6994ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:39:07 | 200 |      1.6319ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:39:08 | 200 |      1.5975ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:39:34 | 200 |      1.0459ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:39:38 | 200 |       2.274ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:40:04 | 200 |      1.6436ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:40:08 | 200 |      1.0492ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:40:34 | 200 |      1.1217ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:40:38 | 200 |      1.6283ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:40:53 | 200 |         3m17s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/03 - 14:41:04 | 200 |       2.208ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:41:08 | 200 |      1.0339ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:41:34 | 200 |      1.6324ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:41:38 | 200 |      2.4957ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:42:05 | 200 |      1.1051ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:42:08 | 200 |      1.6897ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:42:35 | 200 |      1.6101ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:42:38 | 200 |      1.6602ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:43:05 | 200 |      1.7561ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:43:08 | 200 |       1.609ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:43:35 | 200 |      1.0387ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:43:38 | 200 |      2.3075ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:44:05 | 200 |      1.6421ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:44:09 | 200 |      2.1952ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:44:35 | 200 |      2.3847ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:44:39 | 200 |      1.0419ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:45:05 | 200 |      1.6749ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:45:09 | 200 |      2.2657ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:45:35 | 200 |      1.5757ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:45:39 | 200 |      2.4132ms |             ::1 | GET      "/api/tags"
time=2025-08-03T14:45:58.747-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.017509 runner.size="10.1 GiB" runner.vram="5.0 GiB" runner.parallel=1 runner.pid=69536 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
time=2025-08-03T14:45:58.997-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2674469 runner.size="10.1 GiB" runner.vram="5.0 GiB" runner.parallel=1 runner.pid=69536 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
time=2025-08-03T14:45:59.247-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5173721 runner.size="10.1 GiB" runner.vram="5.0 GiB" runner.parallel=1 runner.pid=69536 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
[GIN] 2025/08/03 - 14:46:05 | 200 |      1.0465ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:46:09 | 200 |      2.3467ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:46:35 | 200 |      2.2006ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:46:39 | 200 |      2.7262ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:47:05 | 200 |       1.205ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:47:09 | 200 |      2.3057ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:47:35 | 200 |      1.5857ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:47:39 | 200 |      2.6727ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:48:05 | 200 |      2.2848ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:48:09 | 200 |      1.5669ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:48:35 | 200 |       2.293ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:48:39 | 200 |      2.3355ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:49:05 | 200 |      1.6656ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:49:09 | 200 |      2.2067ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:49:35 | 200 |      1.6695ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:49:39 | 200 |      2.7123ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:50:05 | 200 |      1.5917ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:50:09 | 200 |      1.6137ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:50:35 | 200 |      2.3128ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:50:39 | 200 |       2.204ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:51:05 | 200 |       1.621ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:51:09 | 200 |      2.3596ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:51:35 | 200 |      1.0789ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:51:39 | 200 |      3.5787ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:52:05 | 200 |      1.1059ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:52:09 | 200 |      2.0661ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:52:35 | 200 |      2.1192ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:52:39 | 200 |      1.5678ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:53:05 | 200 |       1.042ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:53:09 | 200 |      2.4014ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:53:35 | 200 |      1.0431ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:53:39 | 200 |      2.7704ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:54:05 | 200 |      1.0933ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:54:09 | 200 |       2.316ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:54:35 | 200 |      1.7273ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:54:39 | 200 |      1.6192ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:55:05 | 200 |      1.5752ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:55:09 | 200 |      1.6423ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:55:35 | 200 |      2.2973ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:55:39 | 200 |      2.2801ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:56:05 | 200 |      1.0712ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:56:09 | 200 |      2.2694ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:56:35 | 200 |      3.5504ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:56:39 | 200 |      2.4151ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:57:05 | 200 |        2.18ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:57:09 | 200 |       2.185ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:57:35 | 200 |      1.0464ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:57:39 | 200 |       3.503ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:58:05 | 200 |      1.2321ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:58:09 | 200 |      1.2124ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:58:35 | 200 |       1.592ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:58:39 | 200 |      2.0019ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:59:05 | 200 |      1.0501ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:59:09 | 200 |      1.5952ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:59:35 | 200 |      2.3593ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 14:59:39 | 200 |      2.4284ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:00:05 | 200 |       1.644ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:00:09 | 200 |      2.2663ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:00:35 | 200 |      1.1344ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:00:39 | 200 |       1.278ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:01:05 | 200 |      2.1898ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:01:09 | 200 |       1.842ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:01:35 | 200 |      2.3869ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:01:39 | 200 |       2.717ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:02:05 | 200 |      1.7536ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:02:09 | 200 |      3.6481ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:02:35 | 200 |      2.2667ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:02:39 | 200 |      1.6532ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:03:05 | 200 |      1.6141ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:03:09 | 200 |       2.754ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:03:25.669-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="63.6 GiB" free_swap="58.1 GiB"
time=2025-08-03T15:03:25.670-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=19 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="10.1 GiB" memory.required.partial="5.0 GiB" memory.required.kv="1.5 GiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="6.8 GiB" memory.weights.repeating="6.0 GiB" memory.weights.nonrepeating="787.5 MiB" memory.graph.full="575.0 MiB" memory.graph.partial="1.3 GiB"
time=2025-08-03T15:03:25.751-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2 --ctx-size 16384 --batch-size 512 --n-gpu-layers 19 --threads 4 --no-mmap --parallel 1 --port 61514"
time=2025-08-03T15:03:25.759-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T15:03:25.759-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T15:03:25.760-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T15:03:25.808-04:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-03T15:03:25.810-04:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:61514"
time=2025-08-03T15:03:25.834-04:00 level=INFO source=ggml.go:92 msg="" architecture=gemma3 file_type=Q4_K_M name="Gemma 3 12b It" description="" num_tensors=626 num_key_values=41
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T15:03:25.935-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T15:03:26.001-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CPU size="5.2 GiB"
time=2025-08-03T15:03:26.001-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CUDA0 size="2.4 GiB"
time=2025-08-03T15:03:26.012-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-03T15:03:26.221-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="578.0 MiB"
time=2025-08-03T15:03:26.221-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="543.0 MiB"
time=2025-08-03T15:03:27.265-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.51 seconds"
[GIN] 2025/08/03 - 15:03:35 | 200 |      2.5042ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:03:39 | 200 |      3.0118ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:04:05 | 200 |      1.1836ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:04:09 | 200 |      1.6594ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:04:35 | 200 |      1.6679ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:04:39 | 200 |      1.6225ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:05:05 | 200 |      1.0473ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:05:09 | 200 |      2.2059ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:05:35 | 200 |      1.7575ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:05:39 | 200 |      1.0775ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:06:05 | 200 |      2.3248ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:06:09 | 200 |      2.1185ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:06:35 | 200 |      1.6079ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:06:39 | 200 |      1.5982ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:07:05 | 200 |      1.0386ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:07:09 | 200 |      1.6065ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:07:35 | 200 |       1.695ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:07:39 | 200 |      1.6982ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:08:05 | 200 |       1.663ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:08:09 | 200 |      3.2201ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:08:34 | 200 |          5m8s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/03 - 15:08:35 | 200 |      2.1665ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:08:39 | 200 |      1.6124ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:09:05 | 200 |      1.9394ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:09:09 | 200 |      1.5987ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:09:35 | 200 |      1.6999ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:09:39 | 200 |      2.1463ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:10:02 | 404 |      1.0355ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:10:05 | 200 |      1.6233ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:10:09 | 200 |      2.3142ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:10:17 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 15:10:17 | 200 |      1.1753ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:10:31 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 15:10:31 | 200 |     96.7882ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 15:10:35 | 200 |       1.784ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:10:39 | 200 |      1.2938ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:11:05 | 200 |      2.4347ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:11:09 | 200 |      1.6335ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:11:35 | 200 |      2.4308ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:11:39 | 200 |      2.0001ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:11:40 | 200 |          1m8s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:12:05 | 200 |      4.5369ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:12:07 | 200 |       3.029ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:12:09 | 200 |      2.7025ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:12:28 | 200 |          1m4s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-03T15:12:28.140-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/03 - 15:12:43 | 200 |      1.7098ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:13:11 | 200 |      2.4882ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:13:29.986-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:13:29.986-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
[GIN] 2025/08/03 - 15:13:41 | 200 |      1.5844ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:13:44.956-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:61514/completion\": context canceled"
[GIN] 2025/08/03 - 15:13:44 | 500 |   15.0174338s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:14:23 | 200 |      2.2653ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:14:42.907-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:14:42.907-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
[GIN] 2025/08/03 - 15:14:51 | 200 |      1.0768ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:15:21 | 200 |      2.6631ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:15:42.882-04:00 level=ERROR source=server.go:800 msg="post predict" error="Post \"http://127.0.0.1:61514/completion\": context canceled"
[GIN] 2025/08/03 - 15:15:42 | 500 |          1m0s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:15:51 | 200 |      1.0445ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:16:21 | 200 |      1.8885ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:16:34 | 200 |      2.4748ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:17:02 | 200 |      2.0416ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:17:31.522-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:17:31.522-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T15:17:31.596-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="1.0 GiB"
[GIN] 2025/08/03 - 15:17:32 | 200 |      1.0645ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:17:36.623-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0256544 runner.size="10.1 GiB" runner.vram="5.0 GiB" runner.parallel=1 runner.pid=78292 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
time=2025-08-03T15:17:36.712-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="64.6 GiB" free_swap="59.5 GiB"
time=2025-08-03T15:17:36.712-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=26 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
time=2025-08-03T15:17:36.873-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.275441 runner.size="10.1 GiB" runner.vram="5.0 GiB" runner.parallel=1 runner.pid=78292 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
load: special tokens cache size = 22
time=2025-08-03T15:17:37.123-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5254883 runner.size="10.1 GiB" runner.vram="5.0 GiB" runner.parallel=1 runner.pid=78292 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-03T15:17:37.139-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 26 --threads 4 --no-mmap --parallel 1 --port 62304"
time=2025-08-03T15:17:37.145-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T15:17:37.145-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T15:17:37.146-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T15:17:37.194-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T15:17:37.298-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T15:17:37.299-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:62304"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-03T15:17:37.397-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-08-03T15:17:39.150-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.00 seconds"
[GIN] 2025/08/03 - 15:17:59 | 200 |   27.8244253s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:18:02 | 200 |      2.9981ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:18:32 | 200 |      1.5948ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:19:02 | 200 |      2.3431ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:19:32 | 200 |      1.0557ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:20:02 | 200 |      2.0312ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:20:27.138-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:20:27.138-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T15:20:27.138-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
[GIN] 2025/08/03 - 15:20:29 | 200 |    2.4421544s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:20:32 | 200 |      2.4619ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:21:02 | 200 |      1.0412ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:21:03.383-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T15:21:03.383-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
time=2025-08-03T15:21:03.383-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 15:21:32 | 200 |       1.752ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:21:34 | 200 |    31.325715s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:22:02 | 200 |      1.6184ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:22:32 | 200 |      2.5082ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:23:02 | 200 |      2.0433ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:23:32 | 200 |      1.0506ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:24:02 | 200 |      2.2638ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:24:32 | 200 |       1.165ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:25:02 | 200 |      1.7231ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:25:32 | 200 |      2.4598ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:26:02 | 200 |      1.4334ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:26:32 | 200 |      1.8828ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:27:02 | 200 |      1.7728ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:27:45 | 200 |      2.2674ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:28:13 | 200 |      1.5967ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:28:28.329-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:28:28.329-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T15:28:28.329-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
time=2025-08-03T15:28:28.422-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="63.5 GiB" free_swap="58.1 GiB"
time=2025-08-03T15:28:28.423-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=26 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-03T15:28:28.900-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 26 --threads 4 --no-mmap --parallel 1 --port 63050"
time=2025-08-03T15:28:28.908-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T15:28:28.908-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T15:28:28.908-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T15:28:28.961-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T15:28:29.067-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T15:28:29.067-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:63050"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-03T15:28:29.159-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-08-03T15:28:30.915-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.01 seconds"
[GIN] 2025/08/03 - 15:28:33 | 200 |    5.1140632s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:28:43 | 200 |      2.3639ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:28:43.816-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:28:43.816-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:28:43.816-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:28:43.816-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:28:45 | 200 |    1.6397463s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:28:48 | 200 |    4.4374015s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:29:13 | 200 |      2.3758ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:29:14.583-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:29:14.583-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=sessionId
time=2025-08-03T15:29:14.583-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=preferredModel
[GIN] 2025/08/03 - 15:29:22 | 200 |    8.3416328s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:29:43 | 200 |      2.0333ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:29:43.804-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:29:43.804-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:29:43.816-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:29:43.816-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:29:44 | 200 |    1.2029908s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:29:46 | 200 |    2.9642903s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:30:13 | 200 |      1.7784ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:30:43 | 200 |      1.6434ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:30:43.813-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:30:43.813-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:30:43.828-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:30:43.828-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 15:30:46 | 200 |    2.3003756s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:30:47 | 200 |    4.0684868s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:31:13 | 200 |      1.4261ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:31:43.821-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:31:43.821-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:31:43 | 200 |      3.2075ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:31:43.839-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:31:43.839-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:31:45 | 200 |    2.1297847s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:31:48 | 200 |    4.4096019s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:32:13 | 200 |      2.3529ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:32:43.828-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:32:43.828-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:32:43 | 200 |      1.5979ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:32:43.843-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:32:43.843-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:32:45 | 200 |    1.9770853s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:32:47 | 200 |    4.0592577s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:33:13 | 200 |      1.6669ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:33:43.828-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:33:43.828-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:33:43.840-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:33:43.840-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:33:43 | 200 |      1.7413ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:33:45 | 200 |    2.1824423s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:33:47 | 200 |    3.9041231s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:34:13 | 200 |       1.062ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:34:43.839-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:34:43.839-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:34:43.855-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:34:43.855-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 15:34:43 | 200 |      1.9584ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:34:46 | 200 |    2.9196595s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:34:48 | 200 |    4.7999425s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:35:13 | 200 |      1.0679ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:35:43.843-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:35:43.843-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:35:43.857-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:35:43.857-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:35:43 | 200 |      1.0323ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:35:45 | 200 |    2.0078415s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:35:47 | 200 |    4.0071209s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:36:13 | 200 |      1.5982ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:36:43.865-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:36:43.865-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:36:43.865-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:36:43.865-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:36:43 | 200 |      1.6463ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:36:46 | 200 |    2.3560747s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:36:47 | 200 |     3.838374s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:37:13 | 200 |      1.8642ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:37:43.859-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:37:43.859-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:37:43.859-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:37:43.859-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 15:37:43 | 200 |      1.6972ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:37:46 | 200 |    2.6701772s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:37:48 | 200 |    4.8676888s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:38:13 | 200 |      2.0394ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:38:43.870-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:38:43.870-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:38:43.870-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:38:43.870-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:38:43 | 200 |      1.0338ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:38:46 | 200 |    3.0043117s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:38:48 | 200 |    4.4074553s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:39:13 | 200 |      1.2057ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:39:43.888-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:39:43.888-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:39:43.888-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:39:43.888-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:39:43 | 200 |       1.861ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:39:45 | 200 |    1.9836501s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:39:47 | 200 |    3.6906103s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:40:13 | 200 |      1.0856ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:40:43.895-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:40:43.895-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:40:43.895-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:40:43.895-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:40:43 | 200 |      1.6148ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:40:45 | 200 |    2.0602486s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:40:47 | 200 |    3.6388239s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:41:07 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 15:41:07 | 200 |      2.6039ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:41:13 | 200 |      1.5609ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:41:20 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 15:41:20 | 200 |    100.3737ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-03T15:41:21.149-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="578.4 MiB"
time=2025-08-03T15:41:21.589-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="63.3 GiB" free_swap="58.0 GiB"
time=2025-08-03T15:41:21.590-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=21 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="9.6 GiB" memory.required.partial="5.0 GiB" memory.required.kv="992.0 MiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="6.8 GiB" memory.weights.repeating="6.0 GiB" memory.weights.nonrepeating="787.5 MiB" memory.graph.full="519.5 MiB" memory.graph.partial="1.3 GiB"
time=2025-08-03T15:41:21.678-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2 --ctx-size 8192 --batch-size 512 --n-gpu-layers 21 --threads 4 --no-mmap --parallel 1 --port 63849"
time=2025-08-03T15:41:21.685-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T15:41:21.685-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T15:41:21.686-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T15:41:21.737-04:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-03T15:41:21.738-04:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:63849"
time=2025-08-03T15:41:21.761-04:00 level=INFO source=ggml.go:92 msg="" architecture=gemma3 file_type=Q4_K_M name="Gemma 3 12b It" description="" num_tensors=626 num_key_values=41
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T15:41:21.873-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T15:41:21.931-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CPU size="4.9 GiB"
time=2025-08-03T15:41:21.931-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CUDA0 size="2.6 GiB"
time=2025-08-03T15:41:21.937-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-03T15:41:22.101-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="306.0 MiB"
time=2025-08-03T15:41:22.101-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="287.0 MiB"
time=2025-08-03T15:41:23.442-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/03 - 15:41:23 | 200 |     2.469835s |       127.0.0.1 | POST     "/api/generate"
time=2025-08-03T15:41:43.905-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:41:43.905-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:41:43.905-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:41:43.906-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:41:43.971-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="1.0 GiB"
[GIN] 2025/08/03 - 15:41:44 | 200 |      2.5695ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:41:48.985-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0112632 runner.size="9.6 GiB" runner.vram="5.0 GiB" runner.parallel=1 runner.pid=114920 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
time=2025-08-03T15:41:49.094-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="63.4 GiB" free_swap="58.1 GiB"
time=2025-08-03T15:41:49.096-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=26 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-08-03T15:41:49.235-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2613691 runner.size="9.6 GiB" runner.vram="5.0 GiB" runner.parallel=1 runner.pid=114920 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
time=2025-08-03T15:41:49.485-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5109004 runner.size="9.6 GiB" runner.vram="5.0 GiB" runner.parallel=1 runner.pid=114920 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-03T15:41:49.511-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 26 --threads 4 --no-mmap --parallel 1 --port 63871"
time=2025-08-03T15:41:49.516-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T15:41:49.517-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T15:41:49.518-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T15:41:49.567-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T15:41:49.669-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T15:41:49.670-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:63871"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-03T15:41:49.769-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-08-03T15:41:51.522-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.01 seconds"
[GIN] 2025/08/03 - 15:41:53 | 200 |    9.9574205s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:41:55 | 200 |   12.0595728s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:42:14 | 200 |      2.7104ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:42:40.662-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="541.6 MiB"
time=2025-08-03T15:42:41.080-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="63.2 GiB" free_swap="57.8 GiB"
time=2025-08-03T15:42:41.081-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=20 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="9.6 GiB" memory.required.partial="4.8 GiB" memory.required.kv="992.0 MiB" memory.required.allocations="[4.8 GiB]" memory.weights.total="6.8 GiB" memory.weights.repeating="6.0 GiB" memory.weights.nonrepeating="787.5 MiB" memory.graph.full="519.5 MiB" memory.graph.partial="1.3 GiB"
time=2025-08-03T15:42:41.157-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2 --ctx-size 8192 --batch-size 512 --n-gpu-layers 20 --threads 4 --no-mmap --parallel 1 --port 63927"
time=2025-08-03T15:42:41.163-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T15:42:41.163-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T15:42:41.164-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T15:42:41.220-04:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-03T15:42:41.221-04:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:63927"
time=2025-08-03T15:42:41.247-04:00 level=INFO source=ggml.go:92 msg="" architecture=gemma3 file_type=Q4_K_M name="Gemma 3 12b It" description="" num_tensors=626 num_key_values=41
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T15:42:41.360-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T15:42:41.416-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-03T15:42:41.431-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CPU size="5.0 GiB"
time=2025-08-03T15:42:41.431-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CUDA0 size="2.5 GiB"
time=2025-08-03T15:42:41.602-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="306.0 MiB"
time=2025-08-03T15:42:41.602-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="287.0 MiB"
time=2025-08-03T15:42:42.922-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
time=2025-08-03T15:42:43.926-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:42:43.926-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:42:43.926-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:42:43.926-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:42:43.993-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="1.2 GiB"
[GIN] 2025/08/03 - 15:42:44 | 200 |      1.0504ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:43:14 | 200 |      1.0389ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:43:43.939-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:43:43.939-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:43:43.939-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:43:43.939-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 15:43:44 | 200 |       1.611ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:44:14 | 200 |      1.4054ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:44:43.938-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:44:43.938-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:44:43.939-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:44:43.939-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:44:44 | 200 |      1.8718ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:45:14 | 200 |      1.2324ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:45:43.947-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:45:43.947-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:45:43.947-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:45:43.947-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:45:44 | 200 |      1.7543ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:46:14 | 200 |      1.0682ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:46:43.958-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:46:43.958-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:46:43.958-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:46:43.958-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:46:44 | 200 |      1.6285ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:47:10 | 200 |         4m29s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/03 - 15:47:14 | 200 |       528.7µs |             ::1 | GET      "/api/tags"
time=2025-08-03T15:47:15.345-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.022196 runner.size="9.6 GiB" runner.vram="4.8 GiB" runner.parallel=1 runner.pid=71000 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
time=2025-08-03T15:47:15.471-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="62.8 GiB" free_swap="57.3 GiB"
time=2025-08-03T15:47:15.471-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=26 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-08-03T15:47:15.594-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2719182 runner.size="9.6 GiB" runner.vram="4.8 GiB" runner.parallel=1 runner.pid=71000 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
time=2025-08-03T15:47:15.845-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5221335 runner.size="9.6 GiB" runner.vram="4.8 GiB" runner.parallel=1 runner.pid=71000 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-03T15:47:15.949-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 26 --threads 4 --no-mmap --parallel 1 --port 64210"
time=2025-08-03T15:47:15.955-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T15:47:15.956-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T15:47:15.956-04:00 level=WARN source=server.go:598 msg="client connection closed before server finished loading, aborting load"
time=2025-08-03T15:47:15.956-04:00 level=ERROR source=sched.go:489 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
[GIN] 2025/08/03 - 15:47:15 | 499 |         4m32s |             ::1 | POST     "/api/generate"
time=2025-08-03T15:47:16.007-04:00 level=INFO source=runner.go:815 msg="starting go runner"
time=2025-08-03T15:47:18.101-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="62.7 GiB" free_swap="57.3 GiB"
time=2025-08-03T15:47:18.102-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=26 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-03T15:47:18.510-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 26 --threads 4 --no-mmap --parallel 1 --port 64214"
time=2025-08-03T15:47:18.515-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T15:47:18.515-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T15:47:18.515-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T15:47:18.565-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T15:47:18.669-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T15:47:18.670-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:64214"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-03T15:47:18.766-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-08-03T15:47:20.534-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.02 seconds"
[GIN] 2025/08/03 - 15:47:22 | 200 |    38.352964s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:47:23 | 200 |   39.4618113s |             ::1 | POST     "/api/generate"
time=2025-08-03T15:47:43.951-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:47:43.951-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:47:43.951-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:47:43.951-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:47:44 | 200 |      1.8027ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:47:44.916-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="566.5 MiB"
[GIN] 2025/08/03 - 15:47:45 | 200 |    1.8567905s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:47:47 | 200 |    3.2468424s |             ::1 | POST     "/api/generate"
time=2025-08-03T15:47:47.585-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="62.7 GiB" free_swap="57.3 GiB"
time=2025-08-03T15:47:47.585-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=20 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="9.6 GiB" memory.required.partial="4.8 GiB" memory.required.kv="992.0 MiB" memory.required.allocations="[4.8 GiB]" memory.weights.total="6.8 GiB" memory.weights.repeating="6.0 GiB" memory.weights.nonrepeating="787.5 MiB" memory.graph.full="519.5 MiB" memory.graph.partial="1.3 GiB"
time=2025-08-03T15:47:47.662-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2 --ctx-size 8192 --batch-size 512 --n-gpu-layers 20 --threads 4 --no-mmap --parallel 1 --port 64238"
time=2025-08-03T15:47:47.673-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T15:47:47.673-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T15:47:47.673-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T15:47:47.771-04:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-03T15:47:47.772-04:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:64238"
time=2025-08-03T15:47:47.800-04:00 level=INFO source=ggml.go:92 msg="" architecture=gemma3 file_type=Q4_K_M name="Gemma 3 12b It" description="" num_tensors=626 num_key_values=41
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
time=2025-08-03T15:47:47.924-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T15:47:47.933-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T15:47:47.998-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CPU size="5.0 GiB"
time=2025-08-03T15:47:47.998-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CUDA0 size="2.5 GiB"
time=2025-08-03T15:47:48.181-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="306.0 MiB"
time=2025-08-03T15:47:48.181-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="287.0 MiB"
time=2025-08-03T15:47:49.679-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.01 seconds"
[GIN] 2025/08/03 - 15:48:14 | 200 |      4.9218ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:48:43.973-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:48:43.973-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:48:43.978-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:48:43.978-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:48:44.059-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="1.2 GiB"
[GIN] 2025/08/03 - 15:48:44 | 200 |       3.994ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:49:14 | 200 |      1.9414ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:49:43.981-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:49:43.982-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:49:43.983-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:49:43.983-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:49:44 | 200 |      1.5865ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:50:14 | 200 |      1.5874ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:50:43.998-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:50:43.998-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:50:43.998-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:50:43.998-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:50:44 | 200 |       1.632ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:51:14 | 200 |      1.8678ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:51:43.987-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:51:43.988-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:51:43.988-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:51:43.988-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:51:44 | 200 |      1.2328ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:52:14 | 200 |      1.7315ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:52:22 | 200 |         4m37s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-03T15:52:27.254-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0148695 runner.size="9.6 GiB" runner.vram="4.8 GiB" runner.parallel=1 runner.pid=60204 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
time=2025-08-03T15:52:27.374-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="62.3 GiB" free_swap="56.9 GiB"
time=2025-08-03T15:52:27.374-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=26 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-08-03T15:52:27.504-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2649204 runner.size="9.6 GiB" runner.vram="4.8 GiB" runner.parallel=1 runner.pid=60204 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
time=2025-08-03T15:52:27.754-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5146092 runner.size="9.6 GiB" runner.vram="4.8 GiB" runner.parallel=1 runner.pid=60204 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-03T15:52:27.832-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 26 --threads 4 --no-mmap --parallel 1 --port 64514"
time=2025-08-03T15:52:27.837-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T15:52:27.837-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T15:52:27.837-04:00 level=WARN source=server.go:598 msg="client connection closed before server finished loading, aborting load"
time=2025-08-03T15:52:27.837-04:00 level=ERROR source=sched.go:489 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
[GIN] 2025/08/03 - 15:52:27 | 499 |         3m43s |             ::1 | POST     "/api/generate"
time=2025-08-03T15:52:28.211-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="62.3 GiB" free_swap="56.8 GiB"
time=2025-08-03T15:52:28.212-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=26 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-03T15:52:28.653-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 26 --threads 4 --no-mmap --parallel 1 --port 64516"
time=2025-08-03T15:52:28.658-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T15:52:28.658-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T15:52:28.659-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T15:52:28.715-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T15:52:28.828-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T15:52:28.829-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:64516"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-03T15:52:28.910-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-08-03T15:52:30.913-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.25 seconds"
[GIN] 2025/08/03 - 15:52:32 | 200 |   48.7219439s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:52:33 | 200 |   49.9989259s |             ::1 | POST     "/api/generate"
time=2025-08-03T15:52:43.994-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:52:43.994-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:52:43.995-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:52:43.995-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:52:44 | 200 |      1.0455ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:52:45 | 200 |    1.3919837s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:52:46 | 200 |    2.6060543s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:53:14 | 200 |      2.6454ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:53:44.002-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:53:44.002-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:53:44.002-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:53:44.002-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:53:44 | 200 |      2.0502ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:53:45 | 200 |    1.8143774s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:53:47 | 200 |    3.5930767s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:54:14 | 200 |      2.1869ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:54:43.998-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:54:43.998-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:54:43.999-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:54:43.999-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 15:54:44 | 200 |      1.0478ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:54:45 | 200 |    1.7388241s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:54:47 | 200 |    3.1381249s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:55:14 | 200 |      1.3373ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:55:44.005-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:55:44.005-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:55:44.005-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:55:44.005-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:55:44 | 200 |      1.2317ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:55:45 | 200 |    1.7355251s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:55:46 | 200 |    2.9050472s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:56:14 | 200 |      1.2803ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:56:44.013-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:56:44.013-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:56:44.013-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:56:44.013-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:56:44 | 200 |      2.4286ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:56:45 | 200 |     1.803675s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:56:47 | 200 |     3.663719s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:57:14 | 200 |      1.6439ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:57:44.018-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:57:44.018-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:57:44.018-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:57:44.018-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:57:44 | 200 |      1.0981ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:57:46 | 200 |    2.1617443s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:57:48 | 200 |    4.7317945s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:58:14 | 200 |      1.6528ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:58:44.030-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:58:44.030-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:58:44.030-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:58:44.030-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 15:58:44 | 200 |      1.6625ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:58:46 | 200 |    2.1090002s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:58:47 | 200 |    3.7834968s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:59:14 | 200 |      1.5824ms |             ::1 | GET      "/api/tags"
time=2025-08-03T15:59:44.032-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T15:59:44.033-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:59:44.033-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T15:59:44.033-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 15:59:44 | 200 |      1.6094ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 15:59:46 | 200 |    2.1504634s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 15:59:48 | 200 |    4.0029646s |             ::1 | POST     "/api/generate"
time=2025-08-03T16:00:01.504-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="554.4 MiB"
time=2025-08-03T16:00:01.954-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="61.8 GiB" free_swap="56.4 GiB"
time=2025-08-03T16:00:01.956-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=20 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="9.6 GiB" memory.required.partial="4.8 GiB" memory.required.kv="992.0 MiB" memory.required.allocations="[4.8 GiB]" memory.weights.total="6.8 GiB" memory.weights.repeating="6.0 GiB" memory.weights.nonrepeating="787.5 MiB" memory.graph.full="519.5 MiB" memory.graph.partial="1.3 GiB"
time=2025-08-03T16:00:02.056-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2 --ctx-size 8192 --batch-size 512 --n-gpu-layers 20 --threads 4 --no-mmap --parallel 1 --port 65153"
time=2025-08-03T16:00:02.067-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T16:00:02.067-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T16:00:02.068-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T16:00:02.125-04:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-03T16:00:02.125-04:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:65153"
time=2025-08-03T16:00:02.151-04:00 level=INFO source=ggml.go:92 msg="" architecture=gemma3 file_type=Q4_K_M name="Gemma 3 12b It" description="" num_tensors=626 num_key_values=41
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T16:00:02.270-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T16:00:02.319-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-03T16:00:02.341-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CPU size="5.0 GiB"
time=2025-08-03T16:00:02.341-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CUDA0 size="2.5 GiB"
time=2025-08-03T16:00:02.520-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="306.0 MiB"
time=2025-08-03T16:00:02.520-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="287.0 MiB"
time=2025-08-03T16:00:03.825-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/03 - 16:00:14 | 200 |      8.5512ms |             ::1 | GET      "/api/tags"
time=2025-08-03T16:00:44.084-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:00:44.084-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:00:44.115-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:00:44.115-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:00:44.232-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="1.2 GiB"
[GIN] 2025/08/03 - 16:00:44 | 200 |      3.5377ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:00:47 | 200 |   46.3024104s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-03T16:00:47.604-04:00 level=INFO source=server.go:761 msg="aborting completion request due to client closing the connection"
time=2025-08-03T16:00:49.261-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0209446 runner.size="9.6 GiB" runner.vram="4.8 GiB" runner.parallel=1 runner.pid=115920 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
time=2025-08-03T16:00:49.388-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="61.9 GiB" free_swap="56.6 GiB"
time=2025-08-03T16:00:49.390-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=26 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-03T16:00:49.511-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2708528 runner.size="9.6 GiB" runner.vram="4.8 GiB" runner.parallel=1 runner.pid=115920 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
time=2025-08-03T16:00:49.762-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5213747 runner.size="9.6 GiB" runner.vram="4.8 GiB" runner.parallel=1 runner.pid=115920 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-03T16:00:49.941-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 26 --threads 4 --no-mmap --parallel 1 --port 65196"
time=2025-08-03T16:00:49.950-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T16:00:49.950-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T16:00:49.951-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T16:00:50.010-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T16:00:50.124-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T16:00:50.125-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:65196"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-03T16:00:50.203-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-08-03T16:00:51.957-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.01 seconds"
[GIN] 2025/08/03 - 16:00:53 | 200 |    9.8627495s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:00:55 | 200 |   11.7295433s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:01:14 | 200 |      2.5752ms |             ::1 | GET      "/api/tags"
time=2025-08-03T16:01:44.061-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:01:44.061-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:01:44.061-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:01:44.061-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 16:01:44 | 200 |      2.0795ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:01:45 | 200 |    1.7868912s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:01:47 | 200 |     3.453784s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:02:11 | 200 |      1.1673ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:02:14 | 200 |      1.9432ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:02:21 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 16:02:21 | 200 |      2.1327ms |       127.0.0.1 | GET      "/api/tags"
time=2025-08-03T16:02:44.061-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:02:44.061-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:02:44.061-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:02:44.061-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 16:02:44 | 200 |      1.0396ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:02:46 | 200 |    1.9893258s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:02:47 | 200 |    3.7299286s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:03:14 | 200 |      1.6493ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:03:29 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 16:03:29 | 200 |       1.099ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:03:29 | 200 |   16.4678413s |             ::1 | POST     "/api/generate"
time=2025-08-03T16:03:44.073-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:03:44.073-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:03:44.073-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:03:44.073-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 16:03:44 | 200 |      1.6268ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:03:46 | 200 |    2.4212802s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:03:48 | 200 |    4.5850194s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:03:59 | 200 |    5.3062798s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:04:14 | 200 |      2.7731ms |             ::1 | GET      "/api/tags"
time=2025-08-03T16:04:44.075-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:04:44.075-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:04:44.075-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:04:44.075-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 16:04:44 | 200 |       1.619ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:04:45 | 200 |    1.5952802s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:04:47 | 200 |    3.1775656s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:05:14 | 200 |      1.7299ms |             ::1 | GET      "/api/tags"
time=2025-08-03T16:05:44.100-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:05:44.100-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:05:44.100-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:05:44.100-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 16:05:44 | 200 |      1.9158ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:05:45 | 200 |    1.8971045s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:05:48 | 200 |    4.0992835s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:06:01 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 16:06:01 | 200 |      1.8311ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:06:14 | 200 |       2.298ms |             ::1 | GET      "/api/tags"
time=2025-08-03T16:06:44.102-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:06:44.102-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:06:44.103-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:06:44.103-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 16:06:44 | 200 |      1.0423ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:06:45 | 200 |    1.1927926s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:06:46 | 200 |    2.3500996s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:07:14 | 200 |      1.5678ms |             ::1 | GET      "/api/tags"
time=2025-08-03T16:07:44.111-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:07:44.112-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:07:44.112-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:07:44.112-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 16:07:44 | 200 |      1.1323ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:07:45 | 200 |    1.5205938s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:07:46 | 200 |    2.8577341s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:08:14 | 200 |      1.7984ms |             ::1 | GET      "/api/tags"
time=2025-08-03T16:08:44.121-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:08:44.121-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:08:44.121-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:08:44.121-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/03 - 16:08:44 | 200 |      1.3391ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:08:45 | 200 |    1.5409976s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:08:47 | 200 |    3.1512227s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:09:14 | 200 |      1.6178ms |             ::1 | GET      "/api/tags"
time=2025-08-03T16:09:44.130-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:09:44.131-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:09:44.130-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:09:44.131-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 16:09:44 | 200 |      1.1369ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:09:45 | 200 |    1.4087436s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:09:47 | 200 |    3.4291256s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:10:14 | 200 |      1.6487ms |             ::1 | GET      "/api/tags"
time=2025-08-03T16:10:44.139-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:10:44.139-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:10:44.141-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:10:44.141-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 16:10:44 | 200 |      1.0816ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:10:46 | 200 |    2.0160294s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:10:47 | 200 |    3.4681173s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:11:01 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 16:11:01 | 200 |     60.6993ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/08/03 - 16:11:14 | 200 |      2.4414ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:11:27 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 16:11:27 | 200 |      2.7933ms |       127.0.0.1 | GET      "/api/tags"
time=2025-08-03T16:11:44.143-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:11:44.143-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
time=2025-08-03T16:11:44.143-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-03T16:11:44.144-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=complexity
[GIN] 2025/08/03 - 16:11:44 | 200 |      2.8428ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:11:45 | 200 |    1.3632487s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:11:47 | 200 |    3.0905909s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:12:14 | 200 |      2.5747ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:12:55 | 200 |      1.9248ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:12:55 | 200 |      87.141ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 16:12:55 | 200 |     84.6368ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 16:12:55 | 200 |     124.776ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 16:12:55 | 200 |    127.8661ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 16:15:08 | 200 |       2.998ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:15:08 | 200 |     99.3032ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 16:15:08 | 200 |    100.8121ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 16:15:08 | 200 |    156.6233ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 16:15:08 | 200 |     163.757ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 16:16:27 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 16:16:27 | 200 |      2.7891ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:17:04 | 200 |      2.2552ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:17:41 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/03 - 16:17:41 | 200 |        2.55ms |       127.0.0.1 | GET      "/api/tags"
time=2025-08-03T16:17:54.449-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="68.0 GiB" free_swap="63.2 GiB"
time=2025-08-03T16:17:54.450-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=25 layers.split="" memory.available="[4.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.7 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.7 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-03T16:17:55.018-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 25 --threads 4 --no-mmap --parallel 1 --port 50185"
time=2025-08-03T16:17:55.025-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-03T16:17:55.025-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-03T16:17:55.026-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-03T16:17:55.074-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-03T16:17:55.175-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-03T16:17:55.175-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:50185"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-03T16:17:55.277-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 25 repeating layers to GPU
load_tensors: offloaded 25/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   852.99 MiB
load_tensors:        CUDA0 model buffer size =  3315.10 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   200.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    24.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 46 (with bs=512), 3 (with bs=1)
time=2025-08-03T16:17:56.780-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/03 - 16:18:13 | 200 |   19.3336811s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:18:42 | 200 |   12.6285935s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/03 - 16:23:30 | 200 |       3.203ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/03 - 16:23:30 | 200 |     92.8031ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 16:23:30 | 200 |     92.2076ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 16:23:30 | 200 |    149.4852ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 16:23:30 | 200 |    156.0295ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/03 - 23:49:38 | 200 |      2.1572ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/03 - 23:58:00 | 200 |      2.5577ms |             ::1 | GET      "/api/tags"
time=2025-08-04T00:02:05.857-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="65.4 GiB" free_swap="60.3 GiB"
time=2025-08-04T00:02:05.857-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=26 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-04T00:02:06.247-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 26 --threads 4 --no-mmap --parallel 1 --port 55309"
time=2025-08-04T00:02:06.256-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-04T00:02:06.257-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-04T00:02:06.257-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-04T00:02:06.382-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-04T00:02:06.481-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-04T00:02:06.482-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:55309"
time=2025-08-04T00:02:06.508-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-08-04T00:02:08.011-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.75 seconds"
[GIN] 2025/08/04 - 00:02:09 | 200 |    3.5574058s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/04 - 00:06:49 | 200 |       518.6µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 00:06:49 | 200 |      2.8109ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/04 - 00:07:08 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 00:07:08 | 200 |     66.4741ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 00:07:08 | 200 |     31.4891ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 00:07:24 | 200 |    1.0419088s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 00:07:57 | 200 |   12.6396526s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 00:19:16 | 200 |      2.1178ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 15:22:30 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 15:22:30 | 200 |      3.5135ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/04 - 15:22:37 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 15:22:37 | 200 |     87.1124ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-04T15:22:38.064-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="62.3 GiB" free_swap="57.5 GiB"
time=2025-08-04T15:22:38.065-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-04T15:22:38.686-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 50222"
time=2025-08-04T15:22:38.694-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-04T15:22:38.694-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-04T15:22:38.694-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-04T15:22:38.745-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-04T15:22:38.842-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-04T15:22:38.842-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:50222"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-04T15:22:38.945-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-04T15:22:40.451-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/04 - 15:22:40 | 200 |    2.5707793s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 15:23:09 | 200 |    4.7419987s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:23:39 | 200 |    12.386445s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:25:14 | 200 |   24.5370179s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:25:48 | 200 |    6.3352931s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:26:16 | 200 |   10.3469596s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:27:42 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 15:27:42 | 200 |     74.8154ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/08/04 - 15:29:09 | 200 |      2.8268ms |             ::1 | GET      "/api/tags"
time=2025-08-04T15:31:48.771-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="61.5 GiB" free_swap="56.6 GiB"
time=2025-08-04T15:31:48.771-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-04T15:31:49.375-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 63166"
time=2025-08-04T15:31:49.385-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-04T15:31:49.385-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-04T15:31:49.385-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-04T15:31:49.436-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-04T15:31:49.533-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-04T15:31:49.534-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:63166"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-04T15:31:49.637-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-04T15:31:51.142-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/04 - 15:31:56 | 200 |    8.1274161s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/04 - 15:38:17 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 15:38:17 | 200 |      3.5765ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/04 - 15:39:45 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 15:39:45 | 200 |      3.4812ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/04 - 15:41:18 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 15:41:18 | 200 |     80.2662ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/08/04 - 15:41:38 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 15:41:38 | 200 |      4.0526ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/04 - 15:41:46 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 15:41:46 | 200 |      2.7514ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/04 - 15:41:56 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 15:41:56 | 200 |     86.2467ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-04T15:41:56.569-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="61.2 GiB" free_swap="56.2 GiB"
time=2025-08-04T15:41:56.569-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-04T15:41:57.171-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 60858"
time=2025-08-04T15:41:57.180-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-04T15:41:57.180-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-04T15:41:57.181-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-04T15:41:57.232-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-04T15:41:57.331-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-04T15:41:57.331-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:60858"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-04T15:41:57.432-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-04T15:41:58.937-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/04 - 15:41:58 | 200 |    2.5351087s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 15:42:33 | 200 |    6.0898641s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:42:58 | 200 |    10.051054s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:44:13 | 200 |    6.8402775s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:44:55 | 200 |    22.524021s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:45:19 | 200 |    6.4338475s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:46:44 | 200 |   34.3501093s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:49:10 | 200 |   36.6665861s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:50:01 | 200 |      2.6926ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 15:52:34 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 15:52:34 | 200 |     90.7625ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/08/04 - 15:53:26 | 200 |      2.7006ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 15:54:29 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 15:54:29 | 200 |     85.6712ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-04T15:54:29.962-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="60.5 GiB" free_swap="55.4 GiB"
time=2025-08-04T15:54:29.962-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-04T15:54:30.589-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 61386"
time=2025-08-04T15:54:30.599-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-04T15:54:30.599-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-04T15:54:30.599-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-04T15:54:30.649-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-04T15:54:30.745-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-04T15:54:30.745-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:61386"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-08-04T15:54:30.851-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-04T15:54:32.356-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/04 - 15:54:32 | 200 |    2.5689441s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 15:54:46 | 200 |    4.0971251s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:56:53 | 200 |    4.0410833s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:57:26 | 200 |    2.3155662s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 15:57:50 | 200 |    4.7388946s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 16:00:29 | 200 |    4.4764666s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/04 - 16:00:48 | 200 |    3.2993277s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/04 - 16:02:31 | 200 |    3.9302944s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/04 - 16:03:06 | 200 |    934.9051ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/04 - 16:03:31 | 200 |    3.5227854s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/04 - 16:05:51 | 200 |      3.5364ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/04 - 16:05:52 | 200 |    167.4517ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 16:05:52 | 200 |    169.9473ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 16:05:52 | 200 |     173.328ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 16:05:52 | 200 |     182.763ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 16:05:52 | 200 |    216.2013ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 16:05:52 | 200 |    220.0539ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 21:17:53 | 200 |      4.2633ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/04 - 21:17:53 | 200 |    155.6471ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 21:17:53 | 200 |    185.1694ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 21:17:53 | 200 |    207.2221ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 21:17:53 | 200 |    243.6095ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 21:17:53 | 200 |    260.4533ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 21:17:53 | 200 |    262.3065ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 21:17:58 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 21:17:58 | 200 |     80.3978ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-04T21:17:59.097-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="65.7 GiB" free_swap="61.2 GiB"
time=2025-08-04T21:17:59.097-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-04T21:17:59.701-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 62536"
time=2025-08-04T21:17:59.711-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-04T21:17:59.711-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-04T21:17:59.711-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-04T21:17:59.764-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-04T21:17:59.862-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-04T21:17:59.864-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:62536"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-04T21:17:59.963-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-04T21:18:01.466-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/04 - 21:18:01 | 200 |    2.5213617s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 21:18:10 | 200 |    2.2764792s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 21:25:49 | 200 |      4.0014ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/04 - 21:25:50 | 200 |    151.4451ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 21:25:50 | 200 |    154.4571ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 21:25:50 | 200 |    153.8221ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 21:25:50 | 200 |    167.0953ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 21:25:50 | 200 |    221.2805ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 21:25:50 | 200 |    229.2371ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 21:26:19 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 21:26:19 | 200 |    298.1448ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/04 - 21:26:36 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 21:26:37 | 200 |    484.8096ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/04 - 21:26:45 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 21:26:45 | 200 |    127.7549ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/04 - 21:26:52 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 21:26:52 | 200 |    133.6586ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/04 - 21:28:10 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 21:28:10 | 200 |    134.5135ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/04 - 21:28:18 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 21:28:18 | 200 |    124.7906ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/04 - 21:28:26 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 21:28:26 | 200 |      3.5161ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/04 - 21:52:55 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 21:52:55 | 200 |    695.6584ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/04 - 22:03:33 | 200 |       4.082ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:04:01 | 200 |      3.7436ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:04:31 | 200 |      2.7201ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:05:01 | 200 |      4.0501ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:05:31 | 200 |      4.1418ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:06:01 | 200 |       2.006ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:06:31 | 200 |      3.2821ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:06:31 | 200 |       2.663ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:06:45 | 200 |      2.4843ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:06:59 | 200 |      3.2592ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:07:00 | 200 |      2.1911ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:07:01 | 200 |      1.7425ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:07:13 | 200 |      3.2994ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:07:25 | 200 |      4.6677ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:07:28 | 200 |     13.9928ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:07:29 | 200 |      2.2686ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:07:31 | 200 |      9.4499ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:07:43 | 200 |       3.997ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:07:53 | 200 |      2.4858ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:07:58 | 200 |      3.8784ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:07:59 | 200 |      2.9079ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:08:01 | 200 |      3.3785ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:08:13 | 200 |      3.3493ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:08:23 | 200 |      2.3662ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:08:28 | 200 |      3.8423ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:08:29 | 200 |      2.6754ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:08:31 | 200 |       1.674ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:08:43 | 200 |      2.9566ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:08:53 | 200 |      4.1643ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:08:58 | 200 |      4.0463ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:08:59 | 200 |      4.4725ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:09:01 | 200 |       4.713ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:09:13 | 200 |       4.414ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:09:23 | 200 |      3.8112ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:09:28 | 200 |      4.2988ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:09:29 | 200 |      3.6094ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:09:31 | 200 |      3.9791ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:09:43 | 200 |      4.0202ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:09:53 | 200 |      3.2139ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:09:58 | 200 |      3.8122ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:09:59 | 200 |      1.1411ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:10:01 | 200 |      3.7696ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:10:13 | 200 |      4.4044ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:10:23 | 200 |      3.2312ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:10:28 | 200 |      2.5157ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:10:29 | 200 |      1.7243ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:10:31 | 200 |        3.83ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:10:43 | 200 |      3.9613ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:10:53 | 200 |      4.0746ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:10:58 | 200 |      3.2327ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:10:59 | 200 |      3.5926ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:11:01 | 200 |      4.7731ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:11:13 | 200 |       8.001ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:11:23 | 200 |      3.6935ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:11:28 | 200 |      2.8577ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:11:29 | 200 |      4.1661ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:11:31 | 200 |      2.8691ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:11:43 | 200 |       3.632ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:11:53 | 200 |      3.8906ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:11:58 | 200 |      3.8698ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:11:59 | 200 |      1.0345ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:12:01 | 200 |      3.7502ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:12:13 | 200 |      2.2931ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:12:23 | 200 |      3.7318ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:12:28 | 200 |      3.6807ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:12:29 | 200 |      1.6035ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:12:31 | 200 |      3.8338ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:12:43 | 200 |      5.1414ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:12:53 | 200 |      2.2883ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:12:58 | 200 |      3.9277ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:12:59 | 200 |      2.3002ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:13:01 | 200 |      4.1134ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:13:13 | 200 |      4.4535ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:13:23 | 200 |      3.5659ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:13:28 | 200 |      1.2107ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:13:29 | 200 |      3.5736ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:13:31 | 200 |      2.7926ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:13:43 | 200 |      2.2662ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:13:53 | 200 |      2.3687ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:13:58 | 200 |      2.3158ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:13:59 | 200 |      2.9003ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:14:01 | 200 |      2.2706ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:14:13 | 200 |      2.3677ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:14:23 | 200 |      3.6403ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:14:28 | 200 |       1.015ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:14:29 | 200 |      6.4179ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:14:31 | 200 |      2.3145ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:14:43 | 200 |      1.6635ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:14:53 | 200 |      2.6858ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:14:58 | 200 |      4.6464ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:14:59 | 200 |      3.5871ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:15:01 | 200 |      3.4128ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:15:13 | 200 |      2.6459ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:15:23 | 200 |      3.7452ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:15:28 | 200 |      3.5887ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:15:29 | 200 |      2.8415ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:15:32 | 200 |      3.8388ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:15:43 | 200 |      2.4049ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:15:53 | 200 |      3.1216ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:15:58 | 200 |      3.7474ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:15:59 | 200 |      3.4659ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:16:02 | 200 |      1.7374ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:16:13 | 200 |      4.9768ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:16:23 | 200 |      3.5232ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:16:28 | 200 |      3.6942ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:16:29 | 200 |      3.6988ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:16:32 | 200 |      4.0079ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:16:43 | 200 |      4.3234ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:16:53 | 200 |      3.3081ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:16:58 | 200 |      3.2727ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:16:59 | 200 |      2.9125ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:17:02 | 200 |      4.6154ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:17:13 | 200 |      2.2994ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:17:23 | 200 |      2.2654ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:17:28 | 200 |      2.2946ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:17:29 | 200 |      2.8188ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:17:32 | 200 |      2.7943ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:17:43 | 200 |       3.416ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:17:53 | 200 |       6.211ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:17:58 | 200 |      1.5432ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:17:59 | 200 |      2.2014ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:18:02 | 200 |      4.4946ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:18:13 | 200 |      1.6966ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:18:23 | 200 |      2.8515ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:18:28 | 200 |      2.6478ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:18:29 | 200 |      2.0886ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:18:32 | 200 |      2.1047ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:18:43 | 200 |      3.3464ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:18:53 | 200 |      3.0078ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:18:58 | 200 |      1.6852ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:18:59 | 200 |      3.1816ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:19:02 | 200 |      3.4241ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:19:13 | 200 |      3.4107ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:19:24 | 200 |      2.8681ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:19:28 | 200 |      3.4132ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:19:29 | 200 |      2.0935ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:19:32 | 200 |      2.8315ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:19:43 | 200 |      4.4357ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:19:54 | 200 |      3.5301ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:19:58 | 200 |      4.3096ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:19:59 | 200 |      2.2107ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:20:02 | 200 |      4.0062ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:20:13 | 200 |      3.4009ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:20:24 | 200 |      3.5489ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:20:28 | 200 |      10.036ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:20:29 | 200 |       2.834ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:20:32 | 200 |      3.0961ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:20:43 | 200 |      3.2759ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:20:54 | 200 |       2.557ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:20:58 | 200 |      2.8828ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:20:59 | 200 |      2.8769ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:21:02 | 200 |      2.8532ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:21:13 | 200 |      3.4225ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:21:24 | 200 |      2.9918ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:21:28 | 200 |      2.4747ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:21:29 | 200 |      1.3551ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:21:32 | 200 |      4.4389ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:21:43 | 200 |      3.7083ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:21:54 | 200 |      1.4883ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:21:58 | 200 |      1.9522ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:21:59 | 200 |      3.1203ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:22:02 | 200 |      2.2246ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:22:13 | 200 |      2.8249ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:22:24 | 200 |      3.9974ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:22:28 | 200 |      2.6804ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:22:29 | 200 |      1.9225ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:22:32 | 200 |      2.3007ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:22:43 | 200 |       3.319ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:22:54 | 200 |      3.5486ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:22:58 | 200 |      1.8453ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:22:59 | 200 |      2.2784ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:23:02 | 200 |      3.2569ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:23:13 | 200 |      2.4263ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:23:24 | 200 |      2.6586ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:23:28 | 200 |      3.6585ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:23:29 | 200 |      3.4628ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:23:32 | 200 |      3.0964ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:23:43 | 200 |      3.4645ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:23:54 | 200 |      2.8924ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:23:58 | 200 |       1.639ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:23:59 | 200 |      2.0865ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:24:02 | 200 |      3.2857ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:24:23 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:24:23 | 200 |      2.8808ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:42:12 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:42:12 | 200 |      2.7195ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/04 - 22:42:18 | 200 |       605.2µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:42:18 | 200 |     91.3693ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-04T22:42:18.592-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="67.2 GiB" free_swap="59.5 GiB"
time=2025-08-04T22:42:18.594-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-04T22:42:19.216-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 51672"
time=2025-08-04T22:42:19.224-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-04T22:42:19.224-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-04T22:42:19.225-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-04T22:42:19.274-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-04T22:42:19.370-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-04T22:42:19.371-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:51672"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
time=2025-08-04T22:42:19.477-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-04T22:42:20.981-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/04 - 22:42:23 | 200 |    5.4315297s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:42:23 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:42:24 | 200 |      88.206ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:42:26 | 200 |    2.2544687s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:42:26 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:42:26 | 200 |     84.4052ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:42:29 | 200 |    2.8501176s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:42:29 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:42:29 | 200 |     83.5687ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:42:34 | 200 |    4.5869206s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:42:34 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:42:34 | 200 |     84.6513ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:43:01 | 200 |   27.3230289s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:43:01 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:43:01 | 200 |     84.5419ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:43:04 | 200 |    2.8834586s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:43:04 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:43:04 | 200 |     87.4934ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:43:07 | 200 |    2.3596031s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:43:07 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:43:07 | 200 |     94.0458ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:43:10 | 200 |    3.4624048s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:43:10 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:43:10 | 200 |     83.8815ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:43:15 | 200 |    5.0129606s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:43:16 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:43:16 | 200 |     84.2365ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:43:33 | 200 |   17.6946615s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:43:33 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:43:33 | 200 |     91.0804ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:43:36 | 200 |    2.7282302s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:43:36 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:43:36 | 200 |     81.5212ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:43:39 | 200 |    2.2737397s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:43:39 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:43:39 | 200 |     81.1456ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:43:43 | 200 |     4.406092s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:43:43 | 200 |       529.2µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:43:43 | 200 |      86.717ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:43:47 | 200 |    3.8324901s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:43:47 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:43:47 | 200 |     86.9651ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:44:21 | 200 |   33.3809173s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:44:21 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:44:21 | 200 |    147.0539ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-04T22:44:21.696-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="418.8 MiB"
time=2025-08-04T22:44:22.172-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="67.3 GiB" free_swap="59.7 GiB"
time=2025-08-04T22:44:22.173-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=20 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="10.1 GiB" memory.required.partial="5.1 GiB" memory.required.kv="1.5 GiB" memory.required.allocations="[5.1 GiB]" memory.weights.total="6.8 GiB" memory.weights.repeating="6.0 GiB" memory.weights.nonrepeating="787.5 MiB" memory.graph.full="575.0 MiB" memory.graph.partial="1.3 GiB"
time=2025-08-04T22:44:22.286-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2 --ctx-size 16384 --batch-size 512 --n-gpu-layers 20 --threads 4 --no-mmap --parallel 1 --port 51762"
time=2025-08-04T22:44:22.295-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-04T22:44:22.295-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-04T22:44:22.296-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-04T22:44:22.347-04:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-04T22:44:22.348-04:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:51762"
time=2025-08-04T22:44:22.370-04:00 level=INFO source=ggml.go:92 msg="" architecture=gemma3 file_type=Q4_K_M name="Gemma 3 12b It" description="" num_tensors=626 num_key_values=41
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-04T22:44:22.464-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-04T22:44:22.526-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CPU size="5.0 GiB"
time=2025-08-04T22:44:22.526-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CUDA0 size="2.5 GiB"
time=2025-08-04T22:44:22.548-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-04T22:44:22.737-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="578.0 MiB"
time=2025-08-04T22:44:22.738-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="543.0 MiB"
time=2025-08-04T22:44:23.802-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.51 seconds"
[GIN] 2025/08/04 - 22:45:16 | 200 |   55.1088558s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:45:16 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:45:16 | 200 |    144.4081ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:45:31 | 200 |   14.2980902s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:45:31 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:45:31 | 200 |    149.1277ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:45:48 | 200 |   17.6284376s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:45:49 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:45:49 | 200 |    159.3828ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:46:19 | 200 |   30.2895917s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:46:19 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:46:19 | 200 |    151.4221ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:47:19 | 200 |   59.8232102s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:47:19 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:47:19 | 200 |    117.2748ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-04T22:47:24.801-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0254086 runner.size="10.1 GiB" runner.vram="5.1 GiB" runner.parallel=1 runner.pid=144400 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
time=2025-08-04T22:47:25.004-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="67.3 GiB" free_swap="59.7 GiB"
time=2025-08-04T22:47:25.005-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=22 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="9.6 GiB" memory.required.partial="5.1 GiB" memory.required.kv="992.0 MiB" memory.required.allocations="[5.1 GiB]" memory.weights.total="6.8 GiB" memory.weights.repeating="6.0 GiB" memory.weights.nonrepeating="787.5 MiB" memory.graph.full="519.5 MiB" memory.graph.partial="1.3 GiB"
time=2025-08-04T22:47:25.051-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2753661 runner.size="10.1 GiB" runner.vram="5.1 GiB" runner.parallel=1 runner.pid=144400 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
time=2025-08-04T22:47:25.118-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2 --ctx-size 8192 --batch-size 512 --n-gpu-layers 22 --threads 4 --no-mmap --parallel 1 --port 51887"
time=2025-08-04T22:47:25.128-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-04T22:47:25.128-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-04T22:47:25.128-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-04T22:47:25.179-04:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-04T22:47:25.181-04:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:51887"
time=2025-08-04T22:47:25.203-04:00 level=INFO source=ggml.go:92 msg="" architecture=gemma3 file_type=Q4_K_M name="Gemma 3 12b It" description="" num_tensors=626 num_key_values=41
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-04T22:47:25.297-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-04T22:47:25.301-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.525463 runner.size="10.1 GiB" runner.vram="5.1 GiB" runner.parallel=1 runner.pid=144400 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
time=2025-08-04T22:47:25.367-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CPU size="4.8 GiB"
time=2025-08-04T22:47:25.367-04:00 level=INFO source=ggml.go:351 msg="model weights" buffer=CUDA0 size="2.8 GiB"
time=2025-08-04T22:47:25.379-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-04T22:47:25.527-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="306.0 MiB"
time=2025-08-04T22:47:25.527-04:00 level=INFO source=ggml.go:638 msg="compute graph" backend=CPU buffer_type=CPU size="287.0 MiB"
time=2025-08-04T22:47:26.635-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.51 seconds"
[GIN] 2025/08/04 - 22:47:56 | 200 |   37.2571481s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:47:57 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:47:57 | 200 |    138.2214ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:48:09 | 200 |   12.2778746s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:48:09 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:48:09 | 200 |    149.0875ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:48:29 | 200 |   19.3713458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:48:29 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:48:29 | 200 |    146.5689ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:48:52 | 200 |   23.4802763s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:48:52 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:48:53 | 200 |    147.1492ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:49:52 | 200 |   59.8417153s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:49:52 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:49:52 | 200 |     72.0316ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-04T22:49:53.110-04:00 level=INFO source=sched.go:548 msg="updated VRAM based on existing loaded models" gpu=GPU-779a6919-c944-8bfa-0888-f050b82da869 library=cuda total="6.0 GiB" available="915.1 MiB"
time=2025-08-04T22:49:58.136-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0243239 runner.size="9.6 GiB" runner.vram="5.1 GiB" runner.parallel=1 runner.pid=140764 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
time=2025-08-04T22:49:58.195-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="67.3 GiB" free_swap="59.7 GiB"
time=2025-08-04T22:49:58.195-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
time=2025-08-04T22:49:58.386-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2743909 runner.size="9.6 GiB" runner.vram="5.1 GiB" runner.parallel=1 runner.pid=140764 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
load: special tokens cache size = 22
time=2025-08-04T22:49:58.637-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5249613 runner.size="9.6 GiB" runner.vram="5.1 GiB" runner.parallel=1 runner.pid=140764 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-9610e3e07375303f6cd89086b496bcc1ab581177f52042eff536475a29283ba2
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-04T22:49:58.737-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 51988"
time=2025-08-04T22:49:58.746-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-04T22:49:58.746-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-04T22:49:58.747-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-04T22:49:58.797-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-04T22:49:58.900-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-04T22:49:58.901-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:51988"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-04T22:49:58.998-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-04T22:50:00.502-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/04 - 22:50:03 | 200 |   10.5075248s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:50:03 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:50:03 | 200 |     71.5877ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:50:06 | 200 |     2.801648s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:50:06 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:50:06 | 200 |     88.9776ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:50:10 | 200 |    4.1619764s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:50:10 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:50:10 | 200 |     83.3042ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:50:14 | 200 |    3.3820112s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 22:50:14 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 22:50:14 | 200 |     82.3406ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/04 - 22:50:46 | 200 |   31.7958965s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 23:02:06 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 23:02:06 | 200 |       3.736ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/04 - 23:02:22 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/04 - 23:02:22 | 200 |     88.2778ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-04T23:02:22.298-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="67.2 GiB" free_swap="59.8 GiB"
time=2025-08-04T23:02:22.299-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-04T23:02:22.936-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 52503"
time=2025-08-04T23:02:22.945-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-04T23:02:22.945-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-04T23:02:22.945-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-04T23:02:23.000-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-04T23:02:23.102-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-04T23:02:23.103-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:52503"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-04T23:02:23.197-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-04T23:02:24.700-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.75 seconds"
[GIN] 2025/08/04 - 23:02:24 | 200 |    2.5421754s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/04 - 23:02:27 | 200 |    1.0939351s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 23:02:47 | 200 |    4.1688384s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 23:02:57 | 200 |    1.8707124s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 23:03:16 | 200 |    1.5901827s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 23:05:38 | 200 |    3.0902323s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/04 - 23:05:57 | 200 |    3.5133088s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/05 - 00:00:19 | 404 |      3.5161ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/05 - 00:04:15 | 404 |      2.8677ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/05 - 00:04:59 | 404 |      2.7453ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/05 - 21:27:52 | 200 |       527.9µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/05 - 21:27:52 | 200 |      4.4996ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/05 - 21:28:59 | 200 |      3.1994ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/05 - 21:51:41 | 200 |      2.3658ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/05 - 21:51:42 | 200 |    134.4787ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/05 - 21:51:42 | 200 |    137.1208ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/05 - 21:51:42 | 200 |    142.7384ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/05 - 21:51:42 | 200 |    147.4117ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/05 - 21:51:42 | 200 |    196.2021ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/05 - 21:51:42 | 200 |    210.2897ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/06 - 21:05:25 | 200 |      4.0449ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/06 - 21:05:28 | 200 |    155.0434ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/06 - 21:05:28 | 200 |    156.5605ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/06 - 21:05:28 | 200 |    161.3228ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/06 - 21:05:28 | 200 |    169.3033ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/06 - 21:05:28 | 200 |    203.6112ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/06 - 21:05:28 | 200 |    222.6349ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/07 - 19:27:12 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/07 - 19:27:12 | 200 |      4.9561ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/07 - 22:49:30 | 200 |      4.3292ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/07 - 22:49:31 | 200 |    148.8637ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/07 - 22:49:31 | 200 |     159.489ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/07 - 22:49:31 | 200 |    163.4566ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/07 - 22:49:31 | 200 |    180.9993ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/07 - 22:49:31 | 200 |    208.0279ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/07 - 22:49:31 | 200 |    211.8944ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/08 - 20:35:11 | 200 |      2.9653ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/08 - 20:35:11 | 200 |    123.0741ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/08 - 20:35:11 | 200 |    130.5312ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/08 - 20:35:11 | 200 |    138.6946ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/08 - 20:35:11 | 200 |    140.0297ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/08 - 20:35:11 | 200 |    162.0098ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/08 - 20:35:11 | 200 |     172.254ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/08 - 20:42:21 | 200 |      2.2902ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/08 - 20:42:21 | 200 |    409.6791ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/08 - 20:42:21 | 200 |    527.4902ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/08 - 20:42:21 | 200 |    534.0824ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/08 - 20:42:21 | 200 |    534.5736ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/08 - 20:42:21 | 200 |    730.9406ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/08 - 20:42:21 | 200 |    737.9666ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/09 - 13:57:04 | 200 |     22.1904ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/09 - 13:57:04 | 200 |    231.9419ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/09 - 13:57:04 | 200 |    246.7985ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/09 - 13:57:04 | 200 |    249.3486ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/09 - 13:57:04 | 200 |    250.1028ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/09 - 13:57:04 | 200 |    313.1545ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/09 - 13:57:04 | 200 |    319.6026ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/09 - 14:07:43 | 200 |       8.945ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/09 - 14:07:46 | 200 |    1.6561234s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/09 - 14:07:46 | 200 |    1.2676483s |             ::1 | POST     "/api/show"
[GIN] 2025/08/09 - 14:07:46 | 200 |    1.8714402s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/09 - 14:07:46 | 200 |    1.8818824s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/09 - 14:07:46 | 200 |    1.9941444s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/09 - 14:07:46 | 200 |    2.1938507s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/10 - 20:39:21 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/10 - 20:39:21 | 200 |      18.522ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/10 - 20:39:56 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/10 - 20:39:56 | 200 |     134.809ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-10T20:39:56.491-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="51.0 GiB" free_swap="44.4 GiB"
time=2025-08-10T20:39:56.493-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=19 layers.split="" memory.available="[4.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="3.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[3.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-10T20:39:57.292-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 19 --threads 4 --no-mmap --parallel 1 --port 63654"
time=2025-08-10T20:39:57.306-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-10T20:39:57.306-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-10T20:39:57.308-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-10T20:39:57.406-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-10T20:39:58.137-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-10T20:39:58.139-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:63654"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-10T20:39:58.314-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 19 repeating layers to GPU
load_tensors: offloaded 19/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =  1620.95 MiB
load_tensors:        CUDA0 model buffer size =  2547.14 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   152.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    72.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 130 (with bs=512), 3 (with bs=1)
time=2025-08-10T20:40:06.596-04:00 level=INFO source=server.go:630 msg="llama runner started in 9.29 seconds"
[GIN] 2025/08/10 - 20:40:06 | 200 |   10.3472546s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/10 - 20:40:48 | 200 |   11.5699828s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/10 - 20:41:58 | 200 |   17.3435115s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/10 - 20:45:46 | 200 |   12.7002371s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-10T20:51:47.985-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=61.2221426 runner.name=registry.ollama.ai/library/nexusai:latest runner.inference=cuda runner.devices=1 runner.size="5.6 GiB" runner.vram="3.9 GiB" runner.parallel=1 runner.pid=105788 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 runner.num_ctx=4096
[GIN] 2025/08/10 - 21:40:18 | 200 |     230.752ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:46:14 | 200 |      9.9462ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:49:59 | 200 |      6.7662ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:50:01 | 200 |      5.5015ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:50:29 | 200 |      8.0574ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:50:51 | 200 |      5.3735ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:50:59 | 200 |      6.6812ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:51:29 | 200 |      5.0331ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:51:59 | 200 |      7.2392ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:52:29 | 200 |       4.741ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:52:59 | 200 |      8.0726ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:53:29 | 200 |     11.1177ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:53:59 | 200 |      5.0088ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:54:29 | 200 |      5.0715ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:54:59 | 200 |      6.4129ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:55:29 | 200 |      4.9861ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:55:59 | 200 |      5.3377ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:56:29 | 200 |      5.4209ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:56:59 | 200 |      7.3096ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:57:29 | 200 |      5.4249ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:57:59 | 200 |     18.8916ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:58:29 | 200 |     24.9034ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:58:59 | 200 |      8.4948ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:59:29 | 200 |      8.0872ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 21:59:59 | 200 |     17.4864ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:00:29 | 200 |      6.7016ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:00:59 | 200 |      7.1167ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:01:29 | 200 |     14.3262ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:01:35 | 200 |       6.972ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:01:59 | 200 |     54.3233ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:02:03 | 200 |     17.2111ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:02:29 | 200 |      7.6455ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:02:33 | 200 |      5.7375ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:02:59 | 200 |      9.4104ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:03:03 | 200 |      5.0181ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:03:29 | 200 |      5.6941ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:03:59 | 200 |      5.9882ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:04:29 | 200 |      5.3864ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:04:59 | 200 |      9.5334ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:05:29 | 200 |      5.7713ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:05:33 | 200 |       5.591ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:05:33 | 200 |     217.957ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/10 - 22:05:33 | 200 |    219.0486ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/10 - 22:05:33 | 200 |     246.316ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/10 - 22:05:33 | 200 |    241.9946ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/10 - 22:05:33 | 200 |    333.4639ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/10 - 22:05:33 | 200 |    330.7083ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/10 - 22:05:59 | 200 |     12.8567ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:06:29 | 200 |      5.7578ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:06:59 | 200 |      8.8395ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:07:29 | 200 |      7.6042ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:07:59 | 200 |      8.5755ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:08:29 | 200 |      5.0042ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:08:59 | 200 |      5.3151ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:09:29 | 200 |       7.203ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:09:59 | 200 |      4.1359ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:10:29 | 200 |      7.3158ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:10:59 | 200 |       3.401ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:11:29 | 200 |      2.8235ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:11:59 | 200 |     17.5826ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:12:29 | 200 |      2.0651ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:12:59 | 200 |      4.5031ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:13:29 | 200 |      8.6584ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:13:59 | 200 |      7.3482ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:14:29 | 200 |      5.9332ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:14:59 | 200 |      5.4833ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:15:29 | 200 |      2.6214ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:15:59 | 200 |     11.9664ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:16:29 | 200 |      6.9764ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:16:59 | 200 |      6.3671ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:17:29 | 200 |      5.4344ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:17:59 | 200 |      6.3152ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:18:29 | 200 |      2.4876ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:18:59 | 200 |      6.3902ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:19:29 | 200 |      5.3123ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:20:00 | 200 |     10.6619ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:20:30 | 200 |      5.6506ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:21:00 | 200 |      7.0628ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:21:30 | 200 |      5.9676ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:22:00 | 200 |     15.4124ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:22:30 | 200 |      5.9558ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:23:00 | 200 |      6.9541ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:23:30 | 200 |      6.4906ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:24:00 | 200 |     16.7593ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:24:30 | 200 |      3.2596ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:25:00 | 200 |      7.0519ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:25:30 | 200 |      10.201ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:26:00 | 200 |     10.2266ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:26:30 | 200 |      4.0548ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:27:00 | 200 |      6.4314ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:27:30 | 200 |     15.3703ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:28:00 | 200 |     34.9334ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:28:30 | 200 |      5.8535ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:29:00 | 200 |       6.918ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:29:30 | 200 |      5.7899ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:30:00 | 200 |      8.9975ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:30:30 | 200 |      4.5108ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:31:00 | 200 |      5.9216ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:31:30 | 200 |      6.4868ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:32:00 | 200 |      6.8038ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:32:30 | 200 |       4.309ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:33:00 | 200 |      5.0519ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:33:30 | 200 |      6.6592ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:34:00 | 200 |      6.1387ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:34:30 | 200 |      6.6236ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:35:00 | 200 |      5.9675ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:35:30 | 200 |      4.6784ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:36:00 | 200 |      7.2731ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:36:30 | 200 |      4.9905ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:37:00 | 200 |      5.5985ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:37:30 | 200 |      7.8402ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:38:00 | 200 |      4.8892ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:38:30 | 200 |      5.7851ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:39:00 | 200 |      4.6663ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:39:30 | 200 |      5.4914ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:40:00 | 200 |      7.0721ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:40:30 | 200 |      6.8945ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:41:00 | 200 |      7.2059ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:41:30 | 200 |      7.6746ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:42:00 | 200 |        5.58ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:42:30 | 200 |      5.8344ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/10 - 22:43:00 | 200 |      5.2075ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/11 - 10:49:48 | 200 |      5.5932ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/11 - 10:49:48 | 200 |    222.0766ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/11 - 10:49:48 | 200 |    224.6955ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/11 - 10:49:48 | 200 |    251.8671ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/11 - 10:49:48 | 200 |    262.7345ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/11 - 10:49:48 | 200 |    335.4867ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/11 - 10:49:48 | 200 |    338.4538ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/11 - 11:53:56 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 11:53:56 | 200 |      6.8288ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/11 - 11:57:37 | 200 |      9.9383ms |             ::1 | GET      "/api/tags"
time=2025-08-11T12:00:39.561-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="60.8 GiB" free_swap="56.7 GiB"
time=2025-08-11T12:00:39.563-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=27 layers.split="" memory.available="[5.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.0 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-11T12:00:40.711-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 27 --threads 4 --no-mmap --parallel 1 --port 56292"
time=2025-08-11T12:00:40.730-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-11T12:00:40.731-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-11T12:00:40.732-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-11T12:00:40.928-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-11T12:00:41.300-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-11T12:00:41.302-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:56292"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-11T12:00:41.490-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 27 repeating layers to GPU
load_tensors: offloaded 27/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   568.58 MiB
load_tensors:        CUDA0 model buffer size =  3599.51 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   216.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =     8.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 18 (with bs=512), 3 (with bs=1)
time=2025-08-11T12:00:50.804-04:00 level=INFO source=server.go:630 msg="llama runner started in 10.07 seconds"
[GIN] 2025/08/11 - 12:01:04 | 200 |   25.3994797s |             ::1 | POST     "/api/generate"
time=2025-08-11T12:06:26.336-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=21.6512809 runner.name=registry.ollama.ai/library/nexusai:latest runner.inference=cuda runner.devices=1 runner.size="5.6 GiB" runner.vram="5.0 GiB" runner.parallel=1 runner.pid=223552 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 runner.num_ctx=4096
[GIN] 2025/08/11 - 12:09:51 | 200 |     12.3849ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/11 - 12:12:12 | 200 |     13.8725ms |             ::1 | GET      "/api/tags"
time=2025-08-11T12:12:47.428-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="64.1 GiB" free_swap="60.8 GiB"
time=2025-08-11T12:12:47.430-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=27 layers.split="" memory.available="[5.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.0 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-11T12:12:48.432-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 27 --threads 4 --no-mmap --parallel 1 --port 57141"
time=2025-08-11T12:12:48.451-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-11T12:12:48.451-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-11T12:12:48.452-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-11T12:12:48.689-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-11T12:12:48.865-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-11T12:12:48.867-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:57141"
time=2025-08-11T12:12:48.956-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 27 repeating layers to GPU
load_tensors: offloaded 27/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   568.58 MiB
load_tensors:        CUDA0 model buffer size =  3599.51 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   216.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =     8.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 18 (with bs=512), 3 (with bs=1)
time=2025-08-11T12:12:51.716-04:00 level=INFO source=server.go:630 msg="llama runner started in 3.27 seconds"
[GIN] 2025/08/11 - 12:13:01 | 200 |   14.6616719s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 12:20:11 | 200 |     26.4438ms |             ::1 | GET      "/api/tags"
time=2025-08-11T12:21:54.968-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="61.2 GiB" free_swap="57.7 GiB"
time=2025-08-11T12:21:54.971-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=27 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.0 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-11T12:21:56.288-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 27 --threads 4 --no-mmap --parallel 1 --port 55933"
time=2025-08-11T12:21:56.306-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-11T12:21:56.306-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-11T12:21:56.308-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-11T12:21:56.563-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-11T12:21:56.762-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-11T12:21:56.764-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:55933"
time=2025-08-11T12:21:56.814-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 27 repeating layers to GPU
load_tensors: offloaded 27/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   568.58 MiB
load_tensors:        CUDA0 model buffer size =  3599.51 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   216.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =     8.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 18 (with bs=512), 3 (with bs=1)
time=2025-08-11T12:22:09.140-04:00 level=INFO source=server.go:630 msg="llama runner started in 12.83 seconds"
[GIN] 2025/08/11 - 12:22:24 | 500 |   30.0120664s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 12:22:44 | 200 |   19.9268579s |             ::1 | POST     "/api/generate"
time=2025-08-11T12:32:02.190-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="65.9 GiB" free_swap="58.7 GiB"
time=2025-08-11T12:32:02.210-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=26 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-11T12:32:04.732-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 26 --threads 4 --no-mmap --parallel 1 --port 52043"
time=2025-08-11T12:32:04.781-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-11T12:32:04.781-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-11T12:32:04.782-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-11T12:32:04.971-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
time=2025-08-11T12:32:05.305-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-11T12:32:05.297-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-11T12:32:05.299-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:52043"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-08-11T12:32:13.124-04:00 level=INFO source=server.go:630 msg="llama runner started in 8.34 seconds"
[GIN] 2025/08/11 - 12:32:31 | 500 |   29.9950431s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 12:36:29 | 200 |    27.470182s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 12:36:52 | 200 |   22.5824326s |             ::1 | POST     "/api/generate"
time=2025-08-11T12:43:52.270-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="66.1 GiB" free_swap="59.2 GiB"
time=2025-08-11T12:43:52.271-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=26 layers.split="" memory.available="[5.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-11T12:43:53.461-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 26 --threads 4 --no-mmap --parallel 1 --port 58876"
time=2025-08-11T12:43:53.479-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-11T12:43:53.480-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-11T12:43:53.481-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-11T12:43:53.762-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-11T12:43:54.152-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-11T12:43:54.156-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:58876"
time=2025-08-11T12:43:54.241-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-08-11T12:43:59.018-04:00 level=INFO source=server.go:630 msg="llama runner started in 5.54 seconds"
[GIN] 2025/08/11 - 12:44:22 | 500 |   30.0058204s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 12:44:41 | 200 |   18.9897543s |             ::1 | POST     "/api/generate"
time=2025-08-11T12:58:07.923-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="66.1 GiB" free_swap="59.5 GiB"
time=2025-08-11T12:58:07.925-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=26 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-11T12:58:11.405-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 26 --threads 4 --no-mmap --parallel 1 --port 57978"
time=2025-08-11T12:58:11.432-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-11T12:58:11.432-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-11T12:58:11.434-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-11T12:58:11.657-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-11T12:58:11.889-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-11T12:58:11.890-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:57978"
time=2025-08-11T12:58:11.943-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 26 repeating layers to GPU
load_tensors: offloaded 26/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   710.78 MiB
load_tensors:        CUDA0 model buffer size =  3457.31 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   208.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 32 (with bs=512), 3 (with bs=1)
time=2025-08-11T12:58:19.820-04:00 level=INFO source=server.go:630 msg="llama runner started in 8.39 seconds"
[GIN] 2025/08/11 - 12:58:37 | 500 |   30.0264937s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 12:59:05 | 200 |   28.4729683s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 13:00:08 | 200 |   17.0065813s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 13:02:38 | 500 |   30.0212779s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 13:02:43 | 500 |    5.2549591s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 19:03:29 | 200 |         632µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 19:03:29 | 200 |    394.9225ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/11 - 19:03:37 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 19:03:37 | 200 |     11.6921ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/11 - 19:04:05 | 200 |            0s |       127.0.0.1 | HEAD     "/"
time=2025-08-11T19:04:05.513-04:00 level=INFO source=download.go:177 msg="downloading 819c2adf5ce6 in 7 100 MB part(s)"
time=2025-08-11T19:04:21.121-04:00 level=INFO source=download.go:177 msg="downloading c71d239df917 in 1 11 KB part(s)"
time=2025-08-11T19:04:22.301-04:00 level=INFO source=download.go:177 msg="downloading b837481ff855 in 1 16 B part(s)"
time=2025-08-11T19:04:23.462-04:00 level=INFO source=download.go:177 msg="downloading 38badd946f91 in 1 408 B part(s)"
[GIN] 2025/08/11 - 19:04:26 | 200 |   21.0775241s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/11 - 19:06:06 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 19:06:06 | 200 |    125.5513ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/11 - 19:07:20 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 19:07:20 | 200 |      8.0595ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/11 - 19:07:32 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 19:07:32 | 200 |     39.3267ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/11 - 19:07:49 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 19:07:49 | 200 |     35.1031ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/11 - 19:07:49 | 400 |     19.4101ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/11 - 20:03:36 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 20:03:36 | 200 |      8.0251ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/11 - 20:11:51 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 20:11:51 | 200 |      6.8455ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/11 - 20:21:13 | 200 |       765.7µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 20:21:13 | 200 |            0s |       127.0.0.1 | GET      "/api/ps"
[GIN] 2025/08/11 - 20:22:08 | 200 |      7.8238ms |             ::1 | GET      "/api/tags"
time=2025-08-11T20:28:47.883-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="62.0 GiB" free_swap="57.4 GiB"
time=2025-08-11T20:28:47.885-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-11T20:28:48.947-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 55153"
time=2025-08-11T20:28:48.966-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-11T20:28:48.966-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-11T20:28:48.968-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-11T20:28:49.151-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-11T20:28:49.357-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-11T20:28:49.362-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:55153"
time=2025-08-11T20:28:49.476-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-11T20:29:02.562-04:00 level=INFO source=server.go:630 msg="llama runner started in 13.60 seconds"
[GIN] 2025/08/11 - 20:29:17 | 500 |   30.0241073s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 20:29:38 | 200 |   20.7353542s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 20:30:04 | 500 |   30.0315074s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 20:30:18 | 200 |   13.9459852s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/11 - 20:32:12 | 200 |      4.6135ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/11 - 20:35:24 | 200 |       627.2µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 20:35:24 | 200 |      7.4635ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/11 - 20:37:21 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 20:37:21 | 200 |            0s |       127.0.0.1 | GET      "/api/ps"
[GIN] 2025/08/11 - 20:37:29 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 20:37:29 | 200 |    132.7007ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-11T20:37:29.862-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="67.5 GiB" free_swap="64.0 GiB"
time=2025-08-11T20:37:29.864-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-11T20:37:30.774-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 55580"
time=2025-08-11T20:37:30.786-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-11T20:37:30.787-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-11T20:37:30.787-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-11T20:37:30.918-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-11T20:37:31.116-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-11T20:37:31.118-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:55580"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
time=2025-08-11T20:37:31.303-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-11T20:37:33.332-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.55 seconds"
[GIN] 2025/08/11 - 20:37:33 | 200 |    3.7320804s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/11 - 20:37:49 | 200 |     5.109875s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 20:38:27 | 200 |    6.7841269s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 20:38:57 | 200 |     7.492846s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 20:39:52 | 200 |    5.0126427s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 20:41:24 | 200 |   53.6730274s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 20:42:55 | 200 |   54.5895775s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 20:43:30 | 200 |   20.7589678s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 20:45:01 | 200 |          1m2s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 20:46:58 | 200 |         1m11s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 20:47:32 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 20:47:32 | 200 |      2.9957ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/11 - 20:48:12 | 200 |   22.6543317s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 20:49:57 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 20:49:58 | 200 |    571.4539ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/11 - 20:51:58 | 200 |   28.0257888s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 20:53:06 | 200 |   15.5598039s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 20:56:22 | 200 |    56.227779s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 20:58:41 | 200 |   45.4667644s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/11 - 21:02:21 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/11 - 21:02:21 | 200 |      5.9981ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/11 - 22:04:57 | 200 |      3.4594ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/11 - 22:05:25 | 200 |      9.3791ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/11 - 22:05:55 | 200 |      8.1321ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/11 - 22:06:25 | 200 |      9.0282ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/11 - 22:14:59 | 403 |      1.2443ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/11 - 22:15:01 | 403 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/11 - 22:15:03 | 403 |            0s |             ::1 | GET      "/api/tags"
[GIN] 2025/08/11 - 22:16:57 | 404 |            0s |             ::1 | GET      "/health"
[GIN] 2025/08/11 - 22:17:27 | 404 |       302.7µs |             ::1 | GET      "/health"
[GIN] 2025/08/11 - 22:18:14 | 200 |     11.5824ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/11 - 22:18:14 | 200 |     11.4102ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/11 - 22:18:24 | 404 |      4.8053ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 01:15:58 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 01:15:58 | 200 |      4.2604ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 11:17:00 | 200 |       617.2µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 11:17:00 | 200 |      84.187ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-12T11:17:00.724-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="62.5 GiB" free_swap="58.0 GiB"
time=2025-08-12T11:17:00.725-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T11:17:01.312-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 55316"
time=2025-08-12T11:17:01.319-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T11:17:01.319-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T11:17:01.320-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T11:17:01.380-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T11:17:01.477-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T11:17:01.478-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:55316"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-12T11:17:01.571-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-12T11:17:03.075-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/12 - 11:17:16 | 200 |   16.3530474s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 11:17:17 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 11:17:17 | 200 |     82.1548ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 11:17:30 | 200 |   13.6489162s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 11:19:04 | 200 |       658.5µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 11:19:04 | 200 |     83.7695ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 11:19:06 | 200 |    2.0605258s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 11:19:06 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 11:19:06 | 200 |     81.8037ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 11:19:08 | 200 |    2.5178216s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 11:44:09 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 11:44:10 | 200 |     85.2169ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-12T11:44:10.250-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="62.7 GiB" free_swap="58.0 GiB"
time=2025-08-12T11:44:10.252-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T11:44:10.872-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 57619"
time=2025-08-12T11:44:10.879-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T11:44:10.879-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T11:44:10.880-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T11:44:10.942-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T11:44:11.036-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T11:44:11.036-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:57619"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-12T11:44:11.132-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-12T11:44:12.385-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.51 seconds"
[GIN] 2025/08/12 - 11:44:29 | 200 |   19.6862169s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 11:48:19 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 11:48:19 | 200 |     77.0453ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 11:48:22 | 200 |    3.4284343s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 11:49:31 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 11:49:31 | 200 |     82.5085ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 11:49:38 | 200 |    7.0588078s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 11:50:08 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 11:50:09 | 200 |     85.6349ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 11:50:13 | 200 |    4.7149412s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 11:51:59 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 11:51:59 | 200 |     88.8251ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 11:52:06 | 200 |    6.4387268s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 12:06:01 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 12:06:01 | 200 |    505.5169ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/12 - 12:10:59 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 12:10:59 | 200 |     84.0094ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-12T12:11:00.186-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="62.3 GiB" free_swap="57.8 GiB"
time=2025-08-12T12:11:00.187-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T12:11:00.777-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 58887"
time=2025-08-12T12:11:00.786-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T12:11:00.786-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T12:11:00.787-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T12:11:00.849-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T12:11:00.943-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T12:11:00.944-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:58887"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-12T12:11:01.039-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-12T12:11:02.543-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/12 - 12:11:11 | 200 |   11.5313973s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 16:20:03 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 16:20:03 | 200 |      4.5503ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 16:20:20 | 200 |       527.4µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 16:20:20 | 200 |    445.1278ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/12 - 16:20:30 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 16:20:30 | 200 |     13.6759ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 16:20:30 | 400 |      5.0349ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 16:20:45 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 16:20:45 | 200 |    149.3036ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/12 - 16:20:51 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 16:20:51 | 200 |     152.086ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/12 - 16:21:01 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 16:21:01 | 200 |    146.9779ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/12 - 16:21:06 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 16:21:07 | 200 |    123.7725ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/12 - 16:21:14 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 16:21:14 | 200 |    143.2284ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/12 - 16:21:22 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 16:21:22 | 200 |    130.3831ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/12 - 16:21:42 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 16:21:42 | 200 |     145.201ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/12 - 16:26:48 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 16:26:48 | 200 |    406.6181ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/12 - 17:47:19 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/08/12 - 17:47:19 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 17:47:19 | 200 |      6.6836ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 17:47:25 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 17:47:25 | 200 |     16.6459ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 17:47:33 | 200 |    114.0151ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:33 | 200 |    116.3804ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:33 | 200 |    116.3562ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:34 | 200 |    434.1866ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:34 | 200 |     445.677ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:34 | 200 |     444.801ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:34 | 200 |    184.2126ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:34 | 200 |    178.5521ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:34 | 200 |    187.7864ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:35 | 200 |    102.3477ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:35 | 200 |    102.1983ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:35 | 200 |    102.3451ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:42 | 200 |    235.7902ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:42 | 200 |    243.0801ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:42 | 200 |    262.6838ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:42 | 200 |    122.3617ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:42 | 200 |    123.3728ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:47:42 | 200 |    123.7967ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-12T17:51:00.229-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="60.5 GiB" free_swap="56.2 GiB"
time=2025-08-12T17:51:00.229-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=11 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="8.7 GiB" memory.required.partial="5.0 GiB" memory.required.kv="1.8 GiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="1.8 GiB" memory.graph.partial="2.3 GiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T17:51:00.758-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 32768 --batch-size 512 --n-gpu-layers 11 --threads 4 --no-mmap --parallel 1 --port 63658"
time=2025-08-12T17:51:00.765-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T17:51:00.765-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T17:51:00.766-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T17:51:00.836-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T17:51:00.937-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T17:51:00.938-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:63658"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-12T17:51:01.017-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 11 repeating layers to GPU
load_tensors: offloaded 11/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =  2689.11 MiB
load_tensors:        CUDA0 model buffer size =  1478.98 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32768
llama_context: n_ctx_per_seq = 32768
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 32768, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   704.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =  1088.00 MiB
llama_kv_cache_unified: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB
llama_context:      CUDA0 compute buffer size =  1954.89 MiB
llama_context:  CUDA_Host compute buffer size =    71.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 242 (with bs=512), 3 (with bs=1)
time=2025-08-12T17:51:03.273-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.51 seconds"
[GIN] 2025/08/12 - 17:51:12 | 200 |   12.4146386s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/12 - 17:51:13 | 200 |    1.1349863s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/12 - 17:55:43 | 200 |    7.3737416s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/12 - 17:56:14 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 17:56:14 | 200 |      3.4347ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 17:56:37 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 17:56:37 | 404 |      2.8602ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 17:56:38 | 200 |    493.9018ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/12 - 17:56:38 | 200 |     64.6223ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-12T17:56:39.109-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="60.4 GiB" free_swap="56.0 GiB"
time=2025-08-12T17:56:39.109-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=27 layers.split="" memory.available="[5.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.0 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T17:56:39.437-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 27 --threads 4 --no-mmap --parallel 1 --port 63875"
time=2025-08-12T17:56:39.443-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T17:56:39.443-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T17:56:39.444-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T17:56:39.514-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T17:56:39.612-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T17:56:39.613-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:63875"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-12T17:56:39.696-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 27 repeating layers to GPU
load_tensors: offloaded 27/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   568.58 MiB
load_tensors:        CUDA0 model buffer size =  3599.51 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   216.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =     8.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 18 (with bs=512), 3 (with bs=1)
time=2025-08-12T17:56:40.950-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.51 seconds"
[GIN] 2025/08/12 - 17:56:40 | 200 |    2.5220488s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 17:56:56 | 200 |    2.1742439s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/12 - 17:58:01 | 200 |    39.884576s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/12 - 17:59:59 | 200 |   23.8304203s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/12 - 18:01:05 | 200 |   40.2809492s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/12 - 18:06:19 | 400 |       504.6µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:19 | 200 |      3.8666ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:06:22 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:22 | 200 |      4.3865ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:06:25 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:25 | 200 |       4.582ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:06:28 | 400 |       506.2µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:28 | 200 |      2.8259ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:06:31 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:31 | 200 |      4.7861ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:06:34 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:34 | 200 |      4.7755ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:06:37 | 400 |      1.6694ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:37 | 200 |       6.412ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:06:40 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:40 | 200 |      4.0422ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:06:43 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:43 | 200 |      3.9067ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:06:46 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:46 | 200 |      4.3498ms |       127.0.0.1 | GET      "/api/tags"
time=2025-08-12T18:06:48.023-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="60.2 GiB" free_swap="55.8 GiB"
time=2025-08-12T18:06:48.024-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=27 layers.split="" memory.available="[5.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.0 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T18:06:48.583-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 27 --threads 4 --no-mmap --parallel 1 --port 64314"
time=2025-08-12T18:06:48.590-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T18:06:48.590-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T18:06:48.591-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T18:06:48.657-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T18:06:48.752-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T18:06:48.752-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:64314"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-12T18:06:48.842-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 27 repeating layers to GPU
load_tensors: offloaded 27/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   568.58 MiB
load_tensors:        CUDA0 model buffer size =  3599.51 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
[GIN] 2025/08/12 - 18:06:49 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:49 | 200 |      4.5457ms |       127.0.0.1 | GET      "/api/tags"
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   216.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =     8.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 18 (with bs=512), 3 (with bs=1)
time=2025-08-12T18:06:50.346-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/12 - 18:06:52 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:52 | 200 |      2.5935ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:06:55 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:55 | 200 |      4.5127ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:06:58 | 400 |       507.3µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:06:58 | 200 |      3.3735ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:01 | 200 |   13.5037169s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/12 - 18:07:01 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:01 | 200 |      3.9026ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:04 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:04 | 200 |      3.7684ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:07 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:07 | 200 |      3.3357ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:10 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:10 | 200 |      4.3523ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:11 | 200 |            0s |       127.0.0.1 | HEAD     "/"
time=2025-08-12T18:07:11.617-04:00 level=INFO source=download.go:177 msg="downloading 970aa74c0a90 in 3 100 MB part(s)"
[GIN] 2025/08/12 - 18:07:13 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:13 | 200 |      7.2063ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:16 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:16 | 200 |      3.8686ms |       127.0.0.1 | GET      "/api/tags"
time=2025-08-12T18:07:17.996-04:00 level=INFO source=download.go:177 msg="downloading ce4a164fc046 in 1 17 B part(s)"
time=2025-08-12T18:07:19.177-04:00 level=INFO source=download.go:177 msg="downloading 31df23ea7daa in 1 420 B part(s)"
[GIN] 2025/08/12 - 18:07:19 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:19 | 200 |      5.4324ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:20 | 200 |    9.5758332s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/12 - 18:07:22 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:22 | 200 |      4.7259ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:25 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:25 | 200 |      6.5571ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:28 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:28 | 200 |      3.9948ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:31 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:31 | 200 |      4.8401ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:34 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:34 | 200 |      5.1675ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:37 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:37 | 200 |      4.9251ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:39 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 18:07:39 | 200 |      6.4264ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:40 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:40 | 200 |      3.6489ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:43 | 400 |       503.4µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:43 | 200 |      4.1481ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:46 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:46 | 200 |      5.3608ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:49 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:49 | 200 |      4.5305ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:52 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:52 | 200 |      4.6589ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:55 | 400 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:55 | 200 |      6.2367ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:07:57 | 200 |     65.0561ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:57 | 200 |     65.5729ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:57 | 200 |      67.353ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:57 | 200 |     84.5763ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:57 | 200 |     84.5763ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:57 | 200 |     85.1754ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:58 | 200 |    220.6841ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:58 | 200 |    255.5131ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:58 | 200 |    258.6546ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:58 | 200 |    192.7325ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:58 | 200 |    191.3714ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:07:58 | 200 |    187.9052ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:08:07 | 200 |     97.8991ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:08:07 | 200 |     99.7591ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:08:07 | 200 |    101.8716ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:08:39 | 200 |     89.3169ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:08:39 | 200 |     90.4619ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:08:39 | 200 |     93.1917ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:08:42 | 200 |    106.8743ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:08:42 | 200 |     108.381ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:08:42 | 200 |    113.8695ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-12T18:08:58.471-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="59.6 GiB" free_swap="55.1 GiB"
time=2025-08-12T18:08:58.472-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=11 layers.split="" memory.available="[5.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="8.7 GiB" memory.required.partial="5.0 GiB" memory.required.kv="1.8 GiB" memory.required.allocations="[5.0 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="1.8 GiB" memory.graph.partial="2.3 GiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T18:08:58.994-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 32768 --batch-size 512 --n-gpu-layers 11 --threads 4 --no-mmap --parallel 1 --port 64503"
time=2025-08-12T18:08:59.000-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T18:08:59.000-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T18:08:59.001-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T18:08:59.070-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T18:08:59.168-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T18:08:59.168-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:64503"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-12T18:08:59.253-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 11 repeating layers to GPU
load_tensors: offloaded 11/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =  2689.11 MiB
load_tensors:        CUDA0 model buffer size =  1478.98 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32768
llama_context: n_ctx_per_seq = 32768
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 32768, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   704.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =  1088.00 MiB
llama_kv_cache_unified: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB
llama_context:      CUDA0 compute buffer size =  1954.89 MiB
llama_context:  CUDA_Host compute buffer size =    71.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 242 (with bs=512), 3 (with bs=1)
time=2025-08-12T18:09:01.259-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.26 seconds"
[GIN] 2025/08/12 - 18:09:28 | 200 |    31.121493s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/12 - 18:09:31 | 200 |    2.6784391s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/12 - 18:09:35 | 200 |    1.6222542s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 18:09:36 | 200 |    915.0595ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 18:09:38 | 200 |    1.7494475s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 18:10:28 | 200 |   31.2986538s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/12 - 18:34:48 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 18:34:48 | 200 |      3.1339ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:34:54 | 200 |      4.7074ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:35:06 | 200 |      3.9969ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:35:06 | 200 |      8.7925ms |             ::1 | GET      "/api/tags"
time=2025-08-12T18:35:27.753-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="55.0 GiB" free_swap="48.8 GiB"
time=2025-08-12T18:35:27.754-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=23 layers.split="" memory.available="[4.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.5 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.5 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T18:35:28.298-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 23 --threads 4 --no-mmap --parallel 1 --port 63565"
time=2025-08-12T18:35:28.307-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T18:35:28.307-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T18:35:28.308-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T18:35:28.395-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T18:35:28.490-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T18:35:28.491-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:63565"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-12T18:35:28.559-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 23/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =  1103.11 MiB
load_tensors:        CUDA0 model buffer size =  3064.98 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   184.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    40.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 74 (with bs=512), 3 (with bs=1)
time=2025-08-12T18:35:30.064-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/12 - 18:35:36 | 200 |      3.2981ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:35:50 | 200 |   24.1193424s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 18:41:34 | 200 |     85.5052ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:41:34 | 200 |     88.3795ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:41:34 | 200 |    107.6325ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:42:30 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/08/12 - 18:42:30 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 18:42:30 | 200 |       6.912ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:42:31 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 18:42:31 | 200 |      3.5707ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:42:32 | 200 |    305.4973ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:42:32 | 200 |    305.0377ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:42:32 | 200 |    309.8016ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:42:32 | 200 |    213.6434ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:42:32 | 200 |     219.662ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:42:32 | 200 |    228.7343ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:42:32 | 200 |    216.7481ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:42:32 | 200 |    221.7681ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:42:32 | 200 |    231.5528ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 18:42:59 | 200 |      3.8009ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:42:59 | 200 |      7.3198ms |             ::1 | GET      "/api/tags"
time=2025-08-12T18:43:06.855-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="55.1 GiB" free_swap="48.8 GiB"
time=2025-08-12T18:43:06.855-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=23 layers.split="" memory.available="[4.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.5 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.5 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T18:43:07.261-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 23 --threads 4 --no-mmap --parallel 1 --port 64425"
time=2025-08-12T18:43:07.269-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T18:43:07.269-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T18:43:07.270-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T18:43:07.345-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T18:43:07.448-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T18:43:07.449-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:64425"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-12T18:43:07.521-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 23/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =  1103.11 MiB
load_tensors:        CUDA0 model buffer size =  3064.98 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   184.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    40.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 74 (with bs=512), 3 (with bs=1)
time=2025-08-12T18:43:10.027-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.76 seconds"
[GIN] 2025/08/12 - 18:43:13 | 200 |    6.6002332s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 18:43:29 | 200 |      7.1457ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:43:59 | 200 |      4.1473ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:44:29 | 200 |      4.9268ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:44:45 | 200 |   58.0879726s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 18:45:00 | 200 |      5.5487ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:45:30 | 200 |      4.4911ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:46:00 | 200 |      4.5127ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:46:30 | 200 |       3.032ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:46:59 | 200 |      4.4056ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:47:29 | 200 |      4.0105ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:47:51 | 200 |    1.5901771s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 18:47:59 | 200 |      3.6103ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:48:29 | 200 |      4.5236ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:48:59 | 200 |     14.7338ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:49:18 | 200 |          1m5s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 18:49:29 | 200 |      5.9137ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:49:34 | 200 |    8.7859768s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 18:49:59 | 200 |       4.971ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:50:29 | 200 |      5.5318ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:50:36 | 200 |   19.9023471s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 18:50:59 | 200 |      6.9001ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:51:29 | 200 |       4.392ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:51:59 | 200 |      5.5394ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:52:29 | 200 |      6.3851ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:52:59 | 200 |      4.8345ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:53:29 | 200 |      3.9251ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:53:34 | 200 |    7.8482236s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 18:53:59 | 200 |      5.6025ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:54:29 | 200 |      5.2909ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:54:59 | 200 |      3.6287ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:55:29 | 200 |      3.1557ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:55:59 | 200 |      4.0733ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:56:29 | 200 |      4.8232ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:56:59 | 200 |      3.4154ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:57:29 | 200 |      3.2702ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:57:59 | 200 |      5.0252ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:58:29 | 200 |      4.6962ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:58:59 | 200 |      3.9856ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:59:29 | 200 |      4.7874ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 18:59:59 | 200 |      4.8828ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:00:29 | 200 |      5.5774ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:00:59 | 200 |      4.2869ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:01:29 | 200 |      4.1835ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:01:59 | 200 |      2.7944ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:02:29 | 200 |      4.5562ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:02:59 | 200 |      3.2321ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:03:29 | 200 |      4.8029ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:03:59 | 200 |      5.3116ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:04:29 | 200 |      3.6541ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:04:59 | 200 |      5.5525ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:05:29 | 200 |       4.553ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:05:59 | 200 |      5.2248ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:06:29 | 200 |      4.5487ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:06:59 | 200 |      3.9393ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:07:29 | 200 |      5.3937ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:07:59 | 200 |      4.8537ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:08:29 | 200 |      5.2966ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:08:59 | 200 |      5.2465ms |             ::1 | GET      "/api/tags"
time=2025-08-12T19:09:22.875-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="55.6 GiB" free_swap="49.3 GiB"
time=2025-08-12T19:09:22.875-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=23 layers.split="" memory.available="[4.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.5 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.5 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T19:09:23.370-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 23 --threads 4 --no-mmap --parallel 1 --port 49173"
time=2025-08-12T19:09:23.377-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T19:09:23.377-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T19:09:23.378-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T19:09:23.449-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T19:09:23.547-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T19:09:23.548-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:49173"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-12T19:09:23.629-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 23/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =  1103.11 MiB
load_tensors:        CUDA0 model buffer size =  3064.98 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   184.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    40.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 74 (with bs=512), 3 (with bs=1)
time=2025-08-12T19:09:25.133-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/12 - 19:09:28 | 200 |    5.5046018s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 19:09:29 | 200 |      4.3304ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:09:59 | 200 |       4.259ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:10:10 | 200 |    4.8893268s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 19:10:29 | 200 |      2.7395ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:10:40 | 200 |   13.5636414s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 19:10:59 | 200 |      3.9016ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:11:29 | 200 |      4.8543ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:11:59 | 200 |       3.123ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:12:29 | 200 |      4.0141ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:12:31 | 200 |    3.2754113s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 19:12:59 | 200 |      5.4355ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:13:30 | 200 |      3.6586ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:14:00 | 200 |      4.4122ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:14:45 | 200 |      6.6085ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:15:45 | 200 |      4.4391ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:16:45 | 200 |      4.1503ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:17:45 | 200 |      3.7606ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:18:45 | 200 |       3.852ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:19:45 | 200 |      3.4849ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:20:45 | 200 |      3.5532ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:21:45 | 200 |      3.1102ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:22:45 | 200 |      3.2206ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:23:45 | 200 |      3.2831ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:24:45 | 200 |      6.1431ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:25:45 | 200 |      4.9303ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:26:45 | 200 |      4.8758ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:27:45 | 200 |      6.1495ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:28:45 | 200 |      5.9924ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:29:45 | 200 |      2.9474ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:30:45 | 200 |       5.621ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:31:45 | 200 |      4.1224ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:32:45 | 200 |      5.9992ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:33:45 | 200 |      4.9376ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:34:45 | 200 |      4.6743ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:35:45 | 200 |      3.9219ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:36:45 | 200 |      3.9776ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:37:45 | 200 |      5.1995ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:38:45 | 200 |      5.8389ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:39:45 | 200 |      4.1151ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:40:45 | 200 |      3.3125ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:41:45 | 200 |      3.6623ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:42:45 | 200 |      3.0825ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:43:45 | 200 |      3.5344ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:44:45 | 200 |      5.7906ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:45:45 | 200 |      5.6612ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:46:45 | 200 |      2.9871ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:47:45 | 200 |      4.6587ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:48:45 | 200 |      3.3512ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:49:45 | 200 |      3.5373ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:50:45 | 200 |      2.7674ms |             ::1 | GET      "/api/tags"
time=2025-08-12T19:51:27.021-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="55.0 GiB" free_swap="48.7 GiB"
time=2025-08-12T19:51:27.022-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=24 layers.split="" memory.available="[4.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.6 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.6 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T19:51:27.604-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 24 --threads 4 --no-mmap --parallel 1 --port 51507"
time=2025-08-12T19:51:27.613-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T19:51:27.613-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T19:51:27.613-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T19:51:27.688-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T19:51:27.792-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T19:51:27.792-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:51507"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-12T19:51:27.864-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloaded 24/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   978.05 MiB
load_tensors:        CUDA0 model buffer size =  3190.04 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   192.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    32.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 60 (with bs=512), 3 (with bs=1)
time=2025-08-12T19:51:29.367-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.75 seconds"
[GIN] 2025/08/12 - 19:51:29 | 200 |    2.4558519s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 19:51:29 | 200 |     39.4488ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 19:51:45 | 200 |      3.1509ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:52:18 | 200 |     40.9972ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 19:52:18 | 200 |     38.3134ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 19:52:45 | 200 |      4.9287ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:53:34 | 200 |     40.9994ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 19:53:34 | 200 |     34.8722ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 19:53:45 | 200 |      4.0425ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:54:33 | 200 |     36.8724ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 19:54:33 | 200 |     28.4493ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 19:54:45 | 200 |      3.3971ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:55:45 | 200 |      5.6738ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:56:45 | 200 |      3.5751ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:57:45 | 200 |      7.7064ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:58:45 | 200 |      4.9771ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 19:59:45 | 200 |       4.654ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:00:45 | 200 |      3.7573ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:01:45 | 200 |      4.8046ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:02:45 | 200 |      4.4199ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:03:45 | 200 |      4.3097ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:04:45 | 200 |      4.9051ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:05:45 | 200 |      4.8073ms |             ::1 | GET      "/api/tags"
time=2025-08-12T20:05:57.878-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="49.0 GiB" free_swap="36.4 GiB"
time=2025-08-12T20:05:57.883-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=24 layers.split="" memory.available="[4.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.6 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.6 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T20:05:58.448-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 24 --threads 4 --no-mmap --parallel 1 --port 52186"
time=2025-08-12T20:05:58.457-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T20:05:58.457-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T20:05:58.457-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T20:05:58.606-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T20:05:58.714-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T20:05:58.715-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:52186"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
time=2025-08-12T20:05:58.960-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloaded 24/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   978.05 MiB
load_tensors:        CUDA0 model buffer size =  3190.04 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   192.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    32.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 60 (with bs=512), 3 (with bs=1)
time=2025-08-12T20:06:00.463-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.01 seconds"
[GIN] 2025/08/12 - 20:06:00 | 200 |    2.7327262s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 20:06:32 | 200 |     38.5227ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 20:06:32 | 200 |     32.6557ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 20:06:45 | 200 |      3.3115ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:07:45 | 200 |      4.1417ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:08:45 | 200 |      2.7915ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:09:45 | 200 |      3.9404ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:10:45 | 200 |      3.0001ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:11:45 | 200 |      4.5782ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:12:45 | 200 |      7.1373ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:13:45 | 200 |      4.6775ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:14:45 | 200 |      8.2641ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:15:45 | 200 |      4.2721ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:16:45 | 200 |       9.178ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:17:45 | 200 |      5.5772ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:18:45 | 200 |      3.7408ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:18:59 | 200 |      5.3282ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:19:30 | 200 |      5.1969ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:20:00 | 200 |      3.5084ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:20:45 | 200 |      3.8886ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:21:45 | 200 |      4.2091ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:22:45 | 200 |      5.4116ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:23:45 | 200 |      4.5823ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:24:45 | 200 |      4.6797ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:25:45 | 200 |      3.9317ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:26:45 | 200 |      4.8972ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:27:45 | 200 |      4.5032ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:28:45 | 200 |      2.9382ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:29:45 | 200 |      3.7157ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:30:45 | 200 |      5.5908ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:31:45 | 200 |       3.936ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:32:45 | 200 |      3.2279ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:33:45 | 200 |      5.0081ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:34:41 | 200 |      4.3269ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:34:59 | 200 |      4.7751ms |             ::1 | GET      "/api/tags"
time=2025-08-12T20:35:01.112-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="60.6 GiB" free_swap="47.9 GiB"
time=2025-08-12T20:35:01.113-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=24 layers.split="" memory.available="[4.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.6 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.6 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T20:35:01.642-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 24 --threads 4 --no-mmap --parallel 1 --port 53767"
time=2025-08-12T20:35:01.649-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T20:35:01.649-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T20:35:01.650-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T20:35:01.720-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T20:35:01.821-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T20:35:01.822-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:53767"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-12T20:35:01.901-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloaded 24/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   978.05 MiB
load_tensors:        CUDA0 model buffer size =  3190.04 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   192.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    32.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 60 (with bs=512), 3 (with bs=1)
time=2025-08-12T20:35:03.654-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.01 seconds"
[GIN] 2025/08/12 - 20:35:09 | 200 |    8.8790031s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 20:35:29 | 200 |       4.321ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:35:59 | 200 |      3.5109ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:36:29 | 200 |      3.9836ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:37:00 | 200 |      4.4848ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:37:30 | 200 |      5.1903ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:38:31 | 200 |      4.8421ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:39:32 | 200 |      4.6425ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:40:00 | 200 |      5.4696ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:40:33 | 200 |      5.0814ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:41:34 | 200 |       6.021ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:42:35 | 200 |      3.5153ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:43:36 | 200 |      4.9908ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:44:37 | 200 |      3.3197ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:45:38 | 200 |      5.8415ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:46:39 | 200 |      5.1492ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:47:40 | 200 |      3.4852ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:48:41 | 200 |      4.7507ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:49:42 | 200 |      4.6362ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:50:43 | 200 |      2.8815ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:51:44 | 200 |        6.59ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:52:45 | 200 |      3.6027ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:53:45 | 200 |      3.0544ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:54:45 | 200 |      3.9731ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:55:45 | 200 |      6.4136ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:56:45 | 200 |      3.8129ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:57:45 | 200 |      3.4962ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:58:45 | 200 |      4.1019ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 20:59:45 | 200 |      4.2109ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:00:45 | 200 |      3.6713ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:01:45 | 200 |      4.8525ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:02:45 | 200 |      3.2811ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:03:05 | 200 |      3.6996ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:03:05 | 200 |      5.2965ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:03:28 | 200 |      4.2286ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:03:28 | 200 |      5.5667ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:03:45 | 200 |      4.9876ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:03:58 | 200 |      6.5261ms |             ::1 | GET      "/api/tags"
time=2025-08-12T21:04:00.121-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="59.1 GiB" free_swap="47.5 GiB"
time=2025-08-12T21:04:00.124-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=24 layers.split="" memory.available="[4.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.6 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.6 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T21:04:00.659-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 24 --threads 4 --no-mmap --parallel 1 --port 55002"
time=2025-08-12T21:04:00.668-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T21:04:00.668-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T21:04:00.669-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T21:04:00.769-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T21:04:00.873-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T21:04:00.874-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:55002"
time=2025-08-12T21:04:00.920-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloaded 24/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   978.05 MiB
load_tensors:        CUDA0 model buffer size =  3190.04 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   192.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    32.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 60 (with bs=512), 3 (with bs=1)
time=2025-08-12T21:04:02.674-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.01 seconds"
[GIN] 2025/08/12 - 21:04:15 | 200 |   15.9598377s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 21:04:28 | 200 |      5.1352ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:04:45 | 200 |      5.3754ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:04:58 | 200 |       3.561ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:05:08 | 200 |   21.9134823s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 21:05:28 | 200 |      3.1404ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:05:45 | 200 |      4.1607ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:05:58 | 200 |      4.3149ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:06:28 | 200 |       3.624ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:06:45 | 200 |      6.6426ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:06:47 | 200 |   16.3630246s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 21:06:58 | 200 |      4.9423ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:07:29 | 200 |      4.2616ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:07:45 | 200 |      4.7363ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:07:58 | 200 |       4.544ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:08:28 | 200 |      3.9003ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:08:45 | 200 |      5.7911ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:08:59 | 200 |      4.0074ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:11:11 | 200 |      4.6915ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:11:11 | 200 |      3.5104ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:11:21 | 200 |       5.424ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:11:28 | 200 |      4.8653ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:12:12 | 200 |      4.3445ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:12:12 | 200 |       3.669ms |             ::1 | GET      "/api/tags"
time=2025-08-12T21:12:41.993-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="59.5 GiB" free_swap="48.1 GiB"
time=2025-08-12T21:12:41.994-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=23 layers.split="" memory.available="[4.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.5 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.5 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T21:12:42.523-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 23 --threads 4 --no-mmap --parallel 1 --port 55457"
time=2025-08-12T21:12:42.530-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T21:12:42.530-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T21:12:42.531-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
[GIN] 2025/08/12 - 21:12:42 | 200 |      5.5356ms |             ::1 | GET      "/api/tags"
time=2025-08-12T21:12:42.619-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T21:12:42.721-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T21:12:42.722-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:55457"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-12T21:12:42.782-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 23/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =  1103.11 MiB
load_tensors:        CUDA0 model buffer size =  3064.98 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   184.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    40.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 74 (with bs=512), 3 (with bs=1)
time=2025-08-12T21:12:44.535-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.01 seconds"
[GIN] 2025/08/12 - 21:13:02 | 200 |   20.6625377s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 21:13:12 | 200 |        3.08ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:13:38 | 200 |    4.9174551s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 21:13:42 | 200 |      6.0969ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:14:13 | 200 |      4.7584ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:14:42 | 200 |      4.6658ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:15:05 | 200 |    3.7552542s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 21:15:12 | 200 |      4.6032ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:15:38 | 200 |    2.4770199s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/12 - 21:15:42 | 200 |      5.7247ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:16:13 | 200 |      5.5553ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:16:43 | 200 |      3.9713ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:17:44 | 200 |      3.7346ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:18:45 | 200 |      3.6255ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:19:45 | 200 |      5.0773ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:20:45 | 200 |       3.677ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:21:45 | 200 |      2.6603ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:22:45 | 200 |      4.6518ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:23:45 | 200 |      3.0136ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:24:45 | 200 |      6.3252ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:25:45 | 200 |      4.6202ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:26:45 | 200 |      4.4019ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:27:45 | 200 |      5.1174ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:28:45 | 200 |      4.4627ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:29:45 | 200 |      3.1752ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:30:45 | 200 |       3.634ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:31:45 | 200 |       3.528ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:32:45 | 200 |      2.9337ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:33:45 | 200 |      7.4327ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:34:45 | 200 |      5.8652ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:35:45 | 200 |      5.8301ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:36:45 | 200 |      3.7007ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:37:45 | 200 |      2.7617ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:38:45 | 200 |      4.7306ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:39:45 | 200 |      3.4408ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:40:45 | 200 |      4.6393ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:41:45 | 200 |      3.6416ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:42:45 | 200 |      4.2875ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:43:45 | 200 |      4.5864ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:44:45 | 200 |      8.2463ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:45:45 | 200 |      3.4105ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:46:45 | 200 |      2.8942ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:47:45 | 200 |      3.4724ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:48:45 | 200 |      4.8107ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:49:45 | 200 |       4.976ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:50:45 | 200 |      3.9296ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:51:45 | 200 |      4.7503ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:52:45 | 200 |      3.9766ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:53:45 | 200 |      4.4189ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:54:45 | 200 |      4.3428ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:55:45 | 200 |      4.1276ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:56:45 | 200 |      3.4582ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:57:45 | 200 |      3.9947ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:58:45 | 200 |      3.3235ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 21:59:45 | 200 |      2.9904ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:00:45 | 200 |      6.4468ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:01:45 | 200 |       5.305ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:02:45 | 200 |      1.9166ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:03:45 | 200 |      4.0664ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:04:45 | 200 |      6.0212ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:05:45 | 200 |      4.4799ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:06:45 | 200 |      4.0344ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:07:45 | 200 |      5.9894ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:08:45 | 200 |      3.9689ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:09:45 | 200 |       4.735ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:10:45 | 200 |      3.3491ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:11:45 | 200 |      5.7049ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:12:45 | 200 |      3.4551ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:13:45 | 200 |      8.5075ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:14:45 | 200 |       6.855ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:15:45 | 200 |      3.3881ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:16:45 | 200 |      4.4433ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:17:45 | 200 |      6.2669ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:18:45 | 200 |      5.3882ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:19:45 | 200 |       7.188ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:20:45 | 200 |      5.3254ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:21:45 | 200 |      6.7534ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:22:45 | 200 |     10.7969ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:23:45 | 200 |      3.4718ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:23:51 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:23:51 | 200 |      9.0555ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:23:51 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:23:51 | 200 |     67.2802ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-12T22:23:51.752-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="56.5 GiB" free_swap="45.6 GiB"
time=2025-08-12T22:23:51.753-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=24 layers.split="" memory.available="[4.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.6 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.6 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T22:23:52.287-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 24 --threads 4 --no-mmap --parallel 1 --port 58705"
time=2025-08-12T22:23:52.293-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T22:23:52.293-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T22:23:52.294-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T22:23:52.373-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T22:23:52.490-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T22:23:52.491-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:58705"
time=2025-08-12T22:23:52.545-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloaded 24/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   978.05 MiB
load_tensors:        CUDA0 model buffer size =  3190.04 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   192.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    32.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 60 (with bs=512), 3 (with bs=1)
time=2025-08-12T22:23:55.051-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.76 seconds"
[GIN] 2025/08/12 - 22:24:06 | 200 |   14.7245223s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:24:45 | 200 |      3.8506ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:25:45 | 200 |      2.4032ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:26:45 | 200 |      4.6449ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:27:45 | 200 |      4.4447ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:28:45 | 200 |      4.0305ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:29:45 | 200 |      3.2527ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:30:45 | 200 |      4.5242ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:31:45 | 200 |      5.6142ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:32:45 | 200 |       5.854ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:33:45 | 200 |      2.7802ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:34:45 | 200 |      4.7429ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:35:45 | 200 |      5.8152ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:36:45 | 200 |      4.3144ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:37:45 | 200 |      3.6935ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:38:45 | 200 |       4.927ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:39:45 | 200 |       3.476ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:40:45 | 200 |       6.335ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:41:45 | 200 |      4.5848ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:42:45 | 200 |      5.4358ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:43:45 | 200 |      4.8624ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:44:45 | 200 |      5.2991ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:45:45 | 200 |      5.0092ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:46:45 | 200 |      3.5902ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:47:45 | 200 |       7.646ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:48:45 | 200 |      5.3699ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:49:45 | 200 |      4.1232ms |             ::1 | GET      "/api/tags"
time=2025-08-12T22:50:11.994-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="55.9 GiB" free_swap="45.2 GiB"
time=2025-08-12T22:50:11.995-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=8 layers.split="" memory.available="[4.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="8.7 GiB" memory.required.partial="4.5 GiB" memory.required.kv="1.8 GiB" memory.required.allocations="[4.5 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="1.8 GiB" memory.graph.partial="2.3 GiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T22:50:12.541-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 32768 --batch-size 512 --n-gpu-layers 8 --threads 4 --no-mmap --parallel 1 --port 59980"
time=2025-08-12T22:50:12.547-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T22:50:12.548-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T22:50:12.548-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T22:50:12.617-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T22:50:12.726-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T22:50:12.727-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:59980"
time=2025-08-12T22:50:12.800-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 8 repeating layers to GPU
load_tensors: offloaded 8/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =  3064.74 MiB
load_tensors:        CUDA0 model buffer size =  1103.35 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32768
llama_context: n_ctx_per_seq = 32768
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 32768, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB
llama_context:      CUDA0 compute buffer size =  1954.89 MiB
llama_context:  CUDA_Host compute buffer size =    71.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 284 (with bs=512), 3 (with bs=1)
time=2025-08-12T22:50:15.304-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.76 seconds"
[GIN] 2025/08/12 - 22:50:19 | 200 |    7.2665053s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:50:45 | 200 |      3.4149ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:51:45 | 200 |      4.9273ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:52:45 | 200 |     11.0898ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:53:45 | 200 |       4.044ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:54:42 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:54:42 | 200 |     67.6346ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-12T22:54:43.296-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="56.1 GiB" free_swap="45.3 GiB"
time=2025-08-12T22:54:43.298-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=24 layers.split="" memory.available="[4.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.6 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.6 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-12T22:54:43.835-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 24 --threads 4 --no-mmap --parallel 1 --port 60171"
time=2025-08-12T22:54:43.841-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-12T22:54:43.841-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-12T22:54:43.842-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-12T22:54:43.912-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-12T22:54:44.011-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-12T22:54:44.011-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:60171"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-12T22:54:44.093-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloaded 24/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   978.05 MiB
load_tensors:        CUDA0 model buffer size =  3190.04 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
[GIN] 2025/08/12 - 22:54:45 | 200 |      6.1483ms |             ::1 | GET      "/api/tags"
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   192.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    32.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 60 (with bs=512), 3 (with bs=1)
time=2025-08-12T22:54:46.852-04:00 level=INFO source=server.go:630 msg="llama runner started in 3.01 seconds"
[GIN] 2025/08/12 - 22:54:49 | 200 |    7.5144536s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:54:49 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:54:50 | 200 |     76.0602ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:54:58 | 200 |    8.3689172s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:54:58 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:54:58 | 200 |     61.2453ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:55:04 | 200 |    6.0204937s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:55:04 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:55:04 | 200 |     73.0976ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:55:10 | 200 |    6.0193748s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:55:11 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:55:11 | 200 |     68.1316ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:55:19 | 200 |     8.614739s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:55:19 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:55:19 | 200 |     74.6876ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:55:27 | 200 |    7.5191985s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:55:27 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:55:27 | 200 |     76.2854ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:55:36 | 200 |    8.7943239s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:55:36 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:55:36 | 200 |     73.9749ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:55:45 | 200 |      2.6621ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:55:47 | 200 |   11.1302885s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:55:47 | 200 |       669.8µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:55:48 | 200 |     73.7531ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:55:57 | 200 |    9.0320439s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:55:57 | 200 |       635.6µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:55:57 | 200 |     74.1068ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:56:07 | 200 |   10.7116223s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:56:08 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:56:08 | 200 |     76.6866ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:56:11 | 200 |    3.6700082s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:56:12 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:56:12 | 200 |     75.5393ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:56:17 | 200 |    5.5279596s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:56:17 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:56:17 | 200 |     64.1543ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:56:29 | 200 |   11.2849168s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:56:29 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:56:29 | 200 |     76.1422ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:56:35 | 200 |    5.9338887s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:56:35 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:56:35 | 200 |     73.0611ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:56:41 | 200 |    5.8259044s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:56:41 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:56:41 | 200 |     72.4113ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:56:45 | 200 |    3.3052101s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:56:45 | 200 |       4.356ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:56:45 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:56:45 | 200 |     71.0011ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:56:51 | 200 |    5.7331681s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:56:51 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:56:51 | 200 |     76.7703ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:57:05 | 200 |   14.3755872s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:57:06 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:57:06 | 200 |     77.6765ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:57:19 | 200 |    13.202832s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:57:19 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 22:57:19 | 200 |     74.4085ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 22:57:28 | 200 |     8.953096s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 22:57:45 | 200 |      3.4344ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:58:45 | 200 |      3.7769ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 22:59:45 | 200 |      4.7914ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:00:43 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 23:00:44 | 200 |     47.3563ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 23:00:44 | 200 |    849.5912ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 23:00:45 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 23:00:45 | 200 |     67.3823ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 23:00:45 | 200 |      4.3598ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:00:49 | 200 |      4.03106s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 23:01:31 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 23:01:31 | 200 |     61.9467ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 23:01:45 | 200 |       5.586ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:01:54 | 200 |   22.8465421s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 23:01:54 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 23:01:54 | 200 |     62.6029ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 23:01:59 | 200 |    4.5566897s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 23:02:45 | 200 |      4.7695ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:03:07 | 200 |       529.9µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 23:03:07 | 200 |     43.4735ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 23:03:20 | 200 |   12.7556111s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 23:03:20 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 23:03:20 | 200 |     55.7521ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 23:03:37 | 200 |   16.5313351s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 23:03:37 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 23:03:37 | 200 |     52.2493ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 23:03:45 | 200 |      2.5641ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:03:55 | 200 |   18.3177722s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 23:04:45 | 200 |      5.4478ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:05:45 | 200 |      5.6849ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:06:01 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 23:06:01 | 200 |      57.515ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 23:06:10 | 200 |    8.9855657s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 23:06:10 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 23:06:10 | 200 |      62.166ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 23:06:19 | 200 |      9.02595s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 23:06:19 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 23:06:19 | 200 |     53.3115ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 23:06:27 | 200 |    7.7054225s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 23:06:27 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 23:06:27 | 200 |     73.0416ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 23:06:35 | 200 |    7.8471654s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 23:06:35 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 23:06:35 | 200 |     57.7384ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 23:06:45 | 200 |      3.7644ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:06:46 | 200 |   10.8567404s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 23:06:46 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/12 - 23:06:46 | 200 |     75.2268ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/12 - 23:06:49 | 200 |    2.8035526s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/12 - 23:07:45 | 200 |      4.6126ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:08:45 | 200 |      2.7743ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:09:45 | 200 |       5.737ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:10:45 | 200 |      5.2052ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:11:45 | 200 |      4.3617ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:12:45 | 200 |       4.642ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:13:45 | 200 |      5.4602ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:14:45 | 200 |       5.224ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:15:45 | 200 |      8.2979ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:16:45 | 200 |      5.5798ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:17:45 | 200 |      3.4912ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:18:45 | 200 |      4.0421ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:19:45 | 200 |      2.7095ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:20:45 | 200 |      5.5383ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:21:45 | 200 |      4.0144ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:22:45 | 200 |      3.5885ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:23:45 | 200 |      2.8341ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:24:45 | 200 |      5.3013ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:25:45 | 200 |      4.6887ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:26:45 | 200 |      3.8368ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:27:45 | 200 |      5.8748ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:28:45 | 200 |      4.4741ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:29:45 | 200 |      5.8998ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:30:45 | 200 |      4.1233ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:31:45 | 200 |      2.7835ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:32:45 | 200 |      3.9579ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:33:45 | 200 |      3.5423ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:34:45 | 200 |      3.3857ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:35:45 | 200 |      4.8019ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:36:45 | 200 |      5.4134ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:37:45 | 200 |       3.458ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:38:45 | 200 |      3.5653ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:39:45 | 200 |      6.8566ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:40:45 | 200 |      3.8963ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:41:45 | 200 |      4.8563ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:42:45 | 200 |      3.6242ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:43:45 | 200 |      4.9123ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:44:45 | 200 |       6.246ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:45:45 | 200 |        7.63ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:46:45 | 200 |      3.0052ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:47:45 | 200 |      2.6613ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:48:45 | 200 |      4.5748ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:49:45 | 200 |      4.4144ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:50:45 | 200 |      5.6495ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:51:45 | 200 |      6.6015ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:52:45 | 200 |      7.8123ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:53:45 | 200 |      4.9878ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:54:45 | 200 |      4.9777ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:55:45 | 200 |      4.5149ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:56:45 | 200 |      7.4733ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:57:45 | 200 |      5.9241ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:58:45 | 200 |     13.6029ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/12 - 23:59:45 | 200 |       9.808ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:00:45 | 200 |      4.7684ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:01:45 | 200 |      3.2264ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:02:45 | 200 |      6.5208ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:03:45 | 200 |      6.4135ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:04:45 | 200 |      4.2559ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:05:45 | 200 |      3.0193ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:06:45 | 200 |      3.9797ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:07:45 | 200 |      5.3942ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:08:45 | 200 |       4.893ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:09:45 | 200 |      4.9581ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:10:45 | 200 |     59.3162ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:11:45 | 200 |      5.9985ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:12:45 | 200 |      4.7977ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:13:45 | 200 |      3.4599ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:14:45 | 200 |      7.6551ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:15:45 | 200 |      3.1704ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:16:45 | 200 |      5.8054ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:17:45 | 200 |      4.9006ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:18:45 | 200 |       3.529ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:19:45 | 200 |      3.6177ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:20:45 | 200 |      5.6316ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:21:45 | 200 |      3.7576ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:22:45 | 200 |      4.2712ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:23:45 | 200 |      3.7328ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:24:45 | 200 |      6.0504ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:25:45 | 200 |      6.5924ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:26:45 | 200 |      4.4244ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:27:45 | 200 |      5.2764ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:28:45 | 200 |      4.1376ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:29:45 | 200 |      4.1247ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:30:45 | 200 |       4.535ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:31:45 | 200 |      4.0288ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:32:45 | 200 |      4.9429ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:33:45 | 200 |      2.8492ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:34:45 | 200 |      8.3681ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:35:45 | 200 |      3.8816ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:36:45 | 200 |      4.7409ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:37:45 | 200 |      2.8609ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:38:45 | 200 |      2.9373ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:39:45 | 200 |      3.5814ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:40:45 | 200 |      5.9891ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:41:45 | 200 |      2.9584ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:42:45 | 200 |      4.5523ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:43:45 | 200 |       4.434ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:44:45 | 200 |       8.229ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:45:45 | 200 |      3.1639ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:46:45 | 200 |      3.9732ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:47:45 | 200 |      4.8754ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:48:45 | 200 |      7.1854ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:49:45 | 200 |      3.6003ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:50:45 | 200 |      7.2529ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:51:45 | 200 |       4.549ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:52:45 | 200 |      3.5152ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:53:45 | 200 |      5.0294ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:54:45 | 200 |      5.9884ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:55:45 | 200 |      5.0622ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:56:45 | 200 |      4.4143ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:57:45 | 200 |       5.488ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:58:45 | 200 |      3.3861ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 00:59:45 | 200 |      7.6558ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:00:45 | 200 |      4.5886ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:01:45 | 200 |      5.2122ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:02:45 | 200 |      4.0931ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:03:45 | 200 |      4.3728ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:04:45 | 200 |      4.2209ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:05:45 | 200 |      6.0533ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:06:45 | 200 |      4.7385ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:07:45 | 200 |      3.9028ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:08:45 | 200 |      4.4296ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:09:45 | 200 |      5.7471ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:10:45 | 200 |      4.6487ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:11:45 | 200 |      5.4933ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:12:45 | 200 |      4.2486ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:13:45 | 200 |      4.5239ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:14:45 | 200 |      5.1208ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:15:45 | 200 |      5.3121ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:16:45 | 200 |      4.2067ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:17:45 | 200 |      3.9495ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:18:45 | 200 |      5.4513ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:19:45 | 200 |      7.3841ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:20:45 | 200 |      3.2597ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:21:45 | 200 |      4.9854ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:22:45 | 200 |      3.0867ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:23:45 | 200 |      3.6931ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:24:45 | 200 |      4.7942ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:25:45 | 200 |      5.5969ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:26:45 | 200 |      5.3631ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:27:45 | 200 |      5.4351ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:28:45 | 200 |        2.57ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:29:45 | 200 |      4.6987ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:30:45 | 200 |      4.5167ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:31:45 | 200 |      3.3617ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:32:45 | 200 |      3.3957ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:33:45 | 200 |      3.6067ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:34:45 | 200 |      5.5551ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:35:45 | 200 |       4.509ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:36:45 | 200 |      3.4079ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:37:45 | 200 |      5.6486ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:38:45 | 200 |      4.1989ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:39:45 | 200 |      5.7651ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:40:45 | 200 |      4.9393ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:41:45 | 200 |      4.5218ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:42:45 | 200 |      4.7748ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:43:45 | 200 |      4.4233ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:44:45 | 200 |      6.7903ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:45:45 | 200 |      5.5954ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:46:45 | 200 |      4.8533ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:47:45 | 200 |      4.5295ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:48:45 | 200 |      4.9759ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:49:45 | 200 |      3.5542ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:50:45 | 200 |      6.9074ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:51:45 | 200 |       5.102ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:52:45 | 200 |      4.2026ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:53:45 | 200 |        3.28ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:54:45 | 200 |      6.2476ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:55:45 | 200 |      5.8042ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:56:45 | 200 |      3.9051ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:57:45 | 200 |      4.5656ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:58:45 | 200 |      4.1559ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 01:59:45 | 200 |      4.7787ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:00:45 | 200 |      5.0406ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:01:45 | 200 |      4.2358ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:02:45 | 200 |      4.7957ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:03:45 | 200 |      3.9626ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:04:45 | 200 |      2.8225ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:05:45 | 200 |      3.3356ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:06:45 | 200 |      4.3678ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:07:45 | 200 |      4.7014ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:08:45 | 200 |      6.8926ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:09:45 | 200 |      4.5255ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:10:45 | 200 |       4.122ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:11:45 | 200 |      3.4025ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:12:45 | 200 |      4.8041ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:13:45 | 200 |      4.0703ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:14:45 | 200 |      4.8995ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:15:45 | 200 |      4.8053ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:16:45 | 200 |      4.8125ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:17:45 | 200 |      3.6181ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:18:45 | 200 |      5.0049ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:19:45 | 200 |      4.1749ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:20:45 | 200 |       4.526ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:21:45 | 200 |      4.3408ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:22:45 | 200 |      6.0515ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:23:45 | 200 |      4.5708ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:24:45 | 200 |      4.5946ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:25:45 | 200 |      3.6837ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:26:45 | 200 |       3.984ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:27:45 | 200 |      4.2087ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:28:45 | 200 |      3.8002ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:29:45 | 200 |      5.5942ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:30:45 | 200 |      3.8397ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:31:45 | 200 |      6.7828ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:32:45 | 200 |      3.4995ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:33:45 | 200 |      3.5913ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:34:45 | 200 |      6.9487ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:35:45 | 200 |      4.6227ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:36:45 | 200 |      3.3956ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:37:45 | 200 |      3.8795ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:38:45 | 200 |      4.3257ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:39:45 | 200 |      6.0207ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:40:45 | 200 |      4.7294ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:41:45 | 200 |      5.8916ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:42:45 | 200 |      4.2151ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:43:45 | 200 |      3.8369ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:44:45 | 200 |      5.0645ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:45:45 | 200 |      3.6223ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:46:45 | 200 |      4.4193ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:47:45 | 200 |      3.9211ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:48:45 | 200 |      4.9231ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:49:45 | 200 |      4.7778ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:50:45 | 200 |      4.3138ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:51:45 | 200 |      6.4483ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:52:45 | 200 |      3.4181ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:53:45 | 200 |       4.027ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:54:45 | 200 |      4.8234ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:55:45 | 200 |      4.1994ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:56:45 | 200 |       4.044ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:57:45 | 200 |      5.1255ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:58:45 | 200 |      5.2021ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 02:59:45 | 200 |      3.3201ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:00:45 | 200 |       4.834ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:01:45 | 200 |      4.5629ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:02:45 | 200 |      4.5616ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:03:45 | 200 |      2.7227ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:04:45 | 200 |       4.534ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:05:45 | 200 |      9.2754ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:06:45 | 200 |      3.9139ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:07:45 | 200 |      5.6936ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:08:45 | 200 |      4.3132ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:09:45 | 200 |      3.5135ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:10:45 | 200 |      4.0057ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:11:45 | 200 |      4.9776ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:12:45 | 200 |      5.0105ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:13:45 | 200 |      4.4351ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:14:45 | 200 |      4.7858ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:15:45 | 200 |      4.0229ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:16:45 | 200 |      4.9843ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:17:45 | 200 |      3.3297ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:18:45 | 200 |      2.7591ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:19:45 | 200 |      5.6516ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:20:45 | 200 |      4.2569ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:21:45 | 200 |      5.0724ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:22:45 | 200 |      4.4266ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:23:45 | 200 |      6.1938ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:24:45 | 200 |      4.7976ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:25:45 | 200 |      6.5354ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:26:45 | 200 |      4.6051ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:27:45 | 200 |      7.3783ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:28:45 | 200 |       3.717ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:29:45 | 200 |      4.0942ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:30:45 | 200 |      4.8959ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:31:45 | 200 |      3.6534ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:32:45 | 200 |      5.4758ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:33:45 | 200 |      3.7088ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:34:45 | 200 |      6.4055ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:35:45 | 200 |      4.2993ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:36:45 | 200 |      4.5196ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:37:45 | 200 |      3.5849ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:38:45 | 200 |      3.5861ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:39:45 | 200 |       4.422ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:40:45 | 200 |      3.7527ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:41:45 | 200 |      4.6345ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:42:45 | 200 |      3.2216ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:43:45 | 200 |      4.2681ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:44:45 | 200 |      4.3895ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:45:45 | 200 |      7.5933ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:46:45 | 200 |      4.7326ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:47:45 | 200 |      3.8083ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:48:45 | 200 |      5.6425ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:49:45 | 200 |      4.4648ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:50:45 | 200 |      4.6437ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:51:45 | 200 |      5.5532ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:52:45 | 200 |      5.8195ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:53:45 | 200 |      4.8063ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:54:45 | 200 |       5.898ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:55:45 | 200 |      6.1174ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:56:45 | 200 |      4.5161ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:57:45 | 200 |      4.7077ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:58:45 | 200 |      9.5771ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 03:59:45 | 200 |       5.316ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:00:45 | 200 |      4.1001ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:01:45 | 200 |      4.9386ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:02:45 | 200 |      4.8483ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:03:45 | 200 |      2.8698ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:04:45 | 200 |      7.4292ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:05:45 | 200 |      4.3133ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:06:45 | 200 |      5.3786ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:07:45 | 200 |       6.305ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:08:45 | 200 |      4.8435ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:09:45 | 200 |      5.7977ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:10:45 | 200 |      6.0157ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:11:45 | 200 |      6.9612ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:12:45 | 200 |      3.2423ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:13:45 | 200 |      5.7194ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:14:45 | 200 |      5.4004ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:15:45 | 200 |      4.0806ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:16:45 | 200 |      4.5997ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:17:45 | 200 |      6.4975ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:18:45 | 200 |      4.1846ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:19:45 | 200 |      3.4241ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:20:45 | 200 |      5.6794ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:21:45 | 200 |      5.7725ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:22:45 | 200 |      3.9971ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:23:45 | 200 |      4.9577ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:24:45 | 200 |      7.3345ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:25:45 | 200 |      4.4365ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:26:45 | 200 |      4.5094ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:27:45 | 200 |      4.1421ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:28:45 | 200 |      4.0322ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:29:45 | 200 |      5.2107ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:30:45 | 200 |      3.5375ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:31:45 | 200 |      4.1372ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:32:45 | 200 |      4.2764ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:33:45 | 200 |       3.476ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:34:45 | 200 |      7.0093ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:35:45 | 200 |      3.2561ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:36:45 | 200 |      3.9974ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:37:45 | 200 |      3.7756ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:38:45 | 200 |      4.9263ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:39:45 | 200 |      7.2108ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:40:45 | 200 |      5.1109ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:41:45 | 200 |      4.5788ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:42:45 | 200 |      5.0622ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:43:45 | 200 |      3.7645ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:44:45 | 200 |      6.4628ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:45:45 | 200 |      4.8069ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:46:45 | 200 |      4.9546ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:47:45 | 200 |      4.5588ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:48:45 | 200 |      2.7138ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:49:45 | 200 |      5.8561ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:50:45 | 200 |      4.5793ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:51:45 | 200 |      5.5517ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:52:45 | 200 |      6.6326ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:53:45 | 200 |      4.9005ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:54:45 | 200 |      5.9685ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:55:45 | 200 |      5.3117ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:56:45 | 200 |      8.0513ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:57:45 | 200 |      4.1779ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:58:45 | 200 |      4.4739ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 04:59:45 | 200 |      4.9755ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:00:45 | 200 |      3.0045ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:01:45 | 200 |      5.0475ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:02:45 | 200 |      5.7981ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:03:45 | 200 |      5.9937ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:04:45 | 200 |      5.7297ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:05:45 | 200 |      4.0832ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:06:45 | 200 |      3.3864ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:07:45 | 200 |      3.8702ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:08:45 | 200 |      3.4279ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:09:45 | 200 |      3.4235ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:10:45 | 200 |      3.7479ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:11:45 | 200 |       4.797ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:12:45 | 200 |      6.6764ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:13:45 | 200 |      3.8158ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:14:45 | 200 |      5.8475ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:15:45 | 200 |      4.2095ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:16:45 | 200 |       3.718ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:17:45 | 200 |      4.0501ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:18:45 | 200 |      3.4407ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:19:45 | 200 |      4.4494ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:20:45 | 200 |      6.3995ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:21:45 | 200 |       4.094ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:22:45 | 200 |      3.9898ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:23:45 | 200 |      6.1605ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:24:45 | 200 |      7.1882ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:25:45 | 200 |       4.052ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:26:45 | 200 |       4.128ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:27:45 | 200 |       5.689ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:28:45 | 200 |      4.5789ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:29:45 | 200 |        5.13ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:30:45 | 200 |       3.343ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:31:45 | 200 |      5.1976ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:32:45 | 200 |      5.3032ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:33:45 | 200 |      4.7451ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:34:45 | 200 |      6.6773ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:35:45 | 200 |      3.2256ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:36:45 | 200 |      6.3132ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:37:45 | 200 |      4.0269ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:38:45 | 200 |      3.9087ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:39:45 | 200 |      4.3741ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:40:45 | 200 |      3.2884ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:41:45 | 200 |      4.9736ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:42:45 | 200 |       8.214ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:43:45 | 200 |      3.7362ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:44:45 | 200 |      6.2544ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:45:45 | 200 |      4.2534ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:46:45 | 200 |      5.9165ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:47:45 | 200 |      4.1259ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:48:45 | 200 |      4.8662ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:49:45 | 200 |      4.4568ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:50:45 | 200 |      4.5263ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:51:45 | 200 |      3.3586ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:52:45 | 200 |      2.9657ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:53:45 | 200 |      5.0917ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:54:45 | 200 |      4.6599ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:55:45 | 200 |      7.0696ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:56:45 | 200 |      4.1169ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:57:45 | 200 |      4.1879ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:58:45 | 200 |      5.1779ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 05:59:45 | 200 |      4.9143ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:00:45 | 200 |      4.9471ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:01:45 | 200 |      3.9512ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:02:45 | 200 |      4.4181ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:03:45 | 200 |      5.4434ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:04:45 | 200 |      7.7814ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:05:45 | 200 |      4.9976ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:06:45 | 200 |      4.4148ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:07:45 | 200 |      4.2889ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:08:45 | 200 |      4.0711ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:09:45 | 200 |       5.952ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:10:45 | 200 |      3.7679ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:11:45 | 200 |      5.0999ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:12:45 | 200 |      4.5311ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:13:45 | 200 |      3.7446ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:14:45 | 200 |      4.6502ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:15:45 | 200 |      3.5932ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:16:45 | 200 |      5.4865ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:17:45 | 200 |      3.9328ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:18:45 | 200 |      3.6629ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:19:45 | 200 |      4.1744ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:20:45 | 200 |      4.0667ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:21:45 | 200 |      4.5881ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:22:45 | 200 |      3.2961ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:23:45 | 200 |       5.019ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:24:45 | 200 |      4.6381ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:25:45 | 200 |      3.3577ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:26:45 | 200 |      3.4117ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:27:45 | 200 |      4.7314ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:28:45 | 200 |        4.42ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:29:45 | 200 |      5.8303ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:30:45 | 200 |      5.6451ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:31:45 | 200 |       6.338ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:32:45 | 200 |      6.8136ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:33:45 | 200 |      4.6404ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:34:45 | 200 |      5.1103ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:35:45 | 200 |      4.7135ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:36:45 | 200 |      4.2459ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:37:45 | 200 |      5.7868ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:38:45 | 200 |      2.9213ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:39:45 | 200 |      5.5548ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:40:45 | 200 |      3.5633ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:41:45 | 200 |      6.6881ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:42:45 | 200 |      2.2011ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:43:45 | 200 |      7.2736ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:44:45 | 200 |      6.9462ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:45:45 | 200 |      4.6048ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:46:45 | 200 |      3.9337ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:47:45 | 200 |      4.3676ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:48:45 | 200 |      4.2401ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:49:45 | 200 |      5.6371ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:50:45 | 200 |      6.8072ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:51:45 | 200 |      4.5868ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:52:45 | 200 |      7.3746ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:53:45 | 200 |      2.8038ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:54:45 | 200 |      9.6607ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:55:45 | 200 |      3.4621ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:56:45 | 200 |      3.4206ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:57:45 | 200 |      3.6259ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:58:45 | 200 |      3.3595ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 06:59:45 | 200 |      5.0487ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:00:45 | 200 |      4.6102ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:01:45 | 200 |      4.7233ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:02:45 | 200 |      4.1042ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:03:45 | 200 |      4.5994ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:04:45 | 200 |      4.2718ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:05:45 | 200 |      6.7871ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:06:45 | 200 |      5.4053ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:07:45 | 200 |      5.3142ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:08:45 | 200 |      3.4168ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:09:45 | 200 |      3.8824ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:10:45 | 200 |      5.7921ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:11:45 | 200 |      4.6317ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:12:45 | 200 |      4.1684ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:13:45 | 200 |      4.7487ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:14:45 | 200 |     10.7589ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:15:45 | 200 |      4.2829ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:16:45 | 200 |      5.3894ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:17:45 | 200 |      3.7696ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:18:45 | 200 |      3.5862ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:19:45 | 200 |      4.6587ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:20:45 | 200 |      3.7039ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:21:45 | 200 |      5.4934ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:22:45 | 200 |        3.25ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:23:45 | 200 |      4.1509ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:24:45 | 200 |      4.1827ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:25:45 | 200 |      3.6003ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:26:45 | 200 |      4.2526ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:27:45 | 200 |      4.5794ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:28:45 | 200 |      4.7614ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:29:45 | 200 |      4.0433ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:30:45 | 200 |      4.4223ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:31:45 | 200 |      4.5433ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:32:45 | 200 |      4.2922ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:33:45 | 200 |      4.4738ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:34:45 | 200 |      8.3865ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:35:45 | 200 |     10.0163ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:36:45 | 200 |      5.3643ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:37:45 | 200 |      4.2406ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:38:45 | 200 |       2.909ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:39:45 | 200 |      4.5183ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:40:45 | 200 |      3.9009ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:41:45 | 200 |      5.9478ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:42:45 | 200 |       5.015ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:43:45 | 200 |      3.8082ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:44:45 | 200 |      7.4249ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:45:45 | 200 |      4.1905ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:46:45 | 200 |      5.5476ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:47:45 | 200 |      3.6669ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:48:45 | 200 |      4.5508ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:49:45 | 200 |      3.8049ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:50:45 | 200 |      3.9022ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:51:45 | 200 |      6.3144ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:52:45 | 200 |       5.051ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:53:45 | 200 |      5.6136ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:54:45 | 200 |      6.2962ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:55:45 | 200 |       5.403ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:56:45 | 200 |      5.4324ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:57:45 | 200 |      3.1898ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:58:45 | 200 |      4.4226ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 07:59:45 | 200 |      4.1421ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 08:00:45 | 200 |      5.5717ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 08:01:45 | 200 |      3.2687ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 08:02:45 | 200 |      6.3216ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 08:03:45 | 200 |      4.0967ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 08:04:45 | 200 |      6.5243ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 08:05:45 | 200 |      4.3958ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 08:06:45 | 200 |      3.3778ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 08:07:45 | 200 |      3.7608ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 08:08:45 | 200 |      3.3391ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 08:09:45 | 200 |      4.3687ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 08:10:45 | 200 |      4.5729ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:25:01 | 200 |     16.5279ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:25:47 | 200 |     13.1912ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:26:47 | 200 |      3.2615ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:27:47 | 200 |      7.5854ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:28:47 | 200 |      3.7222ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:29:47 | 200 |      4.0983ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:30:47 | 200 |      4.4274ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:31:47 | 200 |      4.2458ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:32:47 | 200 |      5.8178ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:33:47 | 200 |      4.2696ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:34:47 | 200 |      4.0888ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:35:47 | 200 |      5.0297ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:36:47 | 200 |      5.0064ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:37:47 | 200 |      4.2946ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:38:47 | 200 |       4.888ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:39:47 | 200 |      5.3315ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:40:47 | 200 |      3.5677ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 16:41:47 | 200 |      4.5273ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 17:30:48 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/13 - 17:30:48 | 200 |     83.4795ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-13T17:30:48.351-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="53.9 GiB" free_swap="45.1 GiB"
time=2025-08-13T17:30:48.351-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=25 layers.split="" memory.available="[4.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.7 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.7 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-13T17:30:48.904-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 25 --threads 4 --no-mmap --parallel 1 --port 53349"
time=2025-08-13T17:30:48.913-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-13T17:30:48.913-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-13T17:30:48.913-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-13T17:30:49.020-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-13T17:30:49.133-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-13T17:30:49.134-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:53349"
time=2025-08-13T17:30:49.165-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 25 repeating layers to GPU
load_tensors: offloaded 25/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   852.99 MiB
load_tensors:        CUDA0 model buffer size =  3315.10 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   200.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    24.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 46 (with bs=512), 3 (with bs=1)
time=2025-08-13T17:30:51.168-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.26 seconds"
[GIN] 2025/08/13 - 17:30:56 | 200 |    8.0622078s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/13 - 17:31:41 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/13 - 17:31:41 | 200 |     81.0439ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/13 - 17:32:03 | 200 |   22.2834048s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/13 - 17:32:03 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/13 - 17:32:03 | 200 |     77.9677ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/13 - 17:32:25 | 200 |   21.3246784s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/13 - 17:32:25 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/13 - 17:32:25 | 200 |     79.7487ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/13 - 17:32:46 | 200 |   21.5214547s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/13 - 17:32:46 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/13 - 17:32:47 | 200 |     70.4988ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/13 - 17:33:09 | 200 |   22.1508574s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/13 - 17:33:09 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/13 - 17:33:09 | 200 |     83.3085ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/13 - 17:33:13 | 200 |    3.8352034s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/13 - 17:50:35 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/13 - 17:50:35 | 200 |     82.0302ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-13T17:50:35.366-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="54.2 GiB" free_swap="45.3 GiB"
time=2025-08-13T17:50:35.367-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=25 layers.split="" memory.available="[4.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.7 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.7 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-13T17:50:35.928-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 25 --threads 4 --no-mmap --parallel 1 --port 54136"
time=2025-08-13T17:50:35.935-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-13T17:50:35.935-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-13T17:50:35.935-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-13T17:50:36.006-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-13T17:50:36.104-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-13T17:50:36.106-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:54136"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-13T17:50:36.186-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 25 repeating layers to GPU
load_tensors: offloaded 25/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   852.99 MiB
load_tensors:        CUDA0 model buffer size =  3315.10 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   200.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    24.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 46 (with bs=512), 3 (with bs=1)
time=2025-08-13T17:50:37.689-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.75 seconds"
[GIN] 2025/08/13 - 17:50:50 | 200 |   14.9022678s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/13 - 17:50:50 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/13 - 17:50:50 | 200 |     70.9859ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/13 - 17:51:04 | 200 |    14.175796s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/13 - 17:51:04 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/13 - 17:51:04 | 200 |     76.1965ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/13 - 17:51:19 | 200 |   14.6093701s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/13 - 17:51:19 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/13 - 17:51:19 | 200 |     75.9875ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/13 - 17:51:35 | 200 |   16.1389412s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/13 - 17:51:35 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/13 - 17:51:35 | 200 |     76.3955ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/13 - 17:51:46 | 200 |    10.279818s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/13 - 17:51:46 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/13 - 17:51:46 | 200 |     76.9272ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/13 - 17:52:02 | 200 |   16.0435164s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/13 - 17:52:45 | 200 |            0s |             ::1 | GET      "/"
[GIN] 2025/08/13 - 17:52:46 | 404 |            0s |             ::1 | GET      "/favicon.ico"
[GIN] 2025/08/13 - 18:32:33 | 200 |      3.6674ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 18:32:35 | 200 |      4.0018ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 18:32:40 | 200 |      6.3131ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 18:32:40 | 200 |      3.8592ms |             ::1 | GET      "/api/tags"
time=2025-08-13T18:33:00.219-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="54.1 GiB" free_swap="45.2 GiB"
time=2025-08-13T18:33:00.219-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=25 layers.split="" memory.available="[4.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.7 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.7 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-13T18:33:00.760-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 25 --threads 4 --no-mmap --parallel 1 --port 55929"
time=2025-08-13T18:33:00.768-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-13T18:33:00.768-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-13T18:33:00.769-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-13T18:33:00.929-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-13T18:33:01.027-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-13T18:33:01.028-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:55929"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
time=2025-08-13T18:33:01.271-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 25 repeating layers to GPU
load_tensors: offloaded 25/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   852.99 MiB
load_tensors:        CUDA0 model buffer size =  3315.10 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   200.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    24.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 46 (with bs=512), 3 (with bs=1)
time=2025-08-13T18:33:03.025-04:00 level=INFO source=server.go:630 msg="llama runner started in 2.26 seconds"
[GIN] 2025/08/13 - 18:33:10 | 200 |      3.6892ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 18:33:37 | 200 |   37.6994404s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/13 - 18:33:40 | 200 |      6.2426ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 18:34:10 | 200 |      6.9435ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 18:34:40 | 200 |      5.1787ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/13 - 19:27:56 | 200 |            0s |             ::1 | GET      "/"
[GIN] 2025/08/13 - 19:27:56 | 404 |            0s |             ::1 | GET      "/favicon.ico"
[GIN] 2025/08/14 - 13:18:44 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/14 - 13:18:44 | 200 |     91.3148ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-14T13:18:44.610-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="53.7 GiB" free_swap="45.1 GiB"
time=2025-08-14T13:18:44.610-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=24 layers.split="" memory.available="[4.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.6 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.6 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-14T13:18:45.204-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 24 --threads 4 --no-mmap --parallel 1 --port 64865"
time=2025-08-14T13:18:45.211-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-14T13:18:45.211-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-14T13:18:45.213-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-14T13:18:45.339-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-14T13:18:45.572-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-14T13:18:45.574-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:64865"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-14T13:18:45.728-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloaded 24/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   978.05 MiB
load_tensors:        CUDA0 model buffer size =  3190.04 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   192.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    32.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 60 (with bs=512), 3 (with bs=1)
time=2025-08-14T13:18:53.518-04:00 level=INFO source=server.go:630 msg="llama runner started in 8.31 seconds"
[GIN] 2025/08/14 - 13:19:33 | 200 |   49.4447026s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/14 - 13:37:45 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/14 - 13:37:45 | 200 |     70.9586ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-14T13:37:45.346-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="54.3 GiB" free_swap="45.5 GiB"
time=2025-08-14T13:37:45.347-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=24 layers.split="" memory.available="[4.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="4.6 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[4.6 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-14T13:37:45.914-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 24 --threads 4 --no-mmap --parallel 1 --port 49543"
time=2025-08-14T13:37:45.926-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-14T13:37:45.927-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-14T13:37:45.927-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-14T13:37:46.083-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-14T13:37:46.331-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-14T13:37:46.332-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:49543"
time=2025-08-14T13:37:46.432-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloaded 24/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   978.05 MiB
load_tensors:        CUDA0 model buffer size =  3190.04 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   192.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =    32.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 60 (with bs=512), 3 (with bs=1)
time=2025-08-14T13:37:51.702-04:00 level=INFO source=server.go:630 msg="llama runner started in 5.78 seconds"
[GIN] 2025/08/14 - 13:38:30 | 200 |   45.5761529s |       127.0.0.1 | POST     "/api/generate"
time=2025-08-14T13:43:54.032-04:00 level=WARN source=sched.go:687 msg="gpu VRAM usage didn't recover within timeout" seconds=23.2612277 runner.name=registry.ollama.ai/library/nexusai:latest runner.inference=cuda runner.devices=1 runner.size="5.6 GiB" runner.vram="4.6 GiB" runner.parallel=1 runner.pid=269360 runner.model=D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 runner.num_ctx=4096
[GIN] 2025/08/14 - 13:53:48 | 200 |    333.1632ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/14 - 13:53:48 | 200 |    338.8321ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/14 - 13:53:48 | 200 |    349.3056ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/14 - 13:53:49 | 200 |    193.3913ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/14 - 13:53:49 | 200 |    195.7561ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/14 - 13:53:49 | 200 |    200.3104ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/14 - 20:12:58 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/14 - 20:12:58 | 200 |     89.6437ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-14T20:12:58.883-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="60.0 GiB" free_swap="57.6 GiB"
time=2025-08-14T20:12:58.884-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-14T20:12:59.513-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 52666"
time=2025-08-14T20:12:59.521-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-14T20:12:59.521-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-14T20:12:59.522-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-14T20:12:59.622-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-14T20:12:59.728-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-14T20:12:59.729-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:52666"
time=2025-08-14T20:12:59.774-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-14T20:13:01.283-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/14 - 20:13:05 | 200 |    6.9470672s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/14 - 20:17:31 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/14 - 20:17:31 | 200 |     83.3826ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/14 - 20:17:57 | 200 |   26.2843116s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/14 - 20:28:33 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/14 - 20:28:33 | 200 |    101.6562ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-14T20:28:33.693-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="59.4 GiB" free_swap="56.3 GiB"
time=2025-08-14T20:28:33.694-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-14T20:28:34.483-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 53350"
time=2025-08-14T20:28:34.495-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-14T20:28:34.495-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-14T20:28:34.496-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-14T20:28:34.568-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-14T20:28:34.667-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-14T20:28:34.669-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:53350"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-14T20:28:34.748-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-14T20:28:38.260-04:00 level=INFO source=server.go:630 msg="llama runner started in 3.76 seconds"
[GIN] 2025/08/14 - 20:28:44 | 200 |   11.0368969s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/14 - 21:26:52 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/14 - 21:26:52 | 200 |      4.1662ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/14 - 21:51:02 | 200 |       8.109ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 21:51:31 | 200 |      4.9376ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 21:51:47 | 200 |      4.1044ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 21:55:44 | 200 |      5.1604ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 21:55:54 | 200 |      3.7755ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 21:55:54 | 200 |      4.7896ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 21:55:56 | 200 |      5.2827ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 22:16:10 | 200 |      5.3471ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 22:16:19 | 200 |      4.6891ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 22:16:20 | 200 |      4.5574ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 22:16:21 | 200 |      4.8066ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 22:20:07 | 200 |      4.9282ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 22:20:18 | 200 |      5.1698ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 22:20:18 | 200 |      4.7503ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 22:20:20 | 200 |      4.4709ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 23:20:46 | 200 |      6.0265ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 23:29:22 | 200 |      3.7968ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 23:46:08 | 200 |      3.3472ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 23:53:25 | 200 |      4.2709ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/14 - 23:53:25 | 200 |      4.1121ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/15 - 00:13:14 | 200 |      5.1814ms |             ::1 | GET      "/api/tags"
time=2025-08-15T00:13:25.642-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="60.0 GiB" free_swap="57.0 GiB"
time=2025-08-15T00:13:25.643-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-15T00:13:26.267-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 65035"
time=2025-08-15T00:13:26.274-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-15T00:13:26.274-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-15T00:13:26.275-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-15T00:13:26.337-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-15T00:13:26.435-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-15T00:13:26.435-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:65035"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-15T00:13:26.528-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-15T00:13:27.781-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.51 seconds"
[GIN] 2025/08/15 - 00:13:29 | 200 |    3.8761873s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/15 - 00:19:26 | 200 |      5.5992ms |             ::1 | GET      "/api/tags"
time=2025-08-15T00:19:34.343-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="58.8 GiB" free_swap="55.8 GiB"
time=2025-08-15T00:19:34.344-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-15T00:19:34.950-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 65284"
time=2025-08-15T00:19:34.959-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-15T00:19:34.959-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-15T00:19:34.961-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-15T00:19:35.024-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-15T00:19:35.121-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-15T00:19:35.121-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:65284"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
time=2025-08-15T00:19:35.212-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
[GIN] 2025/08/15 - 00:19:36 | 200 |      4.0989ms |             ::1 | GET      "/api/tags"
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-15T00:19:36.717-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/15 - 00:19:38 | 200 |    4.1015847s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/15 - 00:19:41 | 200 |     721.215ms |             ::1 | POST     "/api/generate"
[GIN] 2025/08/15 - 00:47:10 | 200 |      5.3099ms |             ::1 | GET      "/api/tags"
time=2025-08-15T01:08:35.223-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="57.9 GiB" free_swap="54.8 GiB"
time=2025-08-15T01:08:35.224-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-15T01:08:35.848-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 51267"
time=2025-08-15T01:08:35.858-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-15T01:08:35.858-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-15T01:08:35.859-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-15T01:08:35.936-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-15T01:08:36.032-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-15T01:08:36.033-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:51267"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-15T01:08:36.110-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-15T01:08:37.614-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/15 - 01:08:40 | 200 |    5.2771407s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/15 - 01:08:54 | 200 |      2.6055ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/15 - 01:08:54 | 200 |       2.079ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/15 - 01:09:48 | 200 |      3.0558ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/15 - 01:09:48 | 200 |      5.1829ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/15 - 01:09:55 | 200 |      4.2191ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/15 - 01:09:55 | 200 |       3.417ms |             ::1 | GET      "/api/tags"
[GIN] 2025/08/15 - 01:10:04 | 400 |      16.973ms |             ::1 | POST     "/api/generate"
time=2025-08-15T01:10:23.615-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/15 - 01:10:25 | 200 |    1.9286391s |             ::1 | POST     "/api/generate"
time=2025-08-15T01:10:40.428-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/15 - 01:10:52 | 200 |     12.46381s |             ::1 | POST     "/api/generate"
time=2025-08-15T01:11:20.230-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/15 - 01:11:44 | 200 |   24.1324667s |             ::1 | POST     "/api/generate"
time=2025-08-15T01:12:38.812-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/15 - 01:13:02 | 200 |   23.4848993s |             ::1 | POST     "/api/generate"
time=2025-08-15T01:13:33.741-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
[GIN] 2025/08/15 - 01:13:58 | 200 |    24.716975s |             ::1 | POST     "/api/generate"
time=2025-08-15T01:22:52.500-04:00 level=WARN source=types.go:572 msg="invalid option provided" option=max_tokens
time=2025-08-15T01:22:52.623-04:00 level=INFO source=server.go:135 msg="system memory" total="95.7 GiB" free="58.3 GiB" free_swap="55.3 GiB"
time=2025-08-15T01:22:52.623-04:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=28 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.2 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[5.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-08-15T01:22:53.224-04:00 level=INFO source=server.go:431 msg="starting llama server" cmd="C:\\Users\\moonc\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\UNIFIED_OLLAMA_MODELS\\blobs\\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 4096 --batch-size 512 --n-gpu-layers 28 --threads 4 --no-mmap --parallel 1 --port 51862"
time=2025-08-15T01:22:53.232-04:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-08-15T01:22:53.232-04:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-08-15T01:22:53.233-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
time=2025-08-15T01:22:53.300-04:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3050 6GB Laptop GPU, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from C:\Users\moonc\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-08-15T01:22:53.402-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-08-15T01:22:53.403-04:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:51862"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU) - 5171 MiB free
time=2025-08-15T01:22:53.485-04:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from D:\UNIFIED_OLLAMA_MODELS\blobs\sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloaded 28/29 layers to GPU
load_tensors:    CUDA_Host model buffer size =   426.37 MiB
load_tensors:        CUDA0 model buffer size =  3741.72 MiB
load_tensors:          CPU model buffer size =   292.36 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   730.36 MiB
llama_context:  CUDA_Host compute buffer size =    15.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 4 (with bs=512), 3 (with bs=1)
time=2025-08-15T01:22:54.989-04:00 level=INFO source=server.go:630 msg="llama runner started in 1.76 seconds"
[GIN] 2025/08/15 - 01:23:28 | 200 |    36.153775s |             ::1 | POST     "/api/generate"
[GIN] 2025/08/15 - 01:23:45 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/15 - 01:23:45 | 200 |     88.3274ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/08/15 - 01:23:45 | 200 |     44.5229ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/15 - 01:24:22 | 200 |   12.6155538s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/15 - 01:24:45 | 200 |   10.2699217s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/15 - 01:25:06 | 200 |    6.8735143s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/08/15 - 01:25:22 | 200 |    6.1393108s |       127.0.0.1 | POST     "/api/chat"
