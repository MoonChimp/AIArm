/**
 * AIArm HRM Error Monitor
 * Monitors error logs and sends alerts when necessary
 */

const fs = require('fs');
const path = require('path');
const { execSync } = require('child_process');
const config = require('./config');

// Log directories
const logDir = path.resolve(config.logDirectory || 'D:/AIArm/Logs');
const errorLogDir = path.join(logDir, 'Errors');
const alertsDir = path.join(logDir, 'Alerts');

// Ensure directories exist
if (!fs.existsSync(errorLogDir)) {
  fs.mkdirSync(errorLogDir, { recursive: true });
}
if (!fs.existsSync(alertsDir)) {
  fs.mkdirSync(alertsDir, { recursive: true });
}

// Log file for this monitor
const monitorLogFile = path.join(logDir, 'error_monitor.log');

// Log function
function logMessage(message) {
  const timestamp = new Date().toISOString();
  const logEntry = `[${timestamp}] ${message}\n`;
  
  // Log to console
  console.log(logEntry.trim());
  
  // Log to file
  fs.appendFileSync(monitorLogFile, logEntry);
}

// Notify function (for severe errors)
function sendNotification(message, level = 'WARNING') {
  // In a production system, this would send an email, SMS, or other notification
  // For now, we'll just log it and create an alert file
  
  const alertFile = path.join(alertsDir, `ALERT_${Date.now()}.txt`);
  const alertContent = `
==========================================================
  ${level} ALERT - ${new Date().toISOString()}
==========================================================

${message}

==========================================================
  Generated by Error Monitor
==========================================================
`;
  
  fs.writeFileSync(alertFile, alertContent);
  
  // Log to console with emphasis
  console.log('\n' + '*'.repeat(80));
  console.log(`${level} ALERT: ${message}`);
  console.log('*'.repeat(80) + '\n');
  
  // On Windows, show a notification
  try {
    execSync(`msg "%username%" "${level} ALERT: ${message.replace(/"/g, '\\"')}"`);
  } catch (error) {
    // Ignore errors with msg command
  }
}

// Check for new error logs
function checkErrorLogs() {
  try {
    // Get all error log files
    const errorLogs = fs.readdirSync(errorLogDir)
      .filter(file => file.endsWith('.log') || file.endsWith('.json'))
      .map(file => path.join(errorLogDir, file));
    
    // Get modification times
    const fileStats = errorLogs.map(file => ({
      file,
      mtime: fs.statSync(file).mtime
    }));
    
    // Sort by modification time (newest first)
    fileStats.sort((a, b) => b.mtime - a.mtime);
    
    // Check the 5 most recent files
    const recentErrors = fileStats.slice(0, 5);
    
    // Process recent error files
    for (const { file, mtime } of recentErrors) {
      // Only process files modified in the last 5 minutes
      const fiveMinutesAgo = new Date(Date.now() - 5 * 60 * 1000);
      if (mtime > fiveMinutesAgo) {
        processErrorFile(file);
      }
    }
  } catch (error) {
    logMessage(`Error checking error logs: ${error.message}`);
  }
}

// Process an error file
function processErrorFile(filePath) {
  try {
    const content = fs.readFileSync(filePath, 'utf8');
    const fileName = path.basename(filePath);
    
    // Check if we've already processed this file
    if (processedFiles.has(filePath)) {
      return;
    }
    
    // Mark as processed
    processedFiles.add(filePath);
    
    // Log the error file detection
    logMessage(`Detected new error file: ${fileName}`);
    
    // Analyze severity based on content
    let severity = 'WARNING';
    
    // Check for critical keywords
    if (content.includes('CRITICAL') || 
        content.includes('fatal') || 
        content.includes('crash') ||
        content.includes('exception')) {
      severity = 'CRITICAL';
    }
    
    // Check for specific bridge errors
    if (fileName.startsWith('surface_') || content.includes('Surface bridge error')) {
      logMessage(`Surface bridge error detected in ${fileName}`);
      
      // Count recent surface bridge errors
      surfaceBridgeErrors++;
      
      if (surfaceBridgeErrors >= 3) {
        sendNotification('Multiple Surface bridge errors detected. Bridge may be unstable.', severity);
        // Reset counter after notification
        surfaceBridgeErrors = 0;
      }
    }
    
    if (fileName.startsWith('deep_') || content.includes('Deep bridge error')) {
      logMessage(`Deep bridge error detected in ${fileName}`);
      
      // Count recent deep bridge errors
      deepBridgeErrors++;
      
      if (deepBridgeErrors >= 3) {
        sendNotification('Multiple Deep bridge errors detected. Bridge may be unstable.', severity);
        // Reset counter after notification
        deepBridgeErrors = 0;
      }
    }
    
    // Check for API errors
    if (fileName.startsWith('api_') || content.includes('API error')) {
      logMessage(`API error detected in ${fileName}`);
      
      // Count recent API errors
      apiErrors++;
      
      if (apiErrors >= 5) {
        sendNotification('Multiple API errors detected. Server may be unstable.', severity);
        // Reset counter after notification
        apiErrors = 0;
      }
    }
    
    // Handle fatal errors immediately
    if (severity === 'CRITICAL') {
      sendNotification(`Critical error detected: ${fileName}`, 'CRITICAL');
    }
  } catch (error) {
    logMessage(`Error processing error file ${filePath}: ${error.message}`);
  }
}

// Check for alert conditions
function checkAlertConditions() {
  try {
    // Read the service status file
    const statusFilePath = path.join(__dirname, 'service_status.txt');
    
    if (fs.existsSync(statusFilePath)) {
      const content = fs.readFileSync(statusFilePath, 'utf8');
      let status;
      
      try {
        status = JSON.parse(content);
      } catch (e) {
        logMessage(`Error parsing service status: ${e.message}`);
        return;
      }
      
      // Check for unhealthy status
      if (status.status === 'unhealthy' || status.status === 'degraded') {
        logMessage(`System status is ${status.status}`);
        
        // Check for alerts
        if (status.alerts && status.alerts.length > 0) {
          for (const alert of status.alerts) {
            logMessage(`Alert: ${alert.message}`);
            
            // Send notification for bridge down alerts
            if (alert.type === 'BRIDGE_DOWN') {
              sendNotification(`Bridge down: ${alert.bridge}`, 'CRITICAL');
            }
            
            // Send notification for inner life down alerts
            if (alert.type === 'INNER_LIFE_DOWN') {
              sendNotification('Inner Life processor is not running', 'CRITICAL');
            }
          }
        }
      }
      
      // Check memory usage
      if (status.server && status.server.memory_usage) {
        const memoryUsage = status.server.memory_usage;
        
        if (memoryUsage.percentUsed > 90) {
          logMessage(`High memory usage: ${memoryUsage.percentUsed}%`);
          
          memoryAlerts++;
          
          if (memoryAlerts >= 3) {
            sendNotification(`High memory usage: ${memoryUsage.percentUsed}%`, 'WARNING');
            memoryAlerts = 0;
          }
        }
      }
      
      // Check CPU usage
      if (status.server && status.server.cpu_usage) {
        const cpuUsage = status.server.cpu_usage;
        
        if (cpuUsage.percent > 90) {
          logMessage(`High CPU usage: ${cpuUsage.percent}%`);
          
          cpuAlerts++;
          
          if (cpuAlerts >= 3) {
            sendNotification(`High CPU usage: ${cpuUsage.percent}%`, 'WARNING');
            cpuAlerts = 0;
          }
        }
      }
      
      // Check pending requests
      if (status.server && status.server.pending_requests) {
        const pendingRequests = status.server.pending_requests;
        
        if (pendingRequests.surface > 5 || pendingRequests.deep > 5) {
          logMessage(`High pending requests: Surface=${pendingRequests.surface}, Deep=${pendingRequests.deep}`);
          
          requestAlerts++;
          
          if (requestAlerts >= 3) {
            sendNotification(`High pending requests: Surface=${pendingRequests.surface}, Deep=${pendingRequests.deep}`, 'WARNING');
            requestAlerts = 0;
          }
        }
      }
    }
  } catch (error) {
    logMessage(`Error checking alert conditions: ${error.message}`);
  }
}

// Check if inner life processor is running
function checkInnerLife() {
  try {
    // Check for inner life process
    const isRunning = fs.existsSync(path.join(config.innerLifeDirectory, 'heartbeat.txt'));
    
    if (!isRunning) {
      innerLifeDownCount++;
      
      if (innerLifeDownCount >= 3) {
        logMessage('Inner Life processor appears to be down');
        sendNotification('Inner Life processor is not running. System may be degraded.', 'WARNING');
        
        // Try to restart if configured
        if (config.recovery?.enabled && config.recovery?.autoRestart) {
          logMessage('Attempting to restart Inner Life processor');
          
          try {
            execSync('start "Nexus Inner Life" cmd /c "python D:\\AIArm\\InnerLife\\inner_life_processor.py"');
            logMessage('Inner Life processor restart initiated');
          } catch (error) {
            logMessage(`Error restarting Inner Life processor: ${error.message}`);
          }
        }
        
        innerLifeDownCount = 0;
      }
    } else {
      innerLifeDownCount = 0;
    }
  } catch (error) {
    logMessage(`Error checking Inner Life status: ${error.message}`);
  }
}

// Clean up old logs
function cleanupOldLogs() {
  try {
    // Get retention days
    const retentionDays = config.logRetentionDays || 7;
    const cutoffDate = new Date(Date.now() - retentionDays * 24 * 60 * 60 * 1000);
    
    // Scan directories for old logs
    const scanDirectory = (dir) => {
      const files = fs.readdirSync(dir);
      
      for (const file of files) {
        const filePath = path.join(dir, file);
        const stats = fs.statSync(filePath);
        
        if (stats.isDirectory()) {
          // Skip subdirectories with special names
          if (file !== 'Backups' && file !== 'Critical') {
            scanDirectory(filePath);
          }
        } else if (stats.isFile() && (file.endsWith('.log') || file.endsWith('.json'))) {
          if (stats.mtime < cutoffDate) {
            // Move to backup directory instead of deleting
            const backupDir = path.join(logDir, 'Backups', 'OldLogs');
            
            if (!fs.existsSync(backupDir)) {
              fs.mkdirSync(backupDir, { recursive: true });
            }
            
            const backupPath = path.join(backupDir, file);
            fs.renameSync(filePath, backupPath);
            
            logMessage(`Archived old log file: ${file}`);
          }
        }
      }
    };
    
    // Scan the main log directory
    scanDirectory(logDir);
    
  } catch (error) {
    logMessage(`Error cleaning up old logs: ${error.message}`);
  }
}

// Track processed files and error counts
const processedFiles = new Set();
let surfaceBridgeErrors = 0;
let deepBridgeErrors = 0;
let apiErrors = 0;
let memoryAlerts = 0;
let cpuAlerts = 0;
let requestAlerts = 0;
let innerLifeDownCount = 0;

// Main monitoring loop
function startMonitoring() {
  logMessage('Starting HRM Error Monitor');
  
  // Check immediately on startup
  checkErrorLogs();
  checkAlertConditions();
  checkInnerLife();
  
  // Set up intervals
  setInterval(checkErrorLogs, 30 * 1000);  // Every 30 seconds
  setInterval(checkAlertConditions, 60 * 1000);  // Every minute
  setInterval(checkInnerLife, 5 * 60 * 1000);  // Every 5 minutes
  setInterval(cleanupOldLogs, 24 * 60 * 60 * 1000);  // Once a day
  
  // Log periodic status
  setInterval(() => {
    logMessage('Error Monitor is running');
  }, 15 * 60 * 1000);  // Every 15 minutes
}

// Start monitoring
startMonitoring();
